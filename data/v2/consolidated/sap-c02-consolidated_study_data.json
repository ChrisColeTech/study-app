{
  "metadata": {
    "exam_code": "sap-c02",
    "name": "AWS Certification Study Dataset - aws-certified-solutions-architect-professional_8 (Fixed Extraction)",
    "description": "Consolidated SAP-C02 certification questions",
    "creation_date": "2025-08-11T21:16:20.227481",
    "version": "2.1-consolidated",
    "total_questions": 179,
    "answered_questions": 179,
    "coverage_percentage": 100.0,
    "consolidation_stats": {
      "source_files": [
        "aws-sap-c02_pro8_study_data.json",
        "sap-c02_6_study_data.json",
        "sap-c02_7_study_data.json",
        "sap-c02_8_study_data.json"
      ],
      "original_questions": 216,
      "unique_questions": 179,
      "duplicates_removed": 37
    },
    "data_quality": {
      "answer_coverage": "100%",
      "explanation_coverage": "97.8%"
    }
  },
  "study_data": [
    {
      "question": {
        "id": "sap-c02_d4b9cf83a2f4",
        "number": 1,
        "text": "An external audit of a company's serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS\nLambda execution roles. Hundreds of the company's Lambda functions have broad access permissions, such as full access to Amazon S3 buckets and Amazon\nDynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task.\nA solutions architect must determine which permissions each Lambda function needs.\nWhat should the solutions architect do to meet this requirement with the LEAST amount of effort?",
        "options": [
          {
            "text": "Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API call",
            "letter": "A"
          },
          {
            "text": "Create an inventory of the required API calls and resources for each Lambda functio",
            "letter": "B"
          },
          {
            "text": "Create new IAM access policies for each Lambda functio",
            "letter": "C"
          },
          {
            "text": "Review the new policies to ensure that they meet the company's business requirements.",
            "letter": "D"
          },
          {
            "text": "Turn on AWS CloudTrail logging for the AWS accoun F. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail lo G. Review the generated policies to ensure that they meet the company's business requirements. H. Turn on AWS CloudTrail logging for the AWS accoun I. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary repor J. Review the repor K. Create IAM access policies that provide more restrictive permissions for each Lambda function. L. Turn on AWS CloudTrail logging for the AWS accoun M. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution rol N. Create a new IAM access policy for each rol O. Export the generated roles to an S3 bucke P. Review the generated policies to ensure that they meet the company's business requirements.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk. IAM Access Analyzer identifies resources shared with external principals by using logic-based reasoning to analyze the resource-based policies in your AWS environment. https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d4b9cf83a2f4",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 1
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216619"
      }
    },
    {
      "question": {
        "id": "sap-c02_94b53941c3d9",
        "number": 2,
        "text": "A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to\ncustomers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application\nlatency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the\nother group in real time.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instanc",
            "letter": "A"
          },
          {
            "text": "Pause application writes to the RDS DB instanc",
            "letter": "B"
          },
          {
            "text": "Promote the Aurora Replica to a standalone DB cluste",
            "letter": "C"
          },
          {
            "text": "Reconfigure the application to use the Aurora database and resume write",
            "letter": "D"
          },
          {
            "text": "Add eu-west-1 as a secondary Region to the 06 cluste F. Enable write forwarding on the DB cluste G. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu- west-1. H. Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instanc I. Configure the replica to replicate write queries back to the primary DB instanc J. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1. K. Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapsho L. Configure MySQL logical replication fromus-east-1 to eu-west-1. Enable write forwarding on the DB cluste M. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1. N. Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluste O. Add eu-west-1 as a secondary Region to the DB cluste P. Enable write forwarding on the DB cluste Q. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "The company should use AWS Amplify to create a static website for uploads of media files. The company should use Amplify Hosting to serve the website through Amazon CloudFront. The company should use Amazon S3 to store the uploaded media files. The company should use Amazon Cognito to authenticate users. This solution will meet the requirements with the least operational overhead because AWS Amplify is a complete solution that lets frontend web and mobile developers easily build, ship, and host full-stack applications on AWS, with the flexibility to leverage the breadth of AWS services as use cases evolve. No cloud expertise needed1. By using AWS Amplify, the company can refactor the application to a serverless architecture that reduces operational complexity and costs. AWS Amplify offers the following features and benefits: Amplify Studio: A visual interface that enables you to build and deploy a full-stack app quickly, including frontend UI and backend. Amplify CLI: A local toolchain that enables you to configure and manage an app backend with just a few commands. Amplify Libraries: Open-source client libraries that enable you to build cloud-powered mobile and web apps. Amplify UI Components: Open-source design system with cloud-connected components for building feature-rich apps fast. Amplify Hosting: Fully managed CI/CD and hosting for fast, secure, and reliable static and server-side rendered apps. By using AWS Amplify to create a static website for uploads of media files, the company can leverage Amplify Studio to visually build a pixel-perfect UI and connect it to a cloud backend in clicks. By using Amplify Hosting to serve the website through Amazon CloudFront, the company can easily deploy its web app or website to the fast, secure, and reliable AWS content delivery network (CDN), with hundreds of points of presence globally. By using Amazon S3 to store the uploaded media files, the company can benefit from a highly scalable, durable, and cost-effective object storage service that can handle any amount of data2. By Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "94b53941c3d9",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 2
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216637"
      }
    },
    {
      "question": {
        "id": "sap-c02_aa615180de50",
        "number": 3,
        "text": "A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful\napplication. The application connects to a PostgreSQL database running on a separate server. The applicationâ€™s user base is expected to grow significantly, so\nthe company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load\nBalancing.\nWhich solution will provide a consistent user experience that will allow the application and database tiers to scale?",
        "options": [
          {
            "text": "Enable Aurora Auto Scaling for Aurora Replica",
            "letter": "A"
          },
          {
            "text": "Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.",
            "letter": "B"
          },
          {
            "text": "Enable Aurora Auto Scaling for Aurora writer",
            "letter": "C"
          },
          {
            "text": "Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.",
            "letter": "D"
          },
          {
            "text": "Enable Aurora Auto Scaling for Aurora Replica F. Use an Application Load Balancer with the round robin routing and sticky sessions enabled. G. Enable Aurora Scaling for Aurora writer H. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "aa615180de50",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 3
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216649"
      }
    },
    {
      "question": {
        "id": "sap-c02_9b097c0d6efd",
        "number": 4,
        "text": "A company has VPC flow logs enabled for its NAT gateway. The company is seeing Action = ACCEPT for inbound traffic that comes from public IP address\n198.51.100.2 destined for a private Amazon EC2 instance.\nA solutions architect must determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block\nare 203.0.\nWhich set of steps should the solutions architect take to meet these requirements?",
        "options": [
          {
            "text": "Open the AWS CloudTrail consol",
            "letter": "A"
          },
          {
            "text": "Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interfac",
            "letter": "B"
          },
          {
            "text": "Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
            "letter": "C"
          },
          {
            "text": "Open the Amazon CloudWatch consol",
            "letter": "D"
          },
          {
            "text": "Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interfac F. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address. G. Open the AWS CloudTrail consol H. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interfac I. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address. J. Open the Amazon CloudWatch consol K. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interfac L. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/ by Cloudxie says \"select appropriate log\" Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "9b097c0d6efd",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 4
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216657"
      }
    },
    {
      "question": {
        "id": "sap-c02_f34aa51af6d3",
        "number": 5,
        "text": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced\nan issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an alias for every new deployed version of the Lambda functio",
            "letter": "A"
          },
          {
            "text": "Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.",
            "letter": "B"
          },
          {
            "text": "Deploy the application into a new CloudFormation stac",
            "letter": "C"
          },
          {
            "text": "Use an Amazon Route 53 weighted routing policy to distribute the load.",
            "letter": "D"
          },
          {
            "text": "Create a version for every new deployed Lambda functio F. Use the AWS CLIupdate-function-contiguration command with the routing-config parameter to distribute the load. G. Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f34aa51af6d3",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 5
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216663"
      }
    },
    {
      "question": {
        "id": "sap-c02_3f5882b7d378",
        "number": 6,
        "text": "A company needs to optimize the cost of an AWS environment that contains multiple accounts in an organization in AWS Organizations. The company conducted\ncost optimization activities 3 years ago and purchased Amazon EC2 Standard Reserved Instances that recently expired.\nThe company needs EC2 instances for 3 more years. Additionally, the company has deployed a new serverless workload.\nWhich strategy will provide the company with the MOST cost savings?",
        "options": [
          {
            "text": "Purchase the same Reserved Instances for an additional 3-year term with All Upfront paymen",
            "letter": "A"
          },
          {
            "text": "Purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs.",
            "letter": "B"
          },
          {
            "text": "Purchase a I-year Compute Savings Plan with No Upfront payment in each member accoun",
            "letter": "C"
          },
          {
            "text": "Use the Savings Plans recommendations in the AWS Cost Management console to choose the Compute Savings Plan.",
            "letter": "D"
          },
          {
            "text": "Purchase a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account to cover EC2 costs in each AWS Regio F. Purchase a 3 year Compute Savings Plan with No Upfront payment in the management account to cover any additional compute costs. G. Purchase a 3-year EC2 Instance Savings Plan with All Upfront payment in each member accoun H. Use the Savings Plans recommendations in the AWS Cost Management console to choose the EC2 Instance Savings Plan.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "The company should purchase the same Reserved Instances for an additional 3-year term with All Upfront payment. The company should purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs. This solution will provide the company with the most cost savings because Reserved Instances and Savings Plans are both pricing models that offer significant discounts compared to On-Demand pricing. Reserved Instances are commitments to use a specific instance type and size in a single Region for a one- or three-year term. You can choose between three payment options: No Upfront, Partial Upfront, or All Upfront. The more you pay upfront, the greater the discoun1t. Savings Plans are flexible pricing models that offer low prices on EC2 instances, Fargate, and Lambda usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a one- or three-year term. You can choose between two types of Savings Plans: Compute Savings Plans and EC2 Instance Savings Plans. Compute Savings Plans apply to any EC2 instance regardless of Region, instance family, operating system, or tenancy, including those that are part of EMR, ECS, or EKS clusters, or launched by Fargate or Lambda. EC2 Instance Savings Plans apply to a specific instance family within a Region and provide the most savings2. By purchasing the same Reserved Instances for an additional 3-year term with All Upfront payment, the company can lock in the lowest possible price for its EC2 instances that run continuously for 3 years. By purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account, the company can benefit from additional discounts on any other compute usage across its member accounts. The other options are not correct because: Purchasing a 1-year Compute Savings Plan with No Upfront payment in each member account would not provide as much cost savings as purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account. A 1-year term offers lower discounts than a 3-year term, and a No Upfront payment option offers lower discounts than an All Upfront payment option. Also, purchasing a Savings Plan in each member account would not allow the company to share the benefits of unused Savings Plan discounts across its organization. Purchasing a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account to cover EC2 costs in each AWS Region would not provide as much cost savings as purchasing Reserved Instances for an additional 3-year term with All Upfront payment. An EC2 Instance Savings Plan offers lower discounts than Reserved Instances for the same instance family and Region. Also, a No Upfront payment option offers lower discounts than an All Upfront payment option. Purchasing a 3-year EC2 Instance Savings Plan with All Upfront payment in each member account would not provide as much flexibility or cost savings as purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account. An EC2 Instance Savings Plan applies only to a specific instance family within a Region and does not cover Fargate or Lambda usage. Also, purchasing a Savings Plan in each member account would not allow the company to share the benefits of unused Savings Plan discounts across its organization. References: https://aws.amazon.com/ec2/pricing/reserved-instances/ https://aws.amazon.com/savingsplans/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "3f5882b7d378",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 6
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216670"
      }
    },
    {
      "question": {
        "id": "sap-c02_5a20c1ded5b4",
        "number": 7,
        "text": "A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB instance that is\nexperiencing overloaded connections. Most of the application's operations insert records into the database. The application currently stores credentials in a text-\nbased configuration file.\nThe solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the credentials secure\nand must provide the ability to rotate the credentials automatically on a regular basis.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Deploy an Amazon RDS Proxy layer in front of the DB instanc",
            "letter": "A"
          },
          {
            "text": "Store the connection credentials as a secret in AWS Secrets Manager. Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions)",
            "letter": "B"
          },
          {
            "text": "Deploy an Amazon RDS Proxy layer in front of the DB instanc",
            "letter": "C"
          },
          {
            "text": "Store the connection credentials in AWS Systems Manager Parameter Store.",
            "letter": "D"
          },
          {
            "text": "Create an Aurora Replic F. Store the connection credentials as a secret in AWS Secrets Manager. G. Create an Aurora Replic H. Store the connection credentials in AWS Systems Manager Parameter Store.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "5a20c1ded5b4",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 7
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216676"
      }
    },
    {
      "question": {
        "id": "sap-c02_0f8fd457163e",
        "number": 8,
        "text": "A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances, Amazon Elastic File System\n(Amazon EFS) file systems, and Amazon RDS DB instances.\nTo meet regulatory and business requirements, the company must make the following changes for data backups:\n* Backups must be retained based on custom daily, weekly, and monthly requirements.\n* Backups must be replicated to at least one other AWS Region immediately after capture.\n* The backup solution must provide a single source of backup status across the AWS environment.\n* The backup solution must send immediate notifications upon failure of any resource backup.\nWhich combination of steps will meet this requirement with the LEAST amount of operational overhead? (Select THREE.)",
        "options": [
          {
            "text": "Create an AWS Backup plan with a backup rule for each of the retention requirements.",
            "letter": "A"
          },
          {
            "text": "Configure an AWS backup plan to copy backups to another Region.",
            "letter": "B"
          },
          {
            "text": "Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.",
            "letter": "C"
          },
          {
            "text": "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP- JOB- COMPLETED.",
            "letter": "D"
          },
          {
            "text": "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements. F. Set up RDS snapshots on each database.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "ABD",
        "explanation": "Cross region with AWS Backup: https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "0f8fd457163e",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 8
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216682"
      }
    },
    {
      "question": {
        "id": "sap-c02_c77443f7e62a",
        "number": 9,
        "text": "A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.Iarge Amazon EC2 instances\nand uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones.\nIn the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an R TO of 10 minutes.\nWhich solution will meet these requirements MOST cost-effectively?",
        "options": [
          {
            "text": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Regio",
            "letter": "A"
          },
          {
            "text": "Create a cross-Region read replica for the DB instanc",
            "letter": "B"
          },
          {
            "text": "Set up a backup plan in AWS Backup to createcross-Region backups for the EC2 instances and the DB instanc",
            "letter": "C"
          },
          {
            "text": "Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Regio",
            "letter": "D"
          },
          {
            "text": "Recover the EC2 instancesfrom the latest EC2 backu F. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster. G. Use infrastructure as code (laC) to provision the new infrastructure in the DR Regio H. Create across-Region read replica for the DB instanc I. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Regio J. Run the EC2 instances at the minimum capacity in the DR Region Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaste K. Increase the desired capacity of the Auto Scaling group. L. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instanc M. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Regio N. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Regio O. Manually restore the backed-up data on new instance P. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster. Q. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Regio R. Create an Amazon Aurora global databas S. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Regio T. Run the Auto Scaling group of EC2 instances at full capacity in the DR Regio . Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "The company should use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. The company should create a cross-Region read replica for the DB instance. The company should set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. The company should run the EC2 instances at the minimum capacity in the DR Region. The company should use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. The company should increase the desired capacity of the Auto Scaling group. This solution will meet the requirements most cost-effectively because AWS Elastic Disaster Recovery (AWS DRS) is a service that minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery. AWS DRS enables RPOs of seconds and RTOs of minute1s. AWS DRS continuously replicates data from the source servers to a staging area subnet in the DR Region, where it uses low-cost storage and minimal compute resources to maintain ongoing replication. In the event of a disaster, AWS DRS automatically converts the servers to boot and run natively on AWS and launches recovery instances on AWS within minutes2. By using AWS DRS, the company can save costs by removing idle recovery site resources and paying for the full disaster recovery site only when needed. By creating a cross-Region read replica for the DB instance, the company can have a standby copy of its primary database in a different AWS Region3. By using infrastructure as code (IaC), the company can provision the new infrastructure in the Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "c77443f7e62a",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 9
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216690"
      }
    },
    {
      "question": {
        "id": "sap-c02_9e15bf667bd2",
        "number": 10,
        "text": "A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company\nrequirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront.\nWhich combination of steps will meet the encryption requirements? (Select THREE.)",
        "options": [
          {
            "text": "Turn on S3 server-side encryption for the S3 bucket that the web application uses.",
            "letter": "A"
          },
          {
            "text": "Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.",
            "letter": "B"
          },
          {
            "text": "Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.",
            "letter": "C"
          },
          {
            "text": "Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).",
            "letter": "D"
          },
          {
            "text": "Configure redirection of HTTP requests to HTTPS requests in CloudFront. F. Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "ACE",
        "explanation": "Turning on S3 server-side encryption for the S3 bucket that the web application uses will enable encrypting the data at rest using Amazon S3 managed keys (SSE- S3)1. Creating a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses will enable enforcing encryption for all requests to the bucket2. Configuring redirection of HTTP requests to HTTPS requests in CloudFront will enable encrypting the data in transit using SSL/TLS3.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "9e15bf667bd2",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 10
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216696"
      }
    },
    {
      "question": {
        "id": "sap-c02_f17326beaef6",
        "number": 11,
        "text": "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years.\nThe company's security team also must receive an email notification every time there is an attempt to delete data in the S3 bucket.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Select THREE.)",
        "options": [
          {
            "text": "Configure AWS CloudTrail to log S3 data events.",
            "letter": "A"
          },
          {
            "text": "Configure S3 server access logging for the S3 bucket.",
            "letter": "B"
          },
          {
            "text": "Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).",
            "letter": "C"
          },
          {
            "text": "Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.",
            "letter": "D"
          },
          {
            "text": "Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering. F. Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AD",
        "explanation": "Configuring AWS CloudTrail to log S3 data events will enable logging all activities for objects in the S3 bucket1. Data events are object-level API operations such as GetObject, DeleteObject, and PutObject1. Configuring Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic will enable sending email notifications every time there is an attempt to delete data in the S3 bucket2. EventBridge can route events from S3 to SNS, which can send emails to subscribers2. Configuring a new S3 bucket to store the logs with an S3 Lifecycle policy will enable keeping the logs for 5 years in a cost-effective way3. A lifecycle policy can transition the logs to a cheaper storage class such as Glacier or delete them after a specified period of time3.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f17326beaef6",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 10
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216702"
      }
    },
    {
      "question": {
        "id": "sap-c02_b7cf9625044d",
        "number": 12,
        "text": "A company uses AWS Organizations to manage more than 1.000 AWS accounts. The company has created a new developer organization. There are 540\ndeveloper member accounts that must be moved to the new developer organization. All accounts are set up with all the required Information so that each account\ncan be operated as a standalone account.\nWhich combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Select THREE.)\nPassing Certification Exams Made Easy visit - https://www.2PassEasy.com\n\n\nWelcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps\nhttps://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions)",
        "options": [
          {
            "text": "Call the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer accounts to the new developer organization.",
            "letter": "A"
          },
          {
            "text": "From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.",
            "letter": "B"
          },
          {
            "text": "From each developer account, remove the account from the old organization using theRemoveAccountFromOrganization operation in the Organizations API.",
            "letter": "C"
          },
          {
            "text": "Sign in to the new developer organization's management account and create a placeholder member account that acts as a target for the developer account migration.",
            "letter": "D"
          },
          {
            "text": "Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts. F. Have each developer sign in to their account and confirm to join the new developer organization.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BE",
        "explanation": "\"This operation can be called only from the organization's management account. Member accounts can remove themselves with LeaveOrganization instead.\" https://docs.aws.amazon.com/organizations/latest/APIReference/API_RemoveAccountFromOrganization.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b7cf9625044d",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 14
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216709"
      }
    },
    {
      "question": {
        "id": "sap-c02_ecf4758c0daa",
        "number": 13,
        "text": "A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3\nbucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.\nThe company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the\ndata indefinitely for compliance reasons.\nWhich solution will meet these requirements MOST cost-effectively?",
        "options": [
          {
            "text": "Use S3 Select to query the dat",
            "letter": "A"
          },
          {
            "text": "Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
            "letter": "B"
          },
          {
            "text": "Use Amazon Redshift Spectrum to query the dat",
            "letter": "C"
          },
          {
            "text": "Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
            "letter": "D"
          },
          {
            "text": "Use an AWS Glue Data Catalog and Amazon Athena to query the dat F. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive. G. Use Amazon Redshift Spectrum to query the dat H. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Generally, unstructured data should be converted structured data before querying them. AWS Glue can do that. https://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ecf4758c0daa",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 15
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216715"
      }
    },
    {
      "question": {
        "id": "sap-c02_634733317eea",
        "number": 14,
        "text": "A solutions architect at a large company needs to set up network security tor outbound traffic to the internet from all AWS accounts within an organization in AWS\nOrganizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Each\naccount has both an internet gateway and a NAT gateway tor outbound traffic to the internet The company deploys resources only into a single AWS Region.\nThe company needs the ability to add centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization. The\npeak load of outbound traffic will not exceed 25 Gbps in each Availability Zone.\nWhich solution meets these requirements?",
        "options": [
          {
            "text": "Create a new VPC for outbound traffic to the interne",
            "letter": "A"
          },
          {
            "text": "Connect the existing transit gateway to the new VP",
            "letter": "B"
          },
          {
            "text": "Configure a new NAT gatewa",
            "letter": "C"
          },
          {
            "text": "Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Regio",
            "letter": "D"
          },
          {
            "text": "Modify all default routes to point to the proxy's Auto Scaling group. F. Create a new VPC for outbound traffic to the interne G. Connect the existing transit gateway to the new VP H. Configure a new NAT gatewa I. Use an AWSNetwork Firewall firewall for rule-based filterin J. Create Network Firewall endpoints in each Availability Zon K. Modify all default routes to point to the Network Firewall endpoints. L. Create an AWS Network Firewall firewall for rule-based filtering in each AWS accoun M. Modify all default routes to point to the Network Firewall firewalls in each account. N. In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filterin O. Modify all default routes to point to the proxy's Auto Scaling group.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "634733317eea",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 20
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216721"
      }
    },
    {
      "question": {
        "id": "sap-c02_29984ca19464",
        "number": 15,
        "text": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists to public subnets and private subnets that\nspan across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.\nA solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The\nsolutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.\nWhich set of additional steps should the solutions architect take to meet these requirements?",
        "options": [
          {
            "text": "Create peering connections between the egress VPC and the spoke VPC",
            "letter": "A"
          },
          {
            "text": "Configure the required routing to allow access to the internet. Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions)",
            "letter": "B"
          },
          {
            "text": "Create a transit gateway, and share it with the existing AWS account",
            "letter": "C"
          },
          {
            "text": "Attach existing VPCs to the transit gateway Configure the required routing to allow access to the internet.",
            "letter": "D"
          },
          {
            "text": "Create a transit gateway in every accoun F. Attach the NAT gateway to the transit gateway G. Configure the required routing to allow access to the internet. H. Create an AWS PrivateLink connection between the egress VPC and the spoke VPC I. Configure the required routing to allow access to the internet",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/NAT-gateway-centralized-egress-ra.pdf?d",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "29984ca19464",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 21
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216728"
      }
    },
    {
      "question": {
        "id": "sap-c02_8d8489a414f2",
        "number": 16,
        "text": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple\nNotification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.\nA solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of Cloud Formation stacks.\nTrusted access has been enabled in Organizations.\nWhat should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
        "options": [
          {
            "text": "Create a stack set in the Organizations member account",
            "letter": "A"
          },
          {
            "text": "Use service-managed permission",
            "letter": "B"
          },
          {
            "text": "Set deployment options to deploy to an organizatio",
            "letter": "C"
          },
          {
            "text": "Use CloudFormation StackSets drift detection.",
            "letter": "D"
          },
          {
            "text": "Create stacks in the Organizations member account F. Use self-service permission G. Set deploymentoptions to deploy to an organizatio H. Enable the CloudFormation StackSets automatic deployment. I. Create a stack set in the Organizations management accoun J. Use service-managed permission K. Set deployment options to deploy to the organizatio L. Enable CloudFormation StackSets automatic deployment. M. Create stacks in the Organizations management accoun N. Use service-managed permission O. Set deployment options to deploy to the organizatio P. Enable CloudFormation StackSets drift detection.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgs-manage-auto-deployment.h",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "8d8489a414f2",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 22
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216735"
      }
    },
    {
      "question": {
        "id": "sap-c02_762d8f2647ee",
        "number": 17,
        "text": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-through put. low-latency\nnetwork connections between all to the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Launch five new EC2 instances into a cluster placement grou",
            "letter": "A"
          },
          {
            "text": "Ensure that the EC2 instance type supports enhanced networking.",
            "letter": "B"
          },
          {
            "text": "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zon",
            "letter": "C"
          },
          {
            "text": "Attach an extra elastic network interface to each EC2 instance.",
            "letter": "D"
          },
          {
            "text": "Launch five new EC2 instances into a partition placement grou F. Ensure that the EC2 instance type supports enhanced networking. G. Launch five new EC2 instances into a spread placement group Attach an extra elastic network interface to each EC2 instance.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement- groups-cluster",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "762d8f2647ee",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 23
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216741"
      }
    },
    {
      "question": {
        "id": "sap-c02_fdcedbbecc60",
        "number": 18,
        "text": "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for\nMySQL database The company hosts the DNS records for the application in Amazon Route 53 A solutions architect must recommend a solution to improve\nthe resiliency of the application\nThe solution must meet the following objectives:\nâ€¢ Application tier RPO of 2 minutes. RTO of 30 minutes\nâ€¢ Database tier RPO of 5 minutes RTO of 30 minutes\nThe company does not want to make significant changes to the existing application architecture The company must ensure optimal latency after a failover\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Configure the EC2 instances to use AWS Elastic Disaster Recovery Create a cross-Region read replica for the RDS DB instance Create an ALB in a second AWS Region Create an AWS Global Accelerator endpoint and associate the endpoint with the ALBs Update DNS records to point to the Global Accelerator endpoint",
            "letter": "A"
          },
          {
            "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes Configure RDS automated backups Configure backup replication to a second AWS Region Create an ALB in the second Region Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs Update DNS records to point to the Global Accelerator endpoint",
            "letter": "B"
          },
          {
            "text": "Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance Configure backup replication to a second AWS Region Create an ALB in the Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions) second Region Configure an Amazon CloudFront distribution in front of the ALB Update DNS records to point to CloudFront",
            "letter": "C"
          },
          {
            "text": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes Create a cross-Region read replica for the RDS DB instance Create an ALB in a second AWS Region Create an AWS Global Accelerator endpoint and associate the endpoint with the ALBs",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "This option meets the RPO and RTO requirements for both the application and database tiers and uses tools like Amazon DLM and RDS automated backups to create and manage the backups. Additionally, it uses Global Accelerator to ensure low latency after failover by directing traffic to the closest healthy endpoint.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "fdcedbbecc60",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 24
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216751"
      }
    },
    {
      "question": {
        "id": "sap-c02_592d76641d72",
        "number": 19,
        "text": "A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS Organizations\nDevelopers can configure VPCs and launch Amazon EC2 instances in a single AWS Region The EC2 instances retrieve approximately 1 TB of data each day from\nAmazon S3\nThe developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3 buckets, along with\nhigh compute costs The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC infrastructure that developers deploy\nwithin the AWS accounts The company does not want this enforcement to negatively affect the speed at which the developers can perform their tasks\nWhich solution will meet these requirements MOST cost-effectively?",
        "options": [
          {
            "text": "Create SCPs to prevent developers from launching unapproved EC2 instance types Provide the developers with an AWS CloudFormation template to deploy an approved VPC configuration with S3 interface endpoints Scope the developers* IAM permissions so that the developers can launch VPC resources only with CloudFormation",
            "letter": "A"
          },
          {
            "text": "Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer accounts When the forecasted cost is 75% of the actual budget cost, send an alert to the developer teams If the actual budget cost is 100%. create a budget action to terminate the developers' EC2 instances and VPC infrastructure",
            "letter": "B"
          },
          {
            "text": "Create an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and approved EC2 instances Share the portfolio with the developer accounts Configure an AWS Service Catalog launch constraint to use an approved IAM role Scope the developers' IAM permissions to allow access only to AWS Service Catalog",
            "letter": "C"
          },
          {
            "text": "Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts If developers launch unapproved EC2 instances or if developers create VPCs without S3 gateway endpoints perform a remediation action to terminate the unapproved resources",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "This solution allows developers to quickly launch resources using pre-approved configurations and instance types, while also ensuring that the resources launched comply with the company's architectural patterns. This can help reduce data transfer and compute costs associated with the resources. Using AWS Service Catalog also allows the company to control access to the approved configurations and resources through the use of IAM roles, while also allowing developers to quickly provision resources without negatively affecting their ability to perform their tasks. Reference: AWS Service Catalog: https://aws.amazon.com/service-catalog/ AWS Service Catalog Constraints: https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints.html AWS Service Catalog Launch Constraints: https://docs.aws.amazon.com/servicecatalog/latest/adminguide/launch-constraints.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "592d76641d72",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 28
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216759"
      }
    },
    {
      "question": {
        "id": "sap-c02_3b9a6d7d554e",
        "number": 20,
        "text": "A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently does not use\nAPI keys to authorize requests. The API model is as follows: GET/posts/[postid] to get post details GET/users[userid] to get user details\nGET/comments/[commentid] to get comments details\nThe company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by marking the\ncomments appears in real time.\nWhich design should be used to reduce comment latency and improve user experience?",
        "options": [
          {
            "text": "Use edge-optimized API with Amazon CloudFront to cache API responses.",
            "letter": "A"
          },
          {
            "text": "Modify the blog application code to request GET comment[commented] every 10 seconds.",
            "letter": "B"
          },
          {
            "text": "Use AWS AppSync and leverage WebSockets to deliver comments.",
            "letter": "C"
          },
          {
            "text": "Change the concurrency limit of the Lambda functions to lower the API response time.",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://docs.aws.amazon.com/appsync/latest/devguide/graphql-overview.html AWS AppSync is a fully managed GraphQL service that allows applications to securely access, manipulate, and receive data as well as real-time updates from multiple data sources1. AWS AppSync supports GraphQL subscriptions to perform real-time operations and can push data to clients that choose to listen to specific events from the backend1. AWS AppSync uses WebSockets to establish and maintain a secure connection between the clients and the API endpoint2. Therefore, using AWS AppSync and leveraging WebSockets is a suitable design to reduce comment latency and improve user experience.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "3b9a6d7d554e",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 33
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216764"
      }
    },
    {
      "question": {
        "id": "sap-c02_974946a6d770",
        "number": 21,
        "text": "A company uses an AWS CodeCommit repository The company must store a backup copy of the data that is in the repository in a second AWS Region\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region",
            "letter": "A"
          },
          {
            "text": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule Create a cross-Region copy in the second Region",
            "letter": "B"
          },
          {
            "text": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository Use CodeBuild to clone the repository Create a zip file of the content Copy the file to an S3 bucket in the second Region",
            "letter": "C"
          },
          {
            "text": "Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository Configure the workflow to copy the snapshot to an S3 bucket in the second Region",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "AWS Backup is a fully managed service that makes it easy to centralize and automate the creation, retention, and restoration of backups across AWS services. It provides a way to schedule automatic backups for CodeCommit repositories on an hourly basis. Additionally, it also supports cross-Region replication, which allows you to copy the backups to a second Region for disaster recovery. By using AWS Backup, the company can set up an automatic and regular backup schedule for the CodeCommit repository, ensuring that the data is regularly backed up and stored in a second Region. This can provide a way to recover quickly from any disaster event that might occur. Reference: AWS Backup documentation: https://aws.amazon.com/backup/ AWS Backup for AWS CodeCommit documentation: https://aws.amazon.com/about-aws/whats-new/2020/07/aws-backup-now-supports-aws-codecommit-repositorie",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "974946a6d770",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 38
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216769"
      }
    },
    {
      "question": {
        "id": "sap-c02_27b651da8cea",
        "number": 22,
        "text": "A company has several AWS accounts. A development team is building an automation framework for cloud governance and remediation processes. The\nautomation framework uses AWS Lambda functions in a centralized account. A solutions architect must implement a least privilege permissions policy that allows\nthe Lambda functions to run in each of the company's AWS accounts.\nWhich combination of steps will meet these requirements? (Choose two.)",
        "options": [
          {
            "text": "In the centralized account, create an IAM role that has the Lambda service as a trusted entit",
            "letter": "A"
          },
          {
            "text": "Add an inline policy to assume the roles of the other AWS accounts.",
            "letter": "B"
          },
          {
            "text": "In the other AWS accounts, create an IAM role that has minimal permission",
            "letter": "C"
          },
          {
            "text": "Add the centralized account's Lambda IAM role as a trusted entity.",
            "letter": "D"
          },
          {
            "text": "In the centralized account, create an IAM role that has roles of the other accounts as trusted entities.Provide minimal permissions. F. In the other AWS accounts, create an IAM role that has permissions to assume the role of the centralized accoun G. Add the Lambda service as a trusted entity. H. In the other AWS accounts, create an IAM role that has minimal permission I. Add the Lambda service as a trusted entity.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AB",
        "explanation": "https://medium.com/@it.melnichenko/invoke-a-lambda-across-multiple-aws-accounts-8c094b2e70be",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "27b651da8cea",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 40
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 137
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216776",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_d26d28e67a36",
        "number": 23,
        "text": "A company's public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application Load Balancer\n(ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for several months.\nRecently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection attacks had\noccurred against the API and that the API service had scaled to its maximum amount.\nA solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must allow legitimate\ntraffic through and must maximize operational efficiency. Which solution meets these requirements?",
        "options": [
          {
            "text": "Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks.",
            "letter": "A"
          },
          {
            "text": "Create a new AWS WAF Bot Control implementatio",
            "letter": "B"
          },
          {
            "text": "Add a rule in the AWS WAF Bot Control managed rule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks.",
            "letter": "C"
          },
          {
            "text": "Create a new AWS WAF web AC",
            "letter": "D"
          },
          {
            "text": "Add a new rule that blocks requests that match the SQL database rule grou F. Set the web ACL to allow all other traffic that does not match those rule G. Attach the web ACL to the ALB in front of the ECS tasks. H. Create a new AWS WAF web AC I. Create a new empty IP set in AWS WA J. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP se K. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP se L. Attach the web ACL to the ALB in front of the ECS tasks.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "The company should create a new AWS WAF web ACL. The company should add a new rule that blocks requests that match the SQL database rule group. The company should set the web ACL to allow all other traffic that does not match those rules. The company should attach the web ACL to the ALB in front of the ECS tasks. This solution will meet the requirements because AWS WAF is a web application firewall that lets you monitor and control web requests that are forwarded to your web applications. You can use AWS WAF to define customizable web security rules that control which traffic can access your web applications and which traffic should be blocked1. By creating a new AWS WAF web ACL, the company can create a collection of rules that define the conditions for allowing or blocking web requests. By adding a new rule that blocks requests that match the SQL database rule group, the company can prevent SQL injection attacks from reaching the ECS API service. The SQL database rule group is a managed rule group provided by AWS that contains rules to protect against common SQL injection attack patterns2. By setting the web ACL to allow all other traffic that does not match those rules, the company can ensure that legitimate traffic can access the API service. By attaching the web ACL to the ALB in front of the ECS tasks, the company can apply the web security rules to all requests that are forwarded by the load balancer. The other options are not correct because: Creating a new AWS WAF Bot Control implementation would not prevent SQL injection attacks from reaching the ECS API service. AWS WAF Bot Control is a feature that gives you visibility and control over common and pervasive bot traffic that can consume excess resources, skew metrics, cause downtime, or perform other undesired activities. However, it does not protect against SQL injection attacks, which are malicious attempts to execute unauthorized SQL statements against your database3. Creating a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks would not prevent SQL injection attacks from reaching the ECS API service. Monitoring mode is a feature that enables you to evaluate how your rules would perform without actually blocking any requests. However, this mode does not provide any protection against attacks, as it only logs and counts requests that match your rules4. Creating a new AWS WAF web ACL and creating a new empty IP set in AWS WAF would not prevent SQL injection attacks from reaching the ECS API service. An IP set is a feature that enables you to specify a list of IP addresses or CIDR blocks that you want to allow or block based on their source IP address. However, this approach would not be effective or efficient against SQL injection attacks, as it would require constantly updating the IP set with new IP addresses of attackers, and it would not block attackers who use proxies or VPNs. Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d26d28e67a36",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 41
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216783"
      }
    },
    {
      "question": {
        "id": "sap-c02_0735e6ccc177",
        "number": 24,
        "text": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS workloads. The\ncompany has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be highly available and cannot be down\nfor longer than 10 minutes. The company needs to minimize ongoing maintenance.\nWhich solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "text": "Migrate to Amazon CloudWatch dashboard",
            "letter": "A"
          },
          {
            "text": "Recreate the dashboards to match the existing Grafana dashboard",
            "letter": "B"
          },
          {
            "text": "Use automatic dashboards where possible.",
            "letter": "C"
          },
          {
            "text": "Create an Amazon Managed Grafana workspac",
            "letter": "D"
          },
          {
            "text": "Configure a new Amazon CloudWatch data source.Export dashboards from the existing Grafana instanc F. Import the dashboards into the new workspace. G. Create an AMI that has Grafana pre-installe H. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AM I. Set the Auto Scaling group's minimum, desired, and maximum number of instances to on J. Create an Application Load Balancer that serves at least two Availability Zones. K. Configure AWS Backup to back up the EC2 instance that runs Grafana once each hou L. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "By creating an AMI that has Grafana pre-installed and storing the existing dashboards in Amazon Elastic File System (Amazon EFS) it allows for faster and more efficient scaling, and by creating an Auto Scaling group that uses the new AMI and setting the Auto Scaling group's minimum, desired, and maximum number of instances to one and creating an Application Load Balancer that serves at least two Availability Zones, it ensures high availability and minimized downtime.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "0735e6ccc177",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 44
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216789"
      }
    },
    {
      "question": {
        "id": "sap-c02_5785008da01d",
        "number": 25,
        "text": "A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic\nDisaster Recovery for this solution.\nThe company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The\ncompany does not want this solution to consume all available network bandwidth because other applications require bandwidth.\nWhich combination of steps will meet these requirements? (Select THREE.)",
        "options": [
          {
            "text": "Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.",
            "letter": "A"
          },
          {
            "text": "Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway.",
            "letter": "B"
          },
          {
            "text": "Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network.",
            "letter": "C"
          },
          {
            "text": "Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.",
            "letter": "D"
          },
          {
            "text": "During configuration of the replication servers, select the option to use private IP addresses for data replication. F. During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance's private IP address matches the source server's private IP address.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BDE",
        "explanation": "AWS Elastic Disaster Recovery (AWS DRS) is a service that minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery1. Users can set up AWS DRS on their source servers to initiate secure data replication to a staging area subnet in their AWS account, in the AWS Region they select. Users can then launch recovery instances on AWS within minutes, using the most up-to-date server state or a previous point in time. To configure a cloud backup of the application with AWS DRS, users need to create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway. A VPC is a logically isolated section of the AWS Cloud where users can launch AWS resources in a virtual network that they define2. A public subnet is a subnet that has a route to an internet gateway3. A virtual private gateway is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection4. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the internet. Users need to create at least two public subnets for redundancy and high availability. Users need to create a virtual private gateway and attach it to the VPC to enable VPN connectivity between the on-premises network and the target AWS network. Users need to create an internet gateway and attach it to the VPC to enable internet access for the replication servers. To ensure that replication traffic does not travel through the public internet, users need to create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network. AWS Direct Connect is a service that establishes a dedicated network connection from an on- premises network to one or more VPCs. A Direct Connect gateway is a globally available resource that allows users to connect multiple VPCs across different Regions to their on-premises networks using one or more Direct Connect connections. Users need to create an AWS Direct Connect connection between their on- premises network and an AWS Region. Users need to create a Direct Connect gateway and associate it with their VPC and their Direct Connect connection. To ensure that the application is not accessible from the internet, users need to select the option to use private IP addresses for data replication during configuration of the replication servers. This option configures the replication servers with private IP addresses only, without assigning any public IP addresses or Elastic IP addresses. This way, the replication servers can only communicate with other resources within the VPC or through VPN connections. Option A is incorrect because creating a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway is not necessary or cost- effective. A private subnet is a subnet that does not have a route to an internet gateway3. A NAT gateway is a highly available, managed Network Address Translation (NAT) service that enables instances in a private subnet to connect to the internet or other AWS services, but prevents the internet from initiating connections with those instances. Users do not need to create private subnets or NAT gateways for this use case, as they can use public subnets with private IP addresses for data replication. Option C is incorrect because creating an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network will not ensure that replication traffic does not travel through the public internet. A Site-to-Site VPN connection consists of two VPN tunnels between an on-premises customer Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "5785008da01d",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 47
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216795"
      }
    },
    {
      "question": {
        "id": "sap-c02_88b7a67d4957",
        "number": 26,
        "text": "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink\nis being used to provide connectivity between the client services and the logging service.\nIn each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances\nwith a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.\nWhich combination of steps should a solutions architect take to resolve this issue? (Select TWO.)",
        "options": [
          {
            "text": "Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnet",
            "letter": "A"
          },
          {
            "text": "Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.",
            "letter": "B"
          },
          {
            "text": "Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnet",
            "letter": "C"
          },
          {
            "text": "Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.",
            "letter": "D"
          },
          {
            "text": "Check the security group for the logging service running on the EC2 instances to ensure it allows Ingress from the NLB subnets. F. Check the security group for the loggia service running on EC2 instances to ensure it allows ingress from the clients. G. Check the security group for the NLB to ensure it allows ingress from the interlace endpoint subnets.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "",
        "confidence": "medium"
      },
      "metadata": {
        "content_hash": "88b7a67d4957",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 52
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216802"
      }
    },
    {
      "question": {
        "id": "sap-c02_8c207f593179",
        "number": 27,
        "text": "A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different departments in the\ncompany. The company has a Microsoft Azure Active Directory that is deployed.\nA solution architect needs to centralize billing and management of the companyâ€™s AWS accounts. The company wants to start using identify federation instead of\nmanual user management. The company also wants to use temporary credentials instead of long-lived access keys.\nWhich combination of steps will meet these requirements? (Select THREE)",
        "options": [
          {
            "text": "Create a new AWS account to serve as a management accoun",
            "letter": "A"
          },
          {
            "text": "Deploy an organization in AWS Organization",
            "letter": "B"
          },
          {
            "text": "Invite each existing AWS account to join the organizatio",
            "letter": "C"
          },
          {
            "text": "Ensure that each account accepts the invitation.",
            "letter": "D"
          },
          {
            "text": "Configure each AWS Accountâ€™s email address to be aws+<account id>@example.com so that account management email messages and invoices are sent to the same place. F. Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management accoun G. Connect IAM Identity Center to the Azure Active Director H. Configure IAM Identity Center for automatic synchronization of users and groups. I. Deploy an AWS Managed Microsoft AD directory in the management accoun J. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM). K. Create AWS IAM Identity Center (AWS Single Sign-On) permission set L. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts. M. Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "ACE",
        "explanation": "",
        "confidence": "medium"
      },
      "metadata": {
        "content_hash": "8c207f593179",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 53
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216813"
      }
    },
    {
      "question": {
        "id": "sap-c02_200699e6a164",
        "number": 28,
        "text": "A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different\nAWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth\nthroughput, and provide a consistent network experience for end users.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VP",
            "letter": "A"
          },
          {
            "text": "Create VPC peering connections that initiate from the central VPC to all other VPCs.",
            "letter": "B"
          },
          {
            "text": "Create an AWS Direct Connect connection between the on-premises data center and AW",
            "letter": "C"
          },
          {
            "text": "Provision a transit VIF, and connect it to a Direct Connect gatewa",
            "letter": "D"
          },
          {
            "text": "Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region. F. Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VP G. Use a transit gateway with dynamic routin H. Connect the transit gateway to all other VPCs. I. Create an AWS Direct Connect connection between the on-premises data center and AWS Establish an AWS Site-to-Site VPN connection between all VPCs in each Regio J. Create VPC peering connections that initiate from the central VPC to all other VPCs.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Transit GW + Direct Connect GW + Transit VIF + enabled SiteLink if two different DX locations https://aws.amazon.com/blogs/networking-and-content- delivery/introducing-aws-direct-connect-sitelink/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "200699e6a164",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 56
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216819"
      }
    },
    {
      "question": {
        "id": "sap-c02_b0ebcea8f24b",
        "number": 29,
        "text": "A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store,\nPassing Certification Exams Made Easy visit - https://www.2PassEasy.com\n\n\nWelcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps\nhttps://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions)\nretrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is\nfinished, customers can download the documents directly from Amazon S3.\nDuring the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API.\nThe server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be\navailable to the public for download within 30 minutes.\nWhich solution will meet these requirements with the LEAST amount of effort?",
        "options": [
          {
            "text": "Migrate the application to an AWS Lambda functio",
            "letter": "A"
          },
          {
            "text": "Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3.",
            "letter": "B"
          },
          {
            "text": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store.Mount the file share on an Amazon EC2 instance by using NF",
            "letter": "C"
          },
          {
            "text": "When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway.",
            "letter": "D"
          },
          {
            "text": "Configure Amazon FSx for Lustre with an import and export polic F. Link the new file system to an S3 bucke G. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS. H. Configure AWS DataSync to connect to an Amazon EC2 instanc I. Configure a task to synchronize the generated files to and from Amazon S3.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Powered by Lustre, the worldâ€™s most popular high-performance file system, FSx for Lustre offers shared storage with sub-ms latencies, up to terabytes per second of throughput, and millions of IOPS. FSx for Lustre file systems can also be linked to Amazon Simple Storage Service (S3) buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b0ebcea8f24b",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 61
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216826"
      }
    },
    {
      "question": {
        "id": "sap-c02_c26a7fd3169a",
        "number": 30,
        "text": "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has\nnodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The\ncompany needs to back up the files and retain the backups for 1 year.\nWhich solution will meet these requirements while providing the FASTEST storage performance?",
        "options": [
          {
            "text": "Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluste",
            "letter": "A"
          },
          {
            "text": "Configure the ReplicaSet to mount the file syste",
            "letter": "B"
          },
          {
            "text": "Direct the application to store files in the file syste",
            "letter": "C"
          },
          {
            "text": "Configure AWS Backup to back up and retain copies of the data for 1 year.",
            "letter": "D"
          },
          {
            "text": "Create an Amazon Elastic Block Store (Amazon EBS) volum F. Enable the EBS Multi-Attach feature.Configure the ReplicaSet to mount the EBS volum G. Direct the application to store files in the EBS volum H. Configure AWS Backup to back up and retain copies of the data for 1 year. I. Create an Amazon S3 bucke J. Configure the ReplicaSet to mount the S3 bucke K. Direct the application to store files in the S3 bucke L. Configure S3 Versioning to retain copies of the dat M. Configure an S3 Lifecycle policy to delete objects after 1 year. N. Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locall O. Use a third-party tool to back up the EKS cluster for 1 year.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "In the past, EBS can be attached only to one ec2 instance but not anymore but there are limitations like - it works only on io1/io2 instance types and many others as described here. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html EFS has shareable storage In terms of performance, Amazon EFS is optimized for workloads that require high levels of aggregate throughput and IOPS, whereas EBS is optimized for low- latency, random access I/O operations. Amazon EFS is designed to scale throughput and capacity automatically as your storage needs grow, while EBS volumes can be resized on demand.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "c26a7fd3169a",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 64
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216833"
      }
    },
    {
      "question": {
        "id": "sap-c02_a65dca3d68e0",
        "number": 31,
        "text": "A company wants to run a custom network analysis software package to inspect traffic as traffic leaves and enters a VPC. The company has deployed the solution\nby using AWS Cloud Formation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to the EC2\ninstances.\nWhenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the instance replacement\noccurs.\nWhich combination of steps will resolve this issue? {Select THREE.)",
        "options": [
          {
            "text": "Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance.",
            "letter": "A"
          },
          {
            "text": "Update the Cloud Formation template to install the Amazon CloudWatch agent on the EC2 instances.Configure the CloudWatch agent to send process metrics for the application.",
            "letter": "B"
          },
          {
            "text": "Update the Cloud Formation template to install AWS Systems Manager Agent on the EC2 instances.Configure Systems Manager Agent to send process metrics for the application.",
            "letter": "C"
          },
          {
            "text": "Create an alarm for the custom metric in Amazon CloudWatch for the failure scenario",
            "letter": "D"
          },
          {
            "text": "Configure the alarm to publish a message to an Amazon Simple Notification Service {Amazon SNS) topic. F. Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of servic G. Update the network routes to point to the replacement instance. H. In the Cloud Formation template, write a condition that updates the network routes when a replacement instance is launched.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BDE",
        "explanation": "",
        "confidence": "medium"
      },
      "metadata": {
        "content_hash": "a65dca3d68e0",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 67
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216840"
      }
    },
    {
      "question": {
        "id": "sap-c02_1c792e51a823",
        "number": 32,
        "text": "Passing Certification Exams Made Easy visit - https://www.2PassEasy.com\n\n\nWelcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps\nhttps://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions)\n- (Exam Topic 2)\nA company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application that uses AWS\nLambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.\nThe DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table. The fi-nance team\nand the marketing team have separate AWS accounts.\nWhat should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?",
        "options": [
          {
            "text": "Create an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB tabl",
            "letter": "A"
          },
          {
            "text": "Attach the SCP to the OU of the finance team.",
            "letter": "B"
          },
          {
            "text": "Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access con-trol). Establish trust with the marketing team's accoun",
            "letter": "C"
          },
          {
            "text": "In the mar-keting team's account, create an IAM role that has permissions to as-sume the IAM role in the finance team's account.",
            "letter": "D"
          },
          {
            "text": "Create a resource-based IAM policy that includes conditions for spe-cific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB tabl F. In the marketing team'saccount, create an IAM role that has permissions to access the DynamoDB table in the finance team's account. G. Create an IAM role in the finance team's account to access the Dyna-moDB tabl H. Use an IAM permissions boundary to limit the access to the specific attribute I. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.",
            "letter": "E"
          }
        ],
        "topic": 3,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "The company should create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). The company should attach the policy to the DynamoDB table. In the marketing teamâ€™s account, the company should create an IAM role that has permissions to access the DynamoDB table in the finance teamâ€™s account. This solution will meet the requirements because a resource-based IAM policy is a policy that you attach to an AWS resource (such as a DynamoDB table) to control who can access that resource and what actions they can perform on it. You can use IAM policy conditions to specify fine-grained access control for DynamoDB items and attributes. For example, you can allow or deny access to specific attributes of all items in a table by matching on attribute names1. By creating a resource-based policy that allows access to only specific attributes of the DynamoDB table and attaching it to the table, the company can restrict access to confidential data. By creating an IAM role in the marketing teamâ€™s account that has permissions to access the DynamoDB table in the finance teamâ€™s account, the company can enable cross-account access. The other options are not correct because: Creating an SCP to grant the marketing teamâ€™s AWS account access to the specific attributes of the DynamoDB table would not work because SCPs are policies that you can use with AWS Organizations to manage permissions in your organizationâ€™s accounts. SCPs do not grant permissions; instead, they specify the maximum permissions that identities in an account can have2. SCPs cannot be used to specify fine-grained access control for DynamoDB items and attributes. Creating an IAM role in the finance teamâ€™s account by using IAM policy conditions for specific DynamoDB attributes and establishing trust with the marketing teamâ€™s account would not work because IAM roles are identities that you can create in your account that have specific permissions. You can use an IAM role to delegate access to users, applications, or services that donâ€™t normally have access to your AWS resources3. However, creating an IAM role in the finance teamâ€™s account would not restrict access to specific attributes of the DynamoDB table; it would only allow cross-account access. The company would still need a resource- based policy attached to the table to enforce fine-grained access control. Creating an IAM role in the finance teamâ€™s account to access the DynamoDB table and using an IAM permissions boundary to limit the access to the specific attributes would not work because IAM permissions boundaries are policies that you use to delegate permissions management to other users. You can use permissions boundaries to limit the maximum permissions that an identity-based policy can grant to an IAM entity (user or role)4. Permissions boundaries cannot be used to specify fine-grained access control for DynamoDB items and attributes. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "1c792e51a823",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 72
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216847"
      }
    },
    {
      "question": {
        "id": "sap-c02_c7cae519a5ef",
        "number": 33,
        "text": "A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web.\napplication, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available\nacross application servers. Frontend web servers need session persistence and must scale to meet increases in traffic.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
        "options": [
          {
            "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargat",
            "letter": "A"
          },
          {
            "text": "Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tier",
            "letter": "B"
          },
          {
            "text": "Store the frontend web server session data in Amazon Simple Queue Service (Amazon SOS).",
            "letter": "C"
          },
          {
            "text": "Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session dat",
            "letter": "D"
          },
          {
            "text": "Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones. F. Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node group G. Use ReplicaSets to run the web servers and application H. Create an Amazon Elastic File System (Amazon EFS) Me syste I. Mount the EFS file system across all EKS pods to store frontend web server session data. J. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) Configure Amazon EKS to use managed node group K. Run the web servers and application as Kubernetes deployments in the EKS cluste L. Store the frontend web server session data in an Amazon DynamoDB tabl M. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "Deploying the application on Amazon EKS with managed node groups simplifies the operational overhead of managing the Kubernetes cluster. Running the web servers and application as Kubernetes deployments ensures that the desired number of pods are always running and can scale up or down as needed. Storing the frontend web server session data in an Amazon DynamoDB table provides a fast, scalable, and durable storage option that can be accessed across multiple Availability Zones. Creating an Amazon EFS volume that all applications will mount at the time of deployment allows the application to share data that is frequently accessed between the web and application tiers. References: Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "c7cae519a5ef",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 74
          },
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 117
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216854",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_434c7498b8f4",
        "number": 34,
        "text": "A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams\nlog in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's\nfinance team.\nThe company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some\nresources have been created in other Regions.\nA solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution\nalso must ensure that the company can create resources only in Regions in the United States.\nWhich combination of steps will meet these requirements in the MOST operationally efficient way? (Select THREE.)",
        "options": [
          {
            "text": "Create a new account to serve as a management accoun",
            "letter": "A"
          },
          {
            "text": "Create an Amazon S3 bucket for the finance learn Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.",
            "letter": "B"
          },
          {
            "text": "Create a new account to serve as a management accoun",
            "letter": "C"
          },
          {
            "text": "Deploy an organization in AWS Organizations with all features enable",
            "letter": "D"
          },
          {
            "text": "Invite all the existing accounts to the organizatio F. Ensure that each account accepts the invitation. G. Create an OU that includes all the development team H. Create an SCP that allows the creation of resources only in Regions that are in the United State I. Apply the SCP to the OU. J. Create an OU that includes all the development team K. Create an SCP that denies (he creation of resources in Regions that are outside the United State L. Apply the SCP to the OU. M. Create an 1AM role in the management account Attach a policy that includes permissions to view the Billing and Cost Management consol N. Allow the finance learn users to assume the rol O. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost. P. Create an 1AM role in each AWS accoun Q. Attach a policy that includes permissions to view the Billing and Cost Management consol R. Allow the finance team users to assume the role.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BCE",
        "explanation": "AWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. By creating a management account and inviting all the existing accounts to join the organization, the solutions architect can track and consolidate expenditures for all the accounts using AWS Cost Management tools such as AWS Cost Explorer and AWS Budgets. An organizational unit (OU) is a group of accounts within an organization that can be used to apply policies and simplify management. A service control policy (SCP) is a type of policy that you can use to manage permissions in your organization. By creating an OU that includes all the development teams and applying an SCP that allows the creation of resources only in Regions that are in the United States, the solutions architect can ensure that the company meets its compliance requirements and avoids unwanted charges from other Regions. An IAM role is an identity with permission policies that determine what the identity can and cannot do in AWS. By creating an IAM role in the management account and allowing the finance team users to assume it, the solutions architect can give them access to view the Billing and Cost Management console without sharing credentials or creating additional users. References: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html https://docs.aws.amazon.com/aws-cost-management/latest/userguide/what-is-costmanagement.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "434c7498b8f4",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 75
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 111
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216862",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_49bee9860917",
        "number": 35,
        "text": "A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon EC2 instances in\nthe us-east-1 Region. Artists upload photos of their work as large-size, high-resolution image files from their mobile phones to a centralized Amazon S3 bucket\ncreated in the us-east-l Region. The users in Europe are reporting slow performance for their Image uploads.\nHow can a solutions architect improve the performance of the image upload process?",
        "options": [
          {
            "text": "Redeploy the application to use S3 multipart uploads.",
            "letter": "A"
          },
          {
            "text": "Create an Amazon CloudFront distribution and point to the application as a custom origin",
            "letter": "B"
          },
          {
            "text": "Configure the buckets to use S3 Transfer Acceleration.",
            "letter": "C"
          },
          {
            "text": "Create an Auto Scaling group for the EC2 instances and create a scaling policy.",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Transfer acceleration. S3 Transfer Acceleration utilizes the Amazon CloudFront global network of edge locations to accelerate the transfer of data to and from S3 buckets. By enabling S3 Transfer Acceleration on the centralized S3 bucket, the users in Europe will experience faster uploads as their data will be routed through the closest CloudFront edge location.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "49bee9860917",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 79
          },
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 109
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 105
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216867",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_f4c318a13c72",
        "number": 36,
        "text": "A company processes environment data. The has a set up sensors to provide a continuous stream of data from different areas in a city. The data is available in\nJSON format.\nThe company wants to use an AWS solution to send the data to a database that does not require fixed schemas for storage. The data must be send in real time.\nPassing Certification Exams Made Easy visit - https://www.2PassEasy.com\n\n\nWelcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps\nhttps://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions)\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Use Amazon Kinesis Data Firehouse to send the data to Amazon Redshift.",
            "letter": "A"
          },
          {
            "text": "Use Amazon Kinesis Data streams to send the data to Amazon DynamoDB.",
            "letter": "B"
          },
          {
            "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora.",
            "letter": "C"
          },
          {
            "text": "Use Amazon Kinesis Data firehouse to send the data to Amazon Keyspaces (for Apache Cassandra).",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Amazon Kinesis Data Streams is a service that enables real-time data ingestion and processing. Amazon DynamoDB is a NoSQL database that does not require fixed schemas for storage. By using Kinesis Data Streams and DynamoDB, the company can send the JSON data to a database that can handle schemaless data in real time. References: https://docs.aws.amazon.com/streams/latest/dev/introduction.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f4c318a13c72",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 84
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216872"
      }
    },
    {
      "question": {
        "id": "sap-c02_fd27bd57711a",
        "number": 37,
        "text": "A company needs to audit the security posture of a newly acquired AWS account. The companyâ€™s data security team requires a notification only when an Amazon\nS3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data\nsecurity team's email address subscribed.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an S3 event notification on all S3 buckets for the isPublic even",
            "letter": "A"
          },
          {
            "text": "Select the SNS topic as the target for the event notifications.",
            "letter": "B"
          },
          {
            "text": "Create an analyzer in AWS Identity and Access Management Access Analyze",
            "letter": "C"
          },
          {
            "text": "Create an Amazon EventBridge rule for the event type â€œAccess Analyzer Findingâ€ with a filter for â€œisPublic: true.â€ Select the SNS topic as the EventBridge rule target.",
            "letter": "D"
          },
          {
            "text": "Create an Amazon EventBridge rule for the event type â€œBucket-Level API Call via CloudTrailâ€ with a filter for â€œPutBucketPolicy.â€ Select the SNS topic as the EventBridge rule target. F. Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rul G. Create an Amazon EventBridge rule for the event type â€œConfig Rules Re-evaluation Statusâ€ with a filter for â€œNON_COMPLIANT.â€ Select the SNS topic as the EventBridge rule target.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Access Analyzer is to assess the access policy. https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/access-control-block-public-access.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "fd27bd57711a",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 86
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 104
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216882",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_6a5521765aa8",
        "number": 38,
        "text": "A company is running an event ticketing platform on AWS and wants to optimize the platform's\ncost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for\nMySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate.\nThe platform experiences infrequent high peaks in demand. The surges in demand depend on event dates. Which solution will provide the MOST cost-effective\nsetup for the platform?",
        "options": [
          {
            "text": "Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline loa",
            "letter": "A"
          },
          {
            "text": "Scale the cluster with Spot Instances to handle peak",
            "letter": "B"
          },
          {
            "text": "Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.",
            "letter": "C"
          },
          {
            "text": "Purchase Compute Savings Plans for the predicted medium load of the EKS cluste",
            "letter": "D"
          },
          {
            "text": "Scale the cluster with On-Demand Capacity Reservations based on event dates for peak F. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base loa G. Temporarily scale out database read replicas during peaks. H. Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluste I. Scale the cluster with Spot Instances to handle peak J. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base loa K. Temporarily scale up the DB instance manually during peaks. L. Purchase Compute Savings Plans for the predicted base load of the EKS cluste M. Scale the cluster with Spot Instances to handle peak N. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base loa O. Temporarily scale up the DB instance manually during peaks.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "They all mention using spot instances and EKS based on EC2. A spot instance is not appropriate for a production server and the company is developing new application designed for AWS Fargate, which means we must plan the future cost improvement including AWS Fargate. https://aws.amazon.com/savingsplans/compute-pricing/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6a5521765aa8",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 90
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 99
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216888",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_4dc19c41b657",
        "number": 39,
        "text": "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone contains one public\nsubnet and one private subnet.\nThe service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service. The service needs\nto communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieve\napproximately 1 Â¢â€™ of data from an S3 bucket each day.\nThe company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without compromising the\nservice's security posture or increasing the time spent on ongoing operations.\nPassing Certification Exams Made Easy visit - https://www.2PassEasy.com\n\n\nWelcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps\nhttps://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions)\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Replace the NAT gateways with NAT instance",
            "letter": "A"
          },
          {
            "text": "In the VPC route table, create a route from the private subnets to the NAT instances.",
            "letter": "B"
          },
          {
            "text": "Move the EC2 instances to the public subnet",
            "letter": "C"
          },
          {
            "text": "Remove the NAT gateways.",
            "letter": "D"
          },
          {
            "text": "Set up an S3 gateway VPC endpoint in the VP F. Attach an endpoint policy to the endpoint to allow the required actions on the S3 bucket. G. Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instance H. Host the image on the EFS volume.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Create Amazon S3 gateway endpoint in the VPC and add a VPC endpoint policy. This VPC endpoint policy will have a statement that allows S3 access only via access points owned by the organization.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4dc19c41b657",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 95
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216898"
      }
    },
    {
      "question": {
        "id": "sap-c02_ee2ab02a4b5d",
        "number": 40,
        "text": "A solutions architect is auditing the security setup of an AWS Lambda function for a company. The Lambda function retrieves the latest changes from an Amazon\nAurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the\nLambda function.\nThe Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with AWS KMS\nmanaged encryption keys (SSE-KMS). The data must not travel across the internet. If any database credentials become compromised, the company needs a\nsolution that minimizes the impact of the compromise.\nWhat should the solutions architect recommend to meet these requirements?",
        "options": [
          {
            "text": "Enable IAM database authentication on the Aurora DB cluste",
            "letter": "A"
          },
          {
            "text": "Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authenticatio",
            "letter": "B"
          },
          {
            "text": "Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
            "letter": "C"
          },
          {
            "text": "Enable IAM database authentication on the Aurora DB cluste",
            "letter": "D"
          },
          {
            "text": "Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authenticatio F. Enforce HTTPS on the connection to Amazon S3 during data transfers. G. Save the database credentials in AWS Systems Manager Parameter Stor H. Set up password rotation on the credentials in Parameter Stor I. Change the IAM role for the Lambda function to allow the function to access Parameter Stor J. Modify the Lambda function to retrieve the credentials from Parameter Stor K. Deploy a gateway VPC endpoint for Amazon S3 in the VPC. L. Save the database credentials in AWS Secrets Manage M. Set up password rotation on the credentials in Secrets Manage N. Change the IAM role for the Lambda function to allow the function to access Secrets Manage O. Modify the Lambda function to retrieve the credentials Om Secrets Manage P. Enforce HTTPS on the connection to Amazon S3 during data transfers.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ee2ab02a4b5d",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 99
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 129
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216905",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_7_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_d5688c2a63ec",
        "number": 41,
        "text": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API\nGateway to use several shared libraries and custom classes.\nA solutions architect needs to simplify the deployment of the solution and optimize for code reuse. Which solution will meet these requirements?",
        "options": [
          {
            "text": "Deploy the shared libraries and custom classes into a Docker imag",
            "letter": "A"
          },
          {
            "text": "Store the image in an S3 bucket.Create a Lambda layer that uses the Docker image as the sourc",
            "letter": "B"
          },
          {
            "text": "Deploy the API's Lambda functions as Zip package",
            "letter": "C"
          },
          {
            "text": "Configure the packages to use the Lambda layer.",
            "letter": "D"
          },
          {
            "text": "Deploy the shared libraries and custom classes to a Docker imag F. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the sourc G. Deploy the API's Lambda functions as Zip package H. Configure the packages to use the Lambda layer. I. Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch typ J. Deploy the API's Lambda functions as Zip package K. Configure the packages to use the deployed container as a Lambda layer. L. Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker imag M. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Deploying the shared libraries and custom classes to a Docker image and uploading the image to Amazon Elastic Container Registry (Amazon ECR) and creating a Lambda layer that uses the Docker image as the source. Then, deploying the API's Lambda functions as Zip packages and configuring the packages to use the Lambda layer would meet the requirements for simplifying the deployment and optimizing for code reuse. A Lambda layer is a distribution mechanism for libraries, custom runtimes, and other function dependencies. It allows you to manage your in-development function code separately from your dependencies, this way you can easily update your dependencies without having to update your entire function code. By deploying the shared libraries and custom classes to a Docker image and uploading the image to Amazon Elastic Container Registry (ECR), it makes it easy to manage and version the dependencies. This way, the company can use the same version of the dependencies across different Lambda functions. By creating a Lambda layer that uses the Docker image as the source, the company can configure the API's Lambda functions to use the layer, reducing the need Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d5688c2a63ec",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 101
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 1
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216911",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_d4dcd97523cf",
        "number": 42,
        "text": "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes\nstatic content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this\napplication in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.\nA solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement\nWindows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in\ntime.\nWhich solution will meet these requirements with the LEAST management overhead?",
        "options": [
          {
            "text": "Create an Amazon Elastic File System (Amazon EFS) file shar",
            "letter": "A"
          },
          {
            "text": "Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance",
            "letter": "B"
          },
          {
            "text": "Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.",
            "letter": "C"
          },
          {
            "text": "Create a new AMI from the current EC2 instance that is runnin",
            "letter": "D"
          },
          {
            "text": "Create an Amazon FSx for Lustre file syste F. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance G. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system. H. Create an Amazon FSx for Windows File Server file syste I. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance J. Implement a user data script to install the application and mount the FSx for Windows File Server file syste K. Perform a seamless domain join to join the instance to the AD domain. L. Create a new AMI from the current EC2 instance that is runnin M. Create an Amazon Elastic File System (Amazon EFS) file syste N. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance O. Perform a seamless domain join to join the instance to the AD domain.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_join_instance.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d4dcd97523cf",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 102
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216918"
      }
    },
    {
      "question": {
        "id": "sap-c02_697798013190",
        "number": 43,
        "text": "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the\nenvironment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.\nWhile reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several large instance\ntypes account for a high proportion of the costs. The solutions architect finds out that the company's developers are launching new Amazon EC2 instances as part\nof their testing and that the developers are not using the appropriate instance types.\nThe solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create a desired-instance-type managed rule in AWS Confi",
            "letter": "A"
          },
          {
            "text": "Configure the rule with the instance types that are allowe",
            "letter": "B"
          },
          {
            "text": "Attach the rule to an event to run each time a new EC2 instance is launched.",
            "letter": "C"
          },
          {
            "text": "In the EC2 console, create a launch template that specifies the instance types that are allowe",
            "letter": "D"
          },
          {
            "text": "Assign the launch template to the developers' IAM accounts. F. Create a new IAM polic G. Specify the instance types that are allowe H. Attach the policy to an IAM group that contains the IAM accounts for the developers I. Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "This is doable with IAM policy creation to restrict users to specific instance types. Found the below article. https://blog.vizuri.com/limiting-allowed-aws-instance-type- with-iam-policy",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "697798013190",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 107
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 4
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216925",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_f469d718e556",
        "number": 44,
        "text": "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input\nfiles through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can\ntake up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with\nthe number of files rapidly declining after business hours.\nWhat is the MOST cost-effective migration recommendation?",
        "options": [
          {
            "text": "Create a queue using Amazon SQ",
            "letter": "A"
          },
          {
            "text": "Configure the existing web server to publish to the new queue.When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the file",
            "letter": "B"
          },
          {
            "text": "Store the processed files in an Amazon S3 bucket.",
            "letter": "C"
          },
          {
            "text": "Create a queue using Amazon",
            "letter": "D"
          },
          {
            "text": "Configure the existing web server to publish to the new queu F. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the file G. Store the processed files in Amazon EF H. Shut down the EC2 instance after the task is complete. Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions) I. Create a queue using Amazon M J. Configure the existing web server to publish to the new queue.When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the file K. Store the processed files in Amazon EFS. L. Create a queue using Amazon SO M. Configure the existing web server to publish to the new queu N. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the file O. Scale the EC2 instances based on the SOS queue lengt P. Store the processed files in an Amazon S3 bucket.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f469d718e556",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 111
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216932"
      }
    },
    {
      "question": {
        "id": "sap-c02_bb9a1e900e64",
        "number": 45,
        "text": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs\nRabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not\nwant to make any major changes to the application\nWhich solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "text": "Create an AMI of the web server VM Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer Set up Amazon MQ to replace the on-premises messaging queue Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend",
            "letter": "A"
          },
          {
            "text": "Create a custom AWS Lambda runtime to mimic the web server environment Create an Amazon API Gateway API to replace the front-end web servers Set up Amazon MQ to replace the on-premises messaging queue Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host theorder-processing backend",
            "letter": "B"
          },
          {
            "text": "Create an AMI of the web server VM Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer Set up Amazon MQ to replace the on-premises messaging queue Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend",
            "letter": "C"
          },
          {
            "text": "Create an AMI of the web server VM Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/about-aws/whats-new/2020/11/announcing-amazon-mq-rabbitmq/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "bb9a1e900e64",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 114
          },
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 4
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216938",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_92f134e8a936",
        "number": 46,
        "text": "A company has 10 accounts that are part of an organization in AWS Organizations AWS Config is configured in each account All accounts belong to either the\nProd OU or the NonProd OU\nThe company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic when an\nAmazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source The company's security team is subscribed to the SNS topic\nFor all accounts in the NonProd OU the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source\nWhich solution will meet this requirement with the LEAST operational overhead?",
        "options": [
          {
            "text": "Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic Deploy the updated rule to the NonProd OU",
            "letter": "A"
          },
          {
            "text": "Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU",
            "letter": "B"
          },
          {
            "text": "Configure an SCP to allow the ec2 AulhonzeSecurityGrouplngress action when the value of the aws Sourcelp condition key is not 0.0.0.0/0 Apply the SCP to the NonProd OU",
            "letter": "C"
          },
          {
            "text": "Configure an SCP to deny the ec2 AuthorizeSecurityGrouplngress action when the value of the aws Sourcelp condition key is 0.0.0.0/0 Apply the SCP to the NonProd OU",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "This solution will meet the requirement with the least operational overhead because it directly denies the creation of the security group inbound rule with 0.0.0.0/0 as the source, which is the exact requirement. Additionally, it does not require any additional steps or resources such as invoking a Lambda function or adding a Config rule. An SCP (Service Control Policy) is a policy that you can use to set fine-grained permissions for your AWS accounts within your organization. You can use SCPs to set permissions for the root user of an account and to delegate permissions to IAM users and roles in the accounts. You can use SCPs to set permissions that allow or deny access to specific services, actions, and resources. To implement this solution, you would need to create an SCP that denies the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. This SCP would then be applied to the NonProd OU. This would ensure that any security group inbound rule that includes 0.0.0.0/0 as the source will be denied, thus meeting the requirement. Reference: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_condition-keys.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "92f134e8a936",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 115
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216944"
      }
    },
    {
      "question": {
        "id": "sap-c02_1577c7105831",
        "number": 47,
        "text": "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant\nfinancial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL\ndatabase A solutions architect must design a scalable and highly available solution to meet the demand of 200000 daily users.\nWhich steps should the solutions architect take to design an appropriate solution?",
        "options": [
          {
            "text": "Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones Use an Amazon Route 53 alias record to route traffic from the company's domain to the NLB.",
            "letter": "A"
          },
          {
            "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Passing Certification Exams Made Easy visit - https://www.2PassEasy.com Welcome to download the Newest 2passeasy AWS-Certified-Solutions-Architect-Professional dumps https://www.2passeasy.com/dumps/AWS-Certified-Solutions-Architect-Professional/ (300 New Questions) Availability Zone",
            "letter": "B"
          },
          {
            "text": "The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion polic",
            "letter": "C"
          },
          {
            "text": "Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB",
            "letter": "D"
          },
          {
            "text": "Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Regio F. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions. G. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot Instances spanning three Availability Zones The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Using AWS CloudFormation to launch a stack with an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones, a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy, and an Amazon Route 53 alias record to route traffic from the companyâ€™s domain to the ALB will ensure that",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "1577c7105831",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 120
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216951"
      }
    },
    {
      "question": {
        "id": "sap-c02_a9181e585e7b",
        "number": 48,
        "text": "A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization\nin AWS Organizations. The company requires the cost lor cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS\naccounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.\nWhich actions should a solutions architect take to resolve the problem and prevent it from happening in the future? (Select THREE.)",
        "options": [
          {
            "text": "Create an AWS Config rule in each account to find resources with missing tags.",
            "letter": "A"
          },
          {
            "text": "Create an SCP in the organization with a deny action for ec2:Runlnstances if the Project tag is missing.",
            "letter": "B"
          },
          {
            "text": "Use Amazon Inspector in the organization to find resources with missing tags.",
            "letter": "C"
          },
          {
            "text": "Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.",
            "letter": "D"
          },
          {
            "text": "Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag. F. Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "ABE",
        "explanation": "https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.htm",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "a9181e585e7b",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 122
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 7
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 7
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216956",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_053d444377dc",
        "number": 49,
        "text": "A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of Amazon EC2\ninstances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC is\nlocated in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS CloudFormation to attempt to peer the application\nVPC with the shared services VPC, an error message indicates a peering failure. Which factors could cause this error? (Choose two.)",
        "options": [
          {
            "text": "The IPv4 CIDR ranges of the two VPCs overlap",
            "letter": "A"
          },
          {
            "text": "The VPCs are not in the same Region",
            "letter": "B"
          },
          {
            "text": "One or both accounts do not have access to an Internet gateway",
            "letter": "C"
          },
          {
            "text": "One of the VPCs was not shared through AWS Resource Access Manager",
            "letter": "D"
          },
          {
            "text": "The IAM role in the peer accepter account does not have the correct permissions",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AE",
        "explanation": "https://aws.amazon.com/about-aws/whats-new/2017/11/announcing-support-for-inter-region-vpc-peering/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "053d444377dc",
        "sources": [
          {
            "file": "aws-sap-c02_pro8_study_data.json",
            "question_number": 126
          },
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 8
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.216961",
        "duplicate_sources": [
          "aws-sap-c02_pro8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_99fb24147963",
        "number": 50,
        "text": "A company wants to migrate its data analytics environment from on premises to AWS The environment consists of two simple Node js applications One of the\napplications collects sensor data and loads it into a MySQL database The other application aggregates the data into reports When the aggregation jobs run. some\nof the load jobs fail to run correctly\nThe company must resolve the data loading issue The company also needs the migration to occur without interruptions or changes for the company's customers\nWhat should a solutions architect do to meet these requirements?",
        "options": [
          {
            "text": "Set up an Amazon Aurora MySQL database as a replication target for the on-premises database Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica Set up collection endpomts as AWS Lambda functions behind a Network Load Balancer (NLB). and use Amazon RDS Proxy to wnte to the Aurora MySQL database When the databases are synced disable the replication job and restart the Aurora Replica as the primary instanc",
            "letter": "A"
          },
          {
            "text": "Point the collector DNS record to the NLB.",
            "letter": "B"
          },
          {
            "text": "Set up an Amazon Aurora MySQL database Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora Move the aggregation jobs to run against the Aurora MySQL database Set up collection endpomts behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group When the databases are synced, point the collector DNS record to the ALB Disable the AWS DMS sync task after the cutover from on premises to AWS",
            "letter": "C"
          },
          {
            "text": "Set up an Amazon Aurora MySQL database Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora Create an Aurora Replica for the Aurora MySQL database and move the aggregation jobs to run against the Aurora Replica Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB) and use Amazon RDS Proxy to write to the Aurora MySQL database When the databases are synced, point the collector DNS record to the ALB Disable the AWS DMS sync task after the cutover from on premises to AWS",
            "letter": "D"
          },
          {
            "text": "Set up an Amazon Aurora MySQL database Create an Aurora Replica for the Aurora MySQL database and move the aggregation jobs to run against the Aurora Replica Set up collection endpoints as an Amazon Kinesis data stream Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database When the databases are synced disable the replication job and restart the Aurora Replica as the primary instance Point the collector DNS record to the Kinesis data stream.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66%",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "99fb24147963",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 1
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219688"
      }
    },
    {
      "question": {
        "id": "sap-c02_0ba5d59c11ed",
        "number": 51,
        "text": "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input\nfiles through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can\ntake up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with\nthe number of files rapidly declining after business hours.\nWhat is the MOST cost-effective migration recommendation?",
        "options": [
          {
            "text": "Create a queue using Amazon SQ",
            "letter": "A"
          },
          {
            "text": "Configure the existing web server to publish to the new queue.When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the file",
            "letter": "B"
          },
          {
            "text": "Store the processed files in an Amazon S3 bucket.",
            "letter": "C"
          },
          {
            "text": "Create a queue using Amazon",
            "letter": "D"
          },
          {
            "text": "Configure the existing web server to publish to the new queu F. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the file G. Store the processed files in Amazon EF H. Shut down the EC2 instance after the task is complete. I. Create a queue using Amazon M J. Configure the existing web server to publish to the new queue.When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the file K. Store the processed files in Amazon EFS. L. Create a queue using Amazon SO M. Configure the existing web server to publish to the new queu N. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the file O. Scale the EC2 instances based on the SOS queue lengt P. Store the processed files in an Amazon S3 bucket.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "0ba5d59c11ed",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 2
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 5
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219713",
        "duplicate_sources": [
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_d47c8104af9d",
        "number": 52,
        "text": "A company runs an loT platform on AWS loT sensors in various locations send data to the company's Node js API servers on Amazon EC2 instances running\nbehind an Application Load Balancer The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume\nThe number of sensors the company has deployed in the field has increased over time and is expected to grow significantly The API servers are consistently\noverloaded and RDS metrics show high write latency\nWhich of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-\nefficient? {Select TWO.)",
        "options": [
          {
            "text": "Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS",
            "letter": "A"
          },
          {
            "text": "Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "B"
          },
          {
            "text": "Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data",
            "letter": "C"
          },
          {
            "text": "Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load",
            "letter": "D"
          },
          {
            "text": "Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CE",
        "explanation": "Option C is correct because leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data resolves the issues permanently and enable growth as new sensors are provisioned. Amazon Kinesis Data Streams is a serverless streaming data service that simplifies the capture, processing, and storage of data streams at any scale. Kinesis Data Streams can handle any amount of streaming data and process data from hundreds of thousands of sources with very low latency. AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. Lambda can be triggered by Kinesis Data Streams events and process the data records in real time. Lambda can also scale automatically based on the incoming data volume. By using Kinesis Data Streams and Lambda, the company can reduce the load on the API servers and improve the performance and scalability of the data ingestion and processing layer3 Option E is correct because re-architecting the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance resolves the issues permanently and enable growth as new sensors are provisioned. Amazon DynamoDB is a fully managed key-value and document database that delivers single- digit millisecond performance at any scale. DynamoDB supports auto scaling, which automatically adjusts read and write capacity based on actual traffic patterns. DynamoDB also supports on-demand capacity mode, which instantly accommodates up to double the previous peak traffic on a table. By using DynamoDB instead of RDS MySQL DB instance, the company can eliminate high write latency and improve scalability and performance of the database tier. References: 1: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html 2: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html 3: https://docs.aws.amazon.com/streams/latest/dev/introduction.html : https://docs.aws.amazon.com/lambda/latest/dg/welcome.html : https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html : https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html :",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d47c8104af9d",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 3
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219729"
      }
    },
    {
      "question": {
        "id": "sap-c02_9612c06e04f7",
        "number": 53,
        "text": "A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a standalone\nSimple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement\nemail messages that populate customer data before the application sends the email message to the customer.\nThe company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.\nWhich solution will meet these requirements MOST cost-effectively?",
        "options": [
          {
            "text": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplac",
            "letter": "A"
          },
          {
            "text": "Store the email template in an Amazon S3 bucke",
            "letter": "B"
          },
          {
            "text": "Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the templat",
            "letter": "C"
          },
          {
            "text": "Use an SDK in the Lambda function to send the email message.",
            "letter": "D"
          },
          {
            "text": "Set up Amazon Simple Email Service (Amazon SES) to send email message F. Store the email template in an Amazon S3 bucke G. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the templat H. Use an SDK in the Lambda function to send the email message. I. Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplac J. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer dat K. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameter L. Use the AWS Marketplace SMTP server to send the email message. M. Set up Amazon Simple Email Service (Amazon SES) to send email message N. Store the email template on Amazon SES with parameters for the customer dat O. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "In this solution, the company can use Amazon SES to send email messages, which will minimize operational overhead as SES is a fully managed service that handles sending and receiving email messages. The company can store the email template on Amazon SES with parameters for the customer data and use an AWS Lambda function to call the SendTemplatedEmail API operation, passing in the customer data to replace the parameters and the email destination. This solution eliminates the need to set up and manage an SMTP server on EC2 instances, which can be costly and time-consuming. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "9612c06e04f7",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 5
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219757"
      }
    },
    {
      "question": {
        "id": "sap-c02_0fe34120860e",
        "number": 54,
        "text": "An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes\nthe recipient's signature or a photo of the package with the recipient. The driver's handheld device uploads signatures and photos through FTP to a single Amazon\nEC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. The EC2 instance\nthen adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving.\nAs the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and\nmemory issues. In response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports\nthat files are not always in the archive and that the central system is not always updated.\nA solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always\nupdated. The handheld devices cannot be modified, so the company cannot deploy a new application.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an AMI of the existing EC2 instanc",
            "letter": "A"
          },
          {
            "text": "Create an Auto Scaling group of EC2 instances behind an Application Load Balance",
            "letter": "B"
          },
          {
            "text": "Configure the Auto Scaling group to have a minimum of three instances.",
            "letter": "C"
          },
          {
            "text": "Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instanc",
            "letter": "D"
          },
          {
            "text": "Point the EC2 instance to the new path for file processing. F. Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda functio G. Configure the Lambda function to add the metadata and update the delivery system. H. Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda functio I. Configure the Lambda function to add the metadata and update the delivery system.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Using AWS Transfer Family to create an FTP server that places the files in Amazon S3 and using S3 event notifications through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function will ensure that the archive always receives the files and that the central system is always updated. This solution maximizes scalability and eliminates the need for manual intervention, such as rebooting the EC2 instance.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "0fe34120860e",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 6
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219774"
      }
    },
    {
      "question": {
        "id": "sap-c02_ab123cbc763e",
        "number": 55,
        "text": "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant\nfinancial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL\ndatabase A solutions architect must design a scalable and highly available solution to meet the demand of 200000 daily users.\nWhich steps should the solutions architect take to design an appropriate solution?",
        "options": [
          {
            "text": "Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones Use an Amazon Route 53 alias record to route traffic from the company's domain to the NLB.",
            "letter": "A"
          },
          {
            "text": "Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zone",
            "letter": "B"
          },
          {
            "text": "The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion polic",
            "letter": "C"
          },
          {
            "text": "Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB",
            "letter": "D"
          },
          {
            "text": "Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Regio F. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions. G. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot Instances spanning three Availability Zones The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Using AWS CloudFormation to launch a stack with an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones, a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy, and an Amazon Route 53 alias record to route traffic from the companyâ€™s domain to the ALB will ensure that",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ab123cbc763e",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 7
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 6
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219790",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_563feac50fcc",
        "number": 56,
        "text": "A company has 50 AWS accounts that are members of an organization in AWS Organizations Each account contains multiple VPCs The company wants to use\nAWS Transit Gateway to establish connectivity between the VPCs in each member account Each time a new member account is created, the company wants to\nautomate the process of creating a new VPC and a transit gateway attachment.\nWhich combination of steps will meet these requirements? (Select TWO)",
        "options": [
          {
            "text": "From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager",
            "letter": "A"
          },
          {
            "text": "Prom the management account, share the transit gateway with member accounts by using an AWS Organizations SCP",
            "letter": "B"
          },
          {
            "text": "Launch an AWS CloudFormation stack set from the management account that automatical^/ creates a new VPC and a VPC transit gateway attachment in a member accoun",
            "letter": "C"
          },
          {
            "text": "Associate the attachment with the transit gateway in the management account by using the transit gateway ID.",
            "letter": "D"
          },
          {
            "text": "Launch an AWS CloudFormation stack set from the management account that automatical^ creates a new VPC and a peering transit gateway attachment in a member accoun F. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role. G. From the management account, share the transit gateway with member accounts by using AWS Service Catalog",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "https://aws.amazon.com/blogs/mt/self-service-vpcs-in-aws-control-tower-using-aws-service-catalog/ https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit- gateways.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-transitgatewayattachme",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "563feac50fcc",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 9
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219829"
      }
    },
    {
      "question": {
        "id": "sap-c02_1fa8005577a8",
        "number": 57,
        "text": "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU. the company has two OUs: Research and DataOps.\nBecause of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally. EC2\ninstances that the company deploys in the DataOps OU must use a predefined list of instance types\nA solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing\nmaintenance\nWhich combination of steps will meet these requirements? (Select TWO )",
        "options": [
          {
            "text": "Create an IAM role in one account under the DataOps OU Use the ec2 Instance Type condition key in an inline policy on the role to restrict access to specific instance types.",
            "letter": "A"
          },
          {
            "text": "Create an IAM user in all accounts under the root OU Use the aws RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.",
            "letter": "B"
          },
          {
            "text": "Create an SCP Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1 Apply the SCP to the root OU.",
            "letter": "C"
          },
          {
            "text": "Create an SCP Use the ec2ReoÂ»on condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root O",
            "letter": "D"
          },
          {
            "text": "the DataOps O F. and the Research OU. G. Create an SCP Use the ec2:lnstanceType condition key to restrict access to specific instance types Apply the SCP to the DataOps OU.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CE",
        "explanation": "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.h https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_ec2.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "1fa8005577a8",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 10
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219848"
      }
    },
    {
      "question": {
        "id": "sap-c02_2ab663274426",
        "number": 58,
        "text": "A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties. The company\nruns its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files ate uploaded, they are moved to the data lake by a cron job that\nruns on the same instance. The SFTP server is reachable on DNS sftp.examWe.com through the use of Amazon Route 53.\nWhat should a solutions architect do to improve the reliability and scalability of the SFTP solution?",
        "options": [
          {
            "text": "Move the EC2 instance into an Auto Scaling grou",
            "letter": "A"
          },
          {
            "text": "Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.",
            "letter": "B"
          },
          {
            "text": "Migrate the SFTP server to AWS Transfer for SFT",
            "letter": "C"
          },
          {
            "text": "Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.",
            "letter": "D"
          },
          {
            "text": "Migrate the SFTP server to a file gateway in AWS Storage Gatewa F. Update the DNS record sflp.example.com in Route 53 to point to the file gateway endpoint. G. Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://aws.amazon.com/aws-transfer-family/faqs/ https://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-family.html https://aws.amazon.com/about-aws/whats-new/2018/11/aws-transfer-for-sftp-fully-managed-sftp-for-s3/?nc1=h_",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "2ab663274426",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 10
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 10
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219862",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_72a80e039635",
        "number": 59,
        "text": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode.\nA recent RDS database failover test caused a 40-second outage to the application A solutions architect needs to design a solution to reduce the outage time to\nless than 20 seconds.\nWhich combination of steps should the solutions architect take to meet these requirements? (Select THREE.)",
        "options": [
          {
            "text": "Use Amazon ElastiCache for Memcached in front of the database",
            "letter": "A"
          },
          {
            "text": "Use Amazon ElastiCache for Redis in front of the database.",
            "letter": "B"
          },
          {
            "text": "Use RDS Proxy in front of the database Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "C"
          },
          {
            "text": "Migrate the database to Amazon Aurora MySQL",
            "letter": "D"
          },
          {
            "text": "Create an Amazon Aurora Replica F. Create an RDS for MySQL read replica",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CDE",
        "explanation": "Migrate the database to Amazon Aurora MySQL. - Create an Amazon Aurora Replica. - Use RDS Proxy in front of the database. - These options are correct because they address the requirement of reducing the failover time to less than 20 seconds. Migrating to Amazon Aurora MySQL and creating an Aurora replica can reduce the failover time to less than 20 seconds. Aurora has a built-in, fault-tolerant storage system that can automatically detect and repair failures. Additionally, Aurora has a feature called \"Aurora Global Database\" which allows you to create read-only replicas across multiple AWS regions which can further help to reduce the failover time. Creating an Aurora replica can also help to reduce the failover time as it can take over as the primary DB instance in case of a failure. Using RDS proxy can also help to reduce the failover time as it can route the queries to the healthy DB instance, it also helps to balance the load across multiple DB instances.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "72a80e039635",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 14
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219875"
      }
    },
    {
      "question": {
        "id": "sap-c02_6e3d6247051d",
        "number": 60,
        "text": "An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS Organizations\naccount structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by procurement managers. The\nprocurement team's policy indicates that developers should be able to obtain third-party software from an approved list only and use Private Marketplace in AWS\nMarketplace to achieve this requirement . The procurement team wants administration of Private Marketplace to be restricted to a role named\nprocurement-manager-role, which could be assumed by procurement managers Other IAM users groups, roles, and account administrators in the company should\nbe denied Private Marketplace administrative access\nWhat is the MOST efficient way to design an architecture to meet these requirements?",
        "options": [
          {
            "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization Add the PowerUserAccess managed policy to the role Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.",
            "letter": "A"
          },
          {
            "text": "Create an IAM role named procurement-manager-role in all AWS accounts in the organization Add the AdministratorAccess managed policy to the role Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.",
            "letter": "B"
          },
          {
            "text": "Create an IAM role named procurement-manager-role in all the shared services accounts in the organization Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone exceptthe role named procurement-manager-role Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization.",
            "letter": "C"
          },
          {
            "text": "Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developer",
            "letter": "D"
          },
          {
            "text": "Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the rol F. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-rol G. Apply the SCP to all the shared services accounts in the organization.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. https://aws.amazon.com/blogs/awsmarketplace/controlling-access-to-a-well-architected-private-marketplace-usi This approach allows the procurement managers to assume the procurement-manager-role in shared services accounts, which have the AWSPrivateMarketplaceAdminFullAccess managed policy attached to it and can then manage the Private Marketplace. The organization root-level SCP denies the permission to administer Private Marketplace to everyone except the role named procurement-manager-role and another SCP denies the permission to create an IAM role named procurement-manager-role to everyone in the organization, ensuring that only the procurement team can assume the role and manage the Private Marketplace. This approach provides a centralized way to manage and restrict access to Private Marketplace while maintaining a high level of security.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6e3d6247051d",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 17
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 18
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219893",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_3866a5e46122",
        "number": 61,
        "text": "A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway\nauthentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.\nThe solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an authenticated user.\nWhich solution will meet these requirements with the LEAST amount of effort?",
        "options": [
          {
            "text": "Create an internal Application Load Balancer (ALB). Create a target grou",
            "letter": "A"
          },
          {
            "text": "Select the Lambda function to cal",
            "letter": "B"
          },
          {
            "text": "Use the ALB DNS name to call the API from the VPC.",
            "letter": "C"
          },
          {
            "text": "Remove the DNS entry that is associated with the API in API Gatewa",
            "letter": "D"
          },
          {
            "text": "Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zon F. Update the API in API Gateway with the CNAME recor G. Use the CNAME record to call the API from the VPC. H. Update the API endpoint from Regional to private in API Gatewa I. Create an interface VPC endpoint in the VP J. Create a resource policy, and attach it to the AP K. Use the VPC endpoint to call the API from the VPC. L. Deploy the Lambda functions inside the VP M. Provision an EC2 instance, and install an Apache server.From the Apache server, call the Lambda function N. Use the internal CNAME record of the EC2 instance to call the API from the VPC.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "This solution requires the least amount of effort as it only requires to update the API endpoint to private in API Gateway and create an interface VPC endpoint. Then create a resource policy and attach it to the API. This will make the API only accessible from the VPC and still keep the authentication mechanism intact. Reference: https://aws.amazon.com/api-gateway/features/ Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "3866a5e46122",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 19
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219907"
      }
    },
    {
      "question": {
        "id": "sap-c02_7d8ca690aa40",
        "number": 62,
        "text": "A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For\nthe next version of the application, the security engineer wants to implement the following application design changes to improve security:\nThe database must use strong, randomly generated passwords stored in a secure AWS managed service.\nThe application resources must be deployed through AWS CloudFormation.\nThe application must rotate credentials for the database every 90 days.\nA solutions architect will generate a CloudFormation template to deploy the application.\nWhich resources specified in the CloudFormation template will meet the security engineer's requirements with the LEAST amount of operational overhead?",
        "options": [
          {
            "text": "Generate the database password as a secret resource using AWS Secrets Manage",
            "letter": "A"
          },
          {
            "text": "Create an AWS Lambda function resource to rotate the database passwor",
            "letter": "B"
          },
          {
            "text": "Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.",
            "letter": "C"
          },
          {
            "text": "Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Stor",
            "letter": "D"
          },
          {
            "text": "Create an AWS Lambda function resource to rotate the database passwor F. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days. G. Generate the database password as a secret resource using AWS Secrets Manage H. Create an AWS Lambda function resource to rotate the database passwor I. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days. J. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Stor K. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://aws.amazon.com/blogs/security/how-to-securely-provide-database-credentials-to-lambda-functions-by-us https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_cloudformation.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "7d8ca690aa40",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 20
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219922"
      }
    },
    {
      "question": {
        "id": "sap-c02_6b41059131de",
        "number": 63,
        "text": "A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online\npurchases. The website stores data in an Amazon RDS for MySQL DB instance.\nWhich solution will provide the HIGHEST availability for the database?",
        "options": [
          {
            "text": "Configure automated backups on Amazon RD",
            "letter": "A"
          },
          {
            "text": "In the case of disruption, promote an automated backup to be a standalone DB instanc",
            "letter": "B"
          },
          {
            "text": "Direct database traffic to the promoted DB instanc",
            "letter": "C"
          },
          {
            "text": "Create a replacement read replica that has the promoted DB instance as its source.",
            "letter": "D"
          },
          {
            "text": "Configure global tables and read replicas on Amazon RD F. Activate the cross-Region scop G. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region. H. Configure global tables and automated backups on Amazon RD I. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region. J. Configure read replicas on Amazon RD K. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instanc L. Direct database traffic to the promoted DB instanc M. Create areplacement read replica that has the promoted DB instance as its source.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "This solution will provide the highest availability for the database, as the read replicas will allow the database to be available in multiple Regions, thus reducing the chances of disruption. Additionally, the promotion of the cross-Region read replica to become a standalone DB instance will ensure that the database is still available even if one of the Regions experiences disruptions.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6b41059131de",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 22
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219936"
      }
    },
    {
      "question": {
        "id": "sap-c02_bb0c2477a814",
        "number": 64,
        "text": "A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during the past year.\nA solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Download AWS Cost and Usage Reports for the last 12 months of S3 usag",
            "letter": "A"
          },
          {
            "text": "Review AWS Trusted Advisor recommendations for cost savings.",
            "letter": "B"
          },
          {
            "text": "Use S3 storage class analysi",
            "letter": "C"
          },
          {
            "text": "Import data trends into an Amazon QuickSight dashboard to analyze storage trends.",
            "letter": "D"
          },
          {
            "text": "Use Amazon S3 Storage Len F. Upgrade the default dashboard to include advanced metrics for storage trends. G. Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 month H. Import the csvfile to an Amazon QuickSight dashboard.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "bb0c2477a814",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 24
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219949"
      }
    },
    {
      "question": {
        "id": "sap-c02_a2bed558a8f9",
        "number": 65,
        "text": "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\ndeployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is\nregularly greater than 85%, which causes some performance bottlenecks.\nA solutions architect must mitigate the performance issues before the company launches the application to production.\nWhich solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "text": "Create a new Elastic Beanstalk applicatio",
            "letter": "A"
          },
          {
            "text": "Select a load-balanced environment typ",
            "letter": "B"
          },
          {
            "text": "Select all Availability Zone",
            "letter": "C"
          },
          {
            "text": "Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.",
            "letter": "D"
          },
          {
            "text": "Create a second Elastic Beanstalk environmen F. Apply the traffic-splitting deployment polic G. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes. H. Modify the existing environment's capacity configuration to use a load-balanced environment type.Select all Availability Zone I. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes. J. Select the Rebuild environment action with the load balancing option Select an Availability Zones Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "This solution will meet the requirements with the least operational overhead because it allows the company to modify the existing environment's capacity configuration, so it becomes a load-balanced environment type. By selecting all availability zones, the company can ensure that the application is running in multiple availability zones, which can help to improve the availability and scalability of the application. The company can also add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes, which can help to mitigate the performance issues. This solution does not require creating new Elastic Beanstalk environments or rebuilding the existing one, which reduces the operational overhead. You can refer to the AWS Elastic Beanstalk documentation for more information on how to use this service: https://aws.amazon.com/elasticbeanstalk/ You can refer to the AWS documentation for more information on how to use autoscaling: https://aws.amazon.com/autoscaling/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "a2bed558a8f9",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 27
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219963"
      }
    },
    {
      "question": {
        "id": "sap-c02_75327232b405",
        "number": 66,
        "text": "A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS\naccount. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS\nsecurity best practices.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS accoun",
            "letter": "A"
          },
          {
            "text": "Assign a unique external ID to the resource policy.",
            "letter": "B"
          },
          {
            "text": "In the company's AWS account create an IAM role that trusts the auditors' AWS account Create an IAM policy that has the required permission",
            "letter": "C"
          },
          {
            "text": "Attach the policy to the rol",
            "letter": "D"
          },
          {
            "text": "Assign a unique external ID to the role's trust policy. F. In the company's AWS account, create an IAM use G. Attach the required IAM policies to the IAM user.Create API access keys for the IAM use H. Share the access keys with the auditors. I. In the company's AWS account, create an IAM group that has the required permissions Create an IAM user in the company s account for each audito J. Add the IAM users to the IAM group.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "This solution will allow the external auditors to have read-only access to the company's AWS account while being compliant with AWS security best practices. By creating an IAM role, which is a secure and flexible way of granting access to AWS resources, and trusting the auditors' AWS account, the company can ensure that the auditors only have the permissions that are required for their role and nothing more. Assigning a unique external ID to the role's trust policy, it will ensure that only the auditors' AWS account can assume the role. Reference: AWS IAM Roles documentation: https://aws.amazon.com/iam/features/roles/ AWS IAM Best practices: https://aws.amazon.com/iam/security-best-practices/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "75327232b405",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 32
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 22
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219976",
        "duplicate_sources": [
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_6fcbfa92c75b",
        "number": 67,
        "text": "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the\ncompany's AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all\nthe company's AWS accounts.\nThe company's security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Configure AWS Single Sign-On (AWS SSO) to connect to Active Directory by using SAML 2.0.Enable automatic provisioning by using the System for Cross- domain Identity Management (SCIM) v2.0 protoco",
            "letter": "A"
          },
          {
            "text": "Grant access to the AWS accounts by using attribute-based access controls (ABACs).",
            "letter": "B"
          },
          {
            "text": "Configure AWS Single Sign-On (AWS SSO) by using AWS SSO as an identity sourc",
            "letter": "C"
          },
          {
            "text": "Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protoco",
            "letter": "D"
          },
          {
            "text": "Grant access to the AWS accounts by using AWS SSO permission sets. F. In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provide G. Provision IAM users that are mapped to the federated user H. Grant access that corresponds to appropriate groups in Active Director I. Grant access to the required AWS accounts by using cross-account IAM users. J. In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provide K. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Director L. Grant access to the required AWS accounts by using cross-account IAM roles.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6fcbfa92c75b",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 37
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.219991"
      }
    },
    {
      "question": {
        "id": "sap-c02_3f793bc0ba20",
        "number": 68,
        "text": "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer.\nThe company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate\ntraffic to the application.\nHow should a solutions architect configure the web ACLs to meet these requirements?",
        "options": [
          {
            "text": "Set the action of the web ACL rules to Coun",
            "letter": "A"
          },
          {
            "text": "Enable AWS WAF logging Analyze the requests for false positives Modify the rules to avoid any false positive Over time change the action of the web ACL rules from Count to Block.",
            "letter": "B"
          },
          {
            "text": "Use only rate-based rules in the web ACL",
            "letter": "C"
          },
          {
            "text": "and set the throttle limit as high as possible Temporarily block all requests that exceed the limi",
            "letter": "D"
          },
          {
            "text": "Define nested rules to narrow the scope of the rate tracking. F. Set the action o' the web ACL rules to Bloc G. Use only AWS managed rule groups in the web ACLs Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs. H. Use only custom rule groups in the web ACL I. and set the action to Allow Enable AWS WAF logging Analyze the requests tor false positives Modify the rules to avoid any false positive Over time, change the action of the web ACL rules from Allow to Block.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/waf-analyze-count-action-rules/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "3f793bc0ba20",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 42
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 32
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220005",
        "duplicate_sources": [
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_3a1e8306af95",
        "number": 69,
        "text": "A company runs its application in the eu-west-1 Region and has one account for each of its environments development, testing, and production All the\nenvironments are running 24 hours a day 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases The databases are\nbetween 500 GB and 800 GB in size\nThe development team and testing team work on business days during business hours, but the production environment operates 24 hours a day. 7 days a week.\nThe company wants to reduce costs AH resources are tagged with an environment tag with either development, testing, or production as the key. What should a\nsolutions architect do to reduce costs with the LEAST operational effort?",
        "options": [
          {
            "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that runs once every day Configure the rule to invoke one AWS Lambda function that starts or stops instances based on the tag day and time.",
            "letter": "A"
          },
          {
            "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that runs every business day in the evenin",
            "letter": "B"
          },
          {
            "text": "Configure the rule to invoke an AWS Lambda function that stops instances based on thetag-Create a second EventBridge (CloudWatch Events) rule that runs every business day in the morning Configure the second rule to invoke another Lambda function that starts instances based on the tag",
            "letter": "C"
          },
          {
            "text": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule that runs every business day in the evening Configure the rule to invoke an AWS Lambda function that terminates instances based on the tag Create a second EventBridge (CloudWatch Events) rule that runs every business day in the morning Configure the second rule to invoke another Lambda function that restores the instances from their last backup based on the tag.",
            "letter": "D"
          },
          {
            "text": "Create an Amazon EventBridge rule that runs every hou F. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the ta G. day, and time.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag will reduce costs with the least operational effort. This approach allows for instances to be stopped during non-business hours when they are not in use, reducing the costs associated with running them. It also allows for instances to be started again in the morning when the development and testing teams need to use them.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "3a1e8306af95",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 46
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220022"
      }
    },
    {
      "question": {
        "id": "sap-c02_9c278ba85172",
        "number": 70,
        "text": "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2. Amazon S3 and Amazon DynamoDB. The\ndevelopers account resides In a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account:\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\nWhen this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy. What should the solutions\narchitect do to eliminate the developers' ability to use services outside the scope of this policy?",
        "options": [
          {
            "text": "Create an explicit deny statement for each AWS service that should be constrained",
            "letter": "A"
          },
          {
            "text": "Remove the Full AWS Access SCP from the developer account's OU",
            "letter": "B"
          },
          {
            "text": "Modify the Full AWS Access SCP to explicitly deny all services",
            "letter": "C"
          },
          {
            "text": "Add an explicit deny statement using a wildcard to the end of the SCP",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "9c278ba85172",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 49
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220035"
      }
    },
    {
      "question": {
        "id": "sap-c02_6386f890f138",
        "number": 71,
        "text": "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients connect to the API\nby using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IP addresses of all\nthe EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to traffic. The app has not been able to keep up with the traffic.\nA solutions architect needs to implement a solution so that the app can handle the new and varying load. Which solution will meet these requirements with the\nLEAST operational overhead?",
        "options": [
          {
            "text": "Separate the API into individual AWS Lambda function",
            "letter": "A"
          },
          {
            "text": "Configure an Amazon API Gateway REST API with Lambda integration for the backen",
            "letter": "B"
          },
          {
            "text": "Update the Route 53 record to point to the API Gateway API.",
            "letter": "C"
          },
          {
            "text": "Containerize the API logi",
            "letter": "D"
          },
          {
            "text": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluste F. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingres G. Update the Route 53 record to point to the Kubernetes ingress. H. Create an Auto Scaling grou I. Place all the EC2 instances in the Auto Scaling grou J. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilizatio K. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record. L. Create an Application Load Balancer (ALB) in front of the AP M. Move the EC2 instances to private subnets in the VP N. Add the EC2 instances as targets for the AL O. Update the Route 53 record to point to the ALB.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "By breaking down the monolithic API into individual Lambda functions and using API Gateway to handle the incoming requests, the solution can automatically scale to handle the new and varying load without the need for manual scaling actions. Additionally, this option will automatically handle the traffic without the need of having EC2 instances running all the time and only pay for the number of requests and the duration of the execution of the Lambda function. By updating the Route 53 record to point to the API Gateway, the solution can handle the traffic and also it will direct the traffic to the correct endpoint.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6386f890f138",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 50
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 39
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220049",
        "duplicate_sources": [
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_1326518b0b04",
        "number": 72,
        "text": "A company is using multiple AWS accounts The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A The company's applications\nand databases are running in Account B.\nA solutions architect win deploy a two-net application In a new VPC To simplify the configuration, the db.example com CNAME record set tor the Amazon RDS\nendpoint was created in a private hosted zone for Amazon Route 53.\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\nDuring deployment, the application failed to start. Troubleshooting revealed that db.example com is not resolvable on the Amazon EC2 instance The solutions\narchitect confirmed that the record set was created correctly in Route 53.\nWhich combination of steps should the solutions architect take to resolve this issue? (Select TWO )",
        "options": [
          {
            "text": "Deploy the database on a separate EC2 instance in the new VPC Create a record set for the instance's private IP in the private hosted zone",
            "letter": "A"
          },
          {
            "text": "Use SSH to connect to the application tier EC2 instance Add an RDS endpoint IP address to the/eto/resolv.conf file",
            "letter": "B"
          },
          {
            "text": "Create an authorization lo associate the private hosted zone in Account A with the new VPC In Account B",
            "letter": "C"
          },
          {
            "text": "Create a private hosted zone for the example.com domain m Account B Configure Route 53 replication between AWS accounts",
            "letter": "D"
          },
          {
            "text": "Associate a new VPC in Account B with a hosted zone in Account F. Delete the association authorization In Account A.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CE",
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "1326518b0b04",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 51
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220064"
      }
    },
    {
      "question": {
        "id": "sap-c02_0e8ad5d67149",
        "number": 73,
        "text": "A company with global offices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. The company's on-premises network uses the\nconnection to communicate with the company's resources in the AWS Cloud. The connection has a single private virtual interface that connects to a single VPC.\nA solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must provide connectivity\nto other Regions through the same pair of Direct Connect connections as the company expands into other Regions.\nWhich solution meets these requirements?",
        "options": [
          {
            "text": "Provision a Direct Connect gatewa",
            "letter": "A"
          },
          {
            "text": "Delete the existing private virtual interface from the existing connectio",
            "letter": "B"
          },
          {
            "text": "Create the second Direct Connect connectio",
            "letter": "C"
          },
          {
            "text": "Create a new private virtual interlace on each connection, and connect both private victual interfaces to the Direct Connect gatewa",
            "letter": "D"
          },
          {
            "text": "Connect the Direct Connect gateway to the single VPC. F. Keep the existing private virtual interfac G. Create the second Direct Connect connectio H. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC. I. Keep the existing private virtual interfac J. Create the second Direct Connect connectio K. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC. L. Provision a transit gatewa M. Delete the existing private virtual interface from the existing connection.Create the second Direct Connect connectio N. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gatewa O. Associate the transit gateway with the single VPC.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any Region and access it from all other Regions. The following describe scenarios where you can use a Direct Connect gateway. https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "0e8ad5d67149",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 56
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220095"
      }
    },
    {
      "question": {
        "id": "sap-c02_ba3ddc90ccb8",
        "number": 74,
        "text": "A company has its cloud infrastructure on AWS A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one\nAWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts\nWhat should the solutions architect do to meet these requirements?",
        "options": [
          {
            "text": "Use AWS CloudFormation templates Add IAM policies to control the various accounts Deploy the templates across the multiple Regions",
            "letter": "A"
          },
          {
            "text": "Use AWS Organizations Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts",
            "letter": "B"
          },
          {
            "text": "Use AWS Organizations and AWS CloudFormation StackSets Deploy a CloudFormation template from an account that has the necessary IAM permissions",
            "letter": "C"
          },
          {
            "text": "Use nested stacks with AWS CloudFormation templates Change the Region by using nested stacks",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-orga AWS Organizations allows the management of multiple AWS accounts as a single entity and AWS CloudFormation StackSets allows creating, updating, and deleting stacks across multiple accounts and regions in an organization. This solution allows creating a single CloudFormation template that can be deployed across multiple accounts and regions, and also allows for the management of access and permissions for the different accounts through the use of IAM roles and policies in the management account.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ba3ddc90ccb8",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 57
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220109"
      }
    },
    {
      "question": {
        "id": "sap-c02_726c715acb56",
        "number": 75,
        "text": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced\nan issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an alias for every new deployed version of the Lambda functio",
            "letter": "A"
          },
          {
            "text": "Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.",
            "letter": "B"
          },
          {
            "text": "Deploy the application into a new CloudFormation stac",
            "letter": "C"
          },
          {
            "text": "Use an Amazon Route 53 weighted routing policy to distribute the load.",
            "letter": "D"
          },
          {
            "text": "Create a version for every new deployed Lambda functio F. Use the AWS CLIupdate-function-configuration command with the routing-config parameter to distribute the load. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) G. Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias- https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "726c715acb56",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 59
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220122"
      }
    },
    {
      "question": {
        "id": "sap-c02_09ea6c0932a5",
        "number": 76,
        "text": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The\ncompany uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is\ncached. Amazon Route 53 is used to host all public zones.\nAfter an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are\nreturned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.\nWhile the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.\nWhich combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
        "options": [
          {
            "text": "Create an Amazon S3 bucke",
            "letter": "A"
          },
          {
            "text": "Configure the S3 bucket to host a static webpag",
            "letter": "B"
          },
          {
            "text": "Upload the custom error pages to Amazon S3.",
            "letter": "C"
          },
          {
            "text": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.",
            "letter": "D"
          },
          {
            "text": "Modify the existing Amazon Route 53 records by adding health check F. Configure a fallback target if the health check fail G. Modify DNS records to point to a publicly accessible webpage. H. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server. I. Add a custom error response by configuring a CloudFront custom error pag J. Modify DNS records to point to a publicly accessible web page.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CE",
        "explanation": "\"Save your custom error pages in a location that is accessible to CloudFront. We recommend that you store them in an Amazon S3 bucket, and that you donâ€™t store them in the same place as the rest of your website or applicationâ€™s content. If you store the custom error pages on the same origin as your website or application, and the origin starts to return 5xx errors, CloudFront canâ€™t get the custom error pages because the origin server is unavailable.\" https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.htm",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "09ea6c0932a5",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 63
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220138"
      }
    },
    {
      "question": {
        "id": "sap-c02_2ec8ae483b39",
        "number": 77,
        "text": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect\ncreated a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.\nThe solutions architect created the following IAM policy and attached it to an IAM role:\nDuring tests, me solutions architect was able to successfully get existing test objects m the S3 bucket However, attempts to upload a new object resulted in an\nerror message. The error message stated that me action was forbidden.\nWhich action must me solutions architect add to the IAM policy to meet all the requirements?",
        "options": [
          {
            "text": "Kms:GenerateDataKey",
            "letter": "A"
          },
          {
            "text": "KmsGetKeyPolpcy",
            "letter": "B"
          },
          {
            "text": "kmsGetPubKKey",
            "letter": "C"
          },
          {
            "text": "kms:SKjn",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/ \"An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\" This error message indicates that your IAM user or role needs Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "2ec8ae483b39",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 67
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220150"
      }
    },
    {
      "question": {
        "id": "sap-c02_4693bed3dd4d",
        "number": 78,
        "text": "A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An Administrator created the following SCP and has\nattached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:\nDevelopers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the Administrator address this problem?",
        "options": [
          {
            "text": "Add s3:CreateBucket with â‚¬Allowâ‚¬ effect to the SCP.",
            "letter": "A"
          },
          {
            "text": "Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.",
            "letter": "B"
          },
          {
            "text": "Instruct the Developers to add Amazon S3 permissions to their IAM entities.",
            "letter": "C"
          },
          {
            "text": "Remove the SCP from account 1111-1111-1111.",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "However A's explanation is incorrect - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html \"SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions.\" SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4693bed3dd4d",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 68
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 64
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220165",
        "duplicate_sources": [
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_b375c206ec6c",
        "number": 79,
        "text": "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic\nto the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. The company's\nsecurity team is concerned about how to integrate the security tool with AWS technology.\nThe company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a dedicated\nVPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in real time and must not\naffect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region.\nWhich combination of steps should the solutions architect take to meet these requirements? (Select TWO.)",
        "options": [
          {
            "text": "Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC.",
            "letter": "A"
          },
          {
            "text": "Deploy the web application behind a Network Load Balancer.",
            "letter": "B"
          },
          {
            "text": "Deploy an Application Load Balancer in front of the security tool instances.",
            "letter": "C"
          },
          {
            "text": "Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool.",
            "letter": "D"
          },
          {
            "text": "Provision a transit gateway to facilitate communication between VPCs.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AD",
        "explanation": "Option A, Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC, allows the company to use its existing security tool while still running it within the AWS environment. This ensures that all packets coming in and out of the VPC are inspected by the security tool in real time. Option D, Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool, allows for high availability within an AWS Region. By provisioning a Gateway Load Balancer for each Availability Zone, the traffic is redirected to the security tool in the event of any failures or outages. This ensures that the security tool is always available to inspect the traffic, even in the event of a failure.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b375c206ec6c",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 69
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 72
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220180",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_2a144dd107e1",
        "number": 80,
        "text": "A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN The company Is hosting Internal\napplications with VPCs in multiple AWS accounts Currently the applications are accessible from the company's on-premises office network through an AWS Site-to-\nSite VPN connection The VPC in the company's main AWS account has peering connections established with VPCs in other AWS accounts.\nA solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home\nWhat is the MOST cost-effective solution that meets these requirements?\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
        "options": [
          {
            "text": "Create a Client VPN endpoint in each AWS account Configure required routing that allows access to internal applications",
            "letter": "A"
          },
          {
            "text": "Create a Client VPN endpoint in the mam AWS account Configure required routing that allows access to internal applications",
            "letter": "B"
          },
          {
            "text": "Create a Client VPN endpoint in the main AWS account Provision a transit gateway that is connected to each AWS account Configure required routing that allows access to internal applications",
            "letter": "C"
          },
          {
            "text": "Create a Client VPN endpoint in the mam AWS account Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site VPN",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "2a144dd107e1",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 73
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220194"
      }
    },
    {
      "question": {
        "id": "sap-c02_40daf2f404df",
        "number": 81,
        "text": "A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The application is\ndesigned to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durable\nlocation where it can be retrieved with low latency. The data is ephemeral and the company is required to store the data for 120 days only, after which the data can\nbe deleted.\nThe solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.\nWhich storage strategy is the MOST cost-effective and meets the design requirements?",
        "options": [
          {
            "text": "Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieva",
            "letter": "A"
          },
          {
            "text": "Configure a lifecycle policy to delete data older than 120 days.",
            "letter": "B"
          },
          {
            "text": "Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scal",
            "letter": "C"
          },
          {
            "text": "Configure the DynamoOB Time to Live (TTL) feature to delete records older than 120 days.",
            "letter": "D"
          },
          {
            "text": "Design the application to store each incoming record in a single table in an Amazon RDS MySQL databas F. Run a nightly cron job that executes a query to delete any records older than 120 days. G. Design the application to batch incoming records before writing them to an Amazon S3 bucke H. Updatethe metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the dat I. Configure a lifecycle policy to delete the data after 120 days.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "DynamoDB with TTL, cheaper for sustained throughput of small items + suited for fast retrievals. S3 cheaper for storage only, much higher costs with writes. RDS not designed for this use case.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "40daf2f404df",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 74
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 85
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220209",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_f6d58725b822",
        "number": 82,
        "text": "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online\nvisitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53.\nA solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.\nWhich combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.)",
        "options": [
          {
            "text": "Create a dynamic webpage that runs on an Amazon EC2 instanc",
            "letter": "A"
          },
          {
            "text": "Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.",
            "letter": "B"
          },
          {
            "text": "Create an Application Load Balancer that includes HTTP and HTTPS listeners.",
            "letter": "C"
          },
          {
            "text": "Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.",
            "letter": "D"
          },
          {
            "text": "Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function. F. Create an Amazon CloudFront distributio G. Deploy a Lambda@Edge function. H. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CE",
        "explanation": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works-tutorial.ht",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f6d58725b822",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 75
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220245"
      }
    },
    {
      "question": {
        "id": "sap-c02_9a5da3757dfb",
        "number": 83,
        "text": "The company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be able to create\nreports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and\nCost Management solution that provides these cost reports.\nWhich combination of actions will meet these requirements? (Select THREE.)",
        "options": [
          {
            "text": "Activate the user-defined cost allocation tags that represent the application and the team.",
            "letter": "A"
          },
          {
            "text": "Activate the AWS generated cost allocation tags that represent the application and the team.",
            "letter": "B"
          },
          {
            "text": "Create a cost category for each application in Billing and Cost Management.",
            "letter": "C"
          },
          {
            "text": "Activate IAM access to Billing and Cost Management.",
            "letter": "D"
          },
          {
            "text": "Create a cost budget. F. Enable Cost Explorer.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html https://aws.amazon.com/premiumsupport/knowledge-center/cost- explorer-analyze-spending-and-usage/ https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html https://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html The best combination of actions to meet the companyâ€™s requirements is Options A, C, and F. Option A involves activating the user-defined cost allocation tags that represent the application and the team. This will allow the company to assign costs to different applications or teams, and will allow them to be tracked in the monthly AWS bill. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "9a5da3757dfb",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 77
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 96
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220273",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_cf9ddd3df6c1",
        "number": 84,
        "text": "A weather service provides high-resolution weather maps from a web application hosted on AWS in the\neu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon\nCloudFront.\nThe company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time\nto time.\nWhich combination of steps will resolve the us-east-1 performance issues? (Choose two.)",
        "options": [
          {
            "text": "Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.",
            "letter": "A"
          },
          {
            "text": "Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.",
            "letter": "B"
          },
          {
            "text": "Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.",
            "letter": "C"
          },
          {
            "text": "Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.",
            "letter": "D"
          },
          {
            "text": "Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distributio F. Use Lambda@Edge to modify requests from North America to use the new origin.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BD",
        "explanation": "https://aws.amazon.com/about-aws/whats-new/2016/04/transfer-files-into-amazon-s3-up-to-300-percent-faster/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "cf9ddd3df6c1",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 81
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220282"
      }
    },
    {
      "question": {
        "id": "sap-c02_1399dcb6e522",
        "number": 85,
        "text": "A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and\nmodifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a\ndedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts\nautonomously.\nA solutions architect needs to enforce the new process in the most secure way possible.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
        "options": [
          {
            "text": "Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.",
            "letter": "A"
          },
          {
            "text": "Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
            "letter": "B"
          },
          {
            "text": "In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
            "letter": "C"
          },
          {
            "text": "Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances actio",
            "letter": "D"
          },
          {
            "text": "Attach the SCP to each OU of the organization. F. Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AD",
        "explanation": "All features â€“ The default feature set that is available to AWS Organizations. It includes all the functionality of consolidated billing, plus advanced features that give you more control over accounts in your organization. For example, when all features are enabled the management account of the organization has full control over what member accounts can do. The management account can apply SCPs to restrict the services and actions that users (including the root user) and roles in an account can access. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html#feature-set",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "1399dcb6e522",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 84
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 98
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220289",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_d4c2152a2135",
        "number": 86,
        "text": "A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company\nwants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted\nat rest in the companyâ€™s production OU.\nWhich solution will meet this requirement?",
        "options": [
          {
            "text": "Turn on mandatory guardrails in AWS Control Towe",
            "letter": "A"
          },
          {
            "text": "Apply the mandatory guardrails to the production OU.",
            "letter": "B"
          },
          {
            "text": "Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Towe",
            "letter": "C"
          },
          {
            "text": "Apply the guardrail to the production OU.",
            "letter": "D"
          },
          {
            "text": "Use AWS Config to create a new mandatory guardrai F. Apply the rule to all accounts in the production OU. G. Create a custom SCP in AWS Control Towe H. Apply the SCP to the production OU.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "AWS Control Tower provides a set of \"strongly recommended guardrails\" that can be enabled to implement governance and policy enforcement. One of these guardrails is \"Encrypt Amazon RDS instances\" which will detect RDS DB instances that are not encrypted at rest. By enabling this guardrail and applying it to the production OU, the company will be able to enforce encryption for RDS instances in the production environment. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d4c2152a2135",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 86
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 108
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220299",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_3256d19a94a4",
        "number": 87,
        "text": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user\nexperience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The\ncompany manages public DNS internally. The company wants to make the application available through an apex domain.\nWhich solution will meet these requirements with the LEAST effort?",
        "options": [
          {
            "text": "Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the AL",
            "letter": "A"
          },
          {
            "text": "Use a geolocation routing policy to route traffic based on user location.",
            "letter": "B"
          },
          {
            "text": "Place a Network Load Balancer (NLB) in front of the AL",
            "letter": "C"
          },
          {
            "text": "Migrate public DNS to Amazon Route 53.Create a CNAME record for the apex domain to point to the NLB's static IP addres",
            "letter": "D"
          },
          {
            "text": "Use a geolocation routing policy to route traffic based on user location. F. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Region G. Use the accelerator's static IP address to create a record in public DNS for the apex domain. H. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions.Configure a Lambda function to route traffic to application deployments by using the round robin metho I. Create CNAME records for the apex domain to point to the API's URL.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "AWS Global Accelerator is a service that directs traffic to optimal endpoints (in this case, the Application Load Balancer) based on the health of the endpoints and network routing. It allows you to create an accelerator that directs traffic to multiple endpoint groups, one for each Region where the application is deployed. The accelerator uses the AWS global network to optimize the traffic routing to the healthy endpoint. By using Global Accelerator, the company can use a single static IP address for the apex domain, and traffic will be directed to the optimal endpoint based on the user's location, without the need for additional load balancers or routing policies. Reference: AWS Global Accelerator documentation: https://aws.amazon.com/global-accelerator/ Routing User Traffic to the Optimal AWS Region using Global Accelerator documentation: https://aws.amazon.com/blogs/networking-and-content-delivery/routing-user-traffic-to-the-optimal-aws-region-u",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "3256d19a94a4",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 88
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220307"
      }
    },
    {
      "question": {
        "id": "sap-c02_4c59400b9a12",
        "number": 88,
        "text": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in\nAWS Organizations.\nAdministrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed Administrators also must have the ability to\nautomatically update and remediate noncompliant AWS WAF rules in all accounts\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
        "options": [
          {
            "text": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organizatio",
            "letter": "A"
          },
          {
            "text": "Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage Update the parameter as needed to add or remove accounts or OUs Use an Amazon EventBridge (Amazon CloudWatch Events) rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account",
            "letter": "B"
          },
          {
            "text": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rule",
            "letter": "C"
          },
          {
            "text": "Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.",
            "letter": "D"
          },
          {
            "text": "Create AWS WAF rules in the management account of the organization Use AWS Lambda environment variables to store account numbers and OUs to manage Update environment variables as needed to add or remove accounts or OUs Create cross-account IAM roles in member accounts Assume the rotes by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts. F. Use AWS Control Tower to manage AWS WAF rules across accounts in the organization Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage Update AWS KMS as needed to add or remove accounts or OUs Create IAM users in member accounts Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/ In this solution, AWS Firewall Manager is used to manage AWS WAF rules across accounts in the organization. An AWS Systems Manager Parameter Store parameter is used to store account numbers and OUs to manage. This parameter can be updated as needed to add or remove accounts or OUs. An Amazon EventBridge rule is used to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account. This solution allows for easy management of AWS WAF rules across multiple accounts with minimal operational overhead",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4c59400b9a12",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 92
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220316"
      }
    },
    {
      "question": {
        "id": "sap-c02_84adb2013ddf",
        "number": 89,
        "text": "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has\nmapped its departments to the following OUs: Finance. Sales. Human Resources <HR). Marketing, and Operations. Each OU has multiple AWS accounts, one for\neach environment within a department. These environments are development, test, pre-production, and production.\nThe HR department is releasing a new system thai will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in\nits production AWS account. The HR department will install the new application on this account. The HR department wants to make sure that other departments\ncannot share the Rl discounts.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "In the AWS Billing and Cost Management console for the HR department's production account, turn off R1 sharing.",
            "letter": "A"
          },
          {
            "text": "Remove the HR department's production AWS account from the organizatio",
            "letter": "B"
          },
          {
            "text": "Add the account to the consolidating billing configuration only.",
            "letter": "C"
          },
          {
            "text": "In the AWS Billing and Cost Management console, use the organization's management account to turn off R1 sharing for the HR department's production AWS account.",
            "letter": "D"
          },
          {
            "text": "Create an SCP in the organization to restrict access to the RI F. Apply the SCP to the OUs of the other departments.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "You can use the management account of the organization in AWS Billing and Cost Management console to turn off RI sharing for the HR department's production AWS account. This will prevent other departments from sharing the RI discounts and ensure that only the HR department can use the RIs purchased in their production account.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "84adb2013ddf",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 96
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220322"
      }
    },
    {
      "question": {
        "id": "sap-c02_19d9053bf16f",
        "number": 90,
        "text": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instance. The application is a Linux binary, and the source code cannot be\nmodified. The application is single-threaded, uses 2 GB of RAM. and is highly CPU intensive The application is scheduled to run every 4 hours and runs for up to\n20 minutes A solutions architect wants to revise the architecture for the solution.\nWhich strategy should the solutions architect use?",
        "options": [
          {
            "text": "Use AWS Lambda to run the applicatio",
            "letter": "A"
          },
          {
            "text": "Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.",
            "letter": "B"
          },
          {
            "text": "Use AWS Batch to run the applicatio",
            "letter": "C"
          },
          {
            "text": "Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.",
            "letter": "D"
          },
          {
            "text": "Use AWS Fargate to run the applicatio F. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours. G. Use Amazon EC2 Spot Instances to run the applicatio H. Use AWS CodeDeploy to deploy and run the application every 4 hours.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "step function could run a scheduled task when triggered by eventbrige, but why would you add that layer of complexity just to run aws batch when you could directly invoke it through eventbridge. The link provided - https://aws.amazon.com/pt/blogs/compute/orchestrating-high-performance-computing-with-aws-step- functions- makes sense only for HPC, this is a single instance that needs to be run",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "19d9053bf16f",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 100
          },
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 118
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220328",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_4dac6c759e7d",
        "number": 91,
        "text": "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront\ndistribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain\nname that visitors use when they access the application.\nEach week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants\nvisitors to receive an informational message instead of a\nCloudFront error message.\nA solutions architect creates an Amazon S3 bucket as the first step in the process.\nWhich combination of steps should the solutions architect take next to meet the requirements? (Choose three.)",
        "options": [
          {
            "text": "Upload static informational content to the S3 bucket.",
            "letter": "A"
          },
          {
            "text": "Create a new CloudFront distributio",
            "letter": "B"
          },
          {
            "text": "Set the S3 bucket as the origin.",
            "letter": "C"
          },
          {
            "text": "Set the S3 bucket as a second origin in the original CloudFront distributio",
            "letter": "D"
          },
          {
            "text": "Configure the distribution and the S3 bucket to use an origin access identity (OAI). F. During the weekly maintenance, edit the default cache behavior to use the S3 origi G. Revert the change when the maintenance is complete. H. During the weekly maintenance, create a cache behavior for the S3 origin on the new distributio I. Set the path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance is complete. J. During the weekly maintenance, configure Elastic Beanstalk to serve traffic from the S3 bucket.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "ACD",
        "explanation": "The company wants to serve static content from an S3 bucket during the maintenance period. To do this, the following steps are required: Upload static informational content to the S3 bucket. This will provide the source of the content that will be served to the visitors. Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI). This will allow CloudFront to access the S3 bucket securely and prevent public access to the bucket. During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete. This will redirect all web requests to the S3 bucket instead of the Elastic Beanstalk domain name. The other options are not correct because: Creating a new CloudFront distribution is not necessary and would require changing the alternate domain name configuration. Creating a cache behavior for the S3 origin on a new distribution would not work because the visitors would still access the original distribution using the alternate domain name. Configuring Elastic Beanstalk to serve traffic from the S3 bucket is not possible and would not achieve the desired result. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4dac6c759e7d",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 103
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220334"
      }
    },
    {
      "question": {
        "id": "sap-c02_395985ba46a3",
        "number": 92,
        "text": "A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single\nAWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider\napplications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the\nother organization.\nData transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must\nrecommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment.\nWhich guidelines meet these requirements? (Select TWO.)\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
        "options": [
          {
            "text": "Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization.",
            "letter": "A"
          },
          {
            "text": "Place the service provider applications and the service consumer applications in AWS accounts in the same organization.",
            "letter": "B"
          },
          {
            "text": "Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.",
            "letter": "C"
          },
          {
            "text": "Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.",
            "letter": "D"
          },
          {
            "text": "Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CD",
        "explanation": "Cross-zone load balancing enables traffic to be distributed evenly across all registered instances in all enabled Availability Zones. However, this also increases data transfer charges between Availability Zones. By turning off cross-zone load balancing, the service provider applications can reduce inter-Availability Zone data transfer costs. Similarly, by using the Availability Zone-specific endpoint service, the service consumer applications can ensure that they connect to the nearest service provider application in the same Availability Zone, avoiding cross-Availability Zone data transfer charges. References: https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html#vpce-interface-dns",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "395985ba46a3",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 104
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220341"
      }
    },
    {
      "question": {
        "id": "sap-c02_72efcaf65691",
        "number": 93,
        "text": "A company processes environment data. The has a set up sensors to provide a continuous stream of data from different areas in a city. The data is available in\nJSON format.\nThe company wants to use an AWS solution to send the data to a database that does not require fixed schemas for storage. The data must be send in real time.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Use Amazon Kinesis Data Firehouse to send the data to Amazon Redshift.",
            "letter": "A"
          },
          {
            "text": "Use Amazon Kinesis Data streams to send the data to Amazon DynamoDB.",
            "letter": "B"
          },
          {
            "text": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora.",
            "letter": "C"
          },
          {
            "text": "Use Amazon Kinesis Data firehouse to send the data to Amazon Keyspaces (for Apache Cassandra).",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Amazon Kinesis Data Streams is a service that enables real-time data ingestion and processing. Amazon DynamoDB is a NoSQL database that does not require fixed schemas for storage. By using Kinesis Data Streams and DynamoDB, the company can send the JSON data to a database that can handle schemaless data in real time. References: https://docs.aws.amazon.com/streams/latest/dev/introduction.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "72efcaf65691",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 113
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220350"
      }
    },
    {
      "question": {
        "id": "sap-c02_b8cbb003eef3",
        "number": 94,
        "text": "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB\nis associated with an AWS WAF web ACL.\nThe website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access\nlogs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks.\nWhich solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "text": "Create an Amazon CloudWatch alarm that monitors server acces",
            "letter": "A"
          },
          {
            "text": "Set a threshold based on access by IP addres",
            "letter": "B"
          },
          {
            "text": "Configure an alarm action that adds the IP address to the web ACLâ€™s deny list.",
            "letter": "C"
          },
          {
            "text": "Deploy AWS Shield Advanced in addition to AWS WA",
            "letter": "D"
          },
          {
            "text": "Add the ALB as a protected resource. F. Create an Amazon CloudWatch alarm that monitors user IP addresse G. Set a threshold based on access by IP addres H. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application serverâ€™s subnet route table for any IP addresses that activate the alarm. I. Inspect access logs to find a pattern of IP addresses that launched the attack J. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "\"The AWS WAF API supports security automation such as blacklisting IP addresses that exceed request limits, which can be useful for mitigating HTTP flood attacks.\" > https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b8cbb003eef3",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 122
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220364"
      }
    },
    {
      "question": {
        "id": "sap-c02_8d87d27e5a5c",
        "number": 95,
        "text": "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that\nthe instances are optimized based on CPU, memory, and network metrics.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
        "options": [
          {
            "text": "Purchase AWS Business Support or AWS Enterprise Support for the account.",
            "letter": "A"
          },
          {
            "text": "Turn on AWS Trusted Advisor and review any â€œLow Utilization Amazon EC2 Instancesâ€ recommendations.",
            "letter": "B"
          },
          {
            "text": "Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.",
            "letter": "C"
          },
          {
            "text": "Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.",
            "letter": "D"
          },
          {
            "text": "Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BD",
        "explanation": "AWS Trusted Advisor is a service that provides real-time guidance to help users provision their resources following AWS best practices1. One of the Trusted Advisor checks is â€œLow Utilization Amazon EC2 Instancesâ€, which identifies EC2 instances that appear to be underutilized based on CPU, network I/O, and disk I/O metrics1. This check can help users optimize the cost and size of their EC2 instances by recommending smaller or more appropriate instance types. AWS Compute Optimizer is a service that analyzes the configuration and utilization metrics of AWS resources and generates optimization recommendations to reduce the cost and improve the performance of workloads2. Compute Optimizer supports four types of AWS resources: EC2 instances, EBS volumes, ECS services on AWS Fargate, and Lambda functions2. For EC2 instances, Compute Optimizer evaluates the vCPUs, memory, storage, and other specifications, as well as the CPU utilization, network in and out, disk read and write, and other utilization metrics of currently running instances3. It then recommends optimal instance types based on price-performance trade-offs. Option A is incorrect because purchasing AWS Business Support or AWS Enterprise Support for the account will not directly help with cost-optimization and sizing of EC2 instances. However, these support plans do provide access to more Trusted Advisor checks than the basic support plan1. Option C is incorrect because installing the Amazon CloudWatch agent and configuring memory metric collection on the EC2 instances will not provide any optimization recommendations by itself. However, memory metrics can be used by Compute Optimizer to enhance its recommendations if enabled3. Option E is incorrect because creating an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest will not help with cost-optimization and sizing of EC2 instances. Savings Plans are a flexible pricing model that offer lower prices on Amazon EC2 usage in exchange for a commitment to a consistent amount of usage for a 1- or 3-year term4. Savings Plans do not affect the configuration or utilization of EC2 instances.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "8d87d27e5a5c",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 123
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220370"
      }
    },
    {
      "question": {
        "id": "sap-c02_22c39f0237a8",
        "number": 96,
        "text": "A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different departments in the\ncompany. The company has a Microsoft Azure Active Directory that is deployed.\nA solution architect needs to centralize billing and management of the companyâ€™s AWS accounts. The company wants to start using identify federation instead of\nmanual user management. The company also wants to use temporary credentials instead of long-lived access keys.\nWhich combination of steps will meet these requirements? (Select THREE)",
        "options": [
          {
            "text": "Create a new AWS account to serve as a management accoun Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "A"
          },
          {
            "text": "Deploy an organization in AWS Organization",
            "letter": "B"
          },
          {
            "text": "Invite each existing AWS account to join the organizatio",
            "letter": "C"
          },
          {
            "text": "Ensure that each account accepts the invitation.",
            "letter": "D"
          },
          {
            "text": "Configure each AWS Accountâ€™s email address to be aws+<account id>@example.com so that account management email messages and invoices are sent to the same place. F. Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management accoun G. Connect IAM Identity Center to the Azure Active Director H. Configure IAM Identity Center for automatic synchronization of users and groups. I. Deploy an AWS Managed Microsoft AD directory in the management accoun J. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM). K. Create AWS IAM Identity Center (AWS Single Sign-On) permission set L. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts. M. Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "ACE",
        "explanation": "",
        "confidence": "medium"
      },
      "metadata": {
        "content_hash": "22c39f0237a8",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 126
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220381"
      }
    },
    {
      "question": {
        "id": "sap-c02_a39cae2834f0",
        "number": 97,
        "text": "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The\nnumber of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS Keys (SSE-\nKMS).\nA solutions architect reviews the company's monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from\nAmazon S3. The solutions architect needs to optimize costs with minimal changes to the application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "text": "Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption typ",
            "letter": "A"
          },
          {
            "text": "Copy the existing objects to the new S3 bucke",
            "letter": "B"
          },
          {
            "text": "Specify SSE-C.",
            "letter": "C"
          },
          {
            "text": "Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption typ",
            "letter": "D"
          },
          {
            "text": "Use S3 Batch Operations to copy the existing objects to the new S3 bucke F. Specify SSE-S3. G. Use AWS CloudHSM to store the encryption key H. Create a new S3 bucke I. Use S3 Batch Operations to copy the existing objects to the new S3 bucke J. Encrypt the objects by using the keys from CloudHSM. K. Use the S3 Intelligent-Tiering storage class for the S3 bucke L. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "To reduce the volume of Amazon S3 calls to AWS KMS, use Amazon S3 bucket keys, which are protected encryption keys that are reused for a limited time in Amazon S3. Bucket keys can reduce costs for AWS KMS requests by up to 99%. You can configure a bucket key for all objects in an Amazon S3 bucket, or for a specific object in an Amazon S3 bucket. https://docs.aws.amazon.com/fr_fr/kms/latest/developerguide/services-s3.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "a39cae2834f0",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 131
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 131
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220388",
        "duplicate_sources": [
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_028928509302",
        "number": 98,
        "text": "A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2\ninstances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct Connect connection to the data center from the\nRegion that is closest to the data center.\nThe company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center\nalso must have access to AWS public services.\nWhich combination of steps will meet these requirements with the LEAST cost? (Select TWO.)",
        "options": [
          {
            "text": "Create a Direct Connect gateway in the Region that is closest to the data cente",
            "letter": "A"
          },
          {
            "text": "Attach the Direct Connect connection to the Direct Connect gatewa",
            "letter": "B"
          },
          {
            "text": "Use the",
            "letter": "C"
          },
          {
            "text": "Direct Connect gateway to connect the VPCs in the other two Regions.",
            "letter": "D"
          },
          {
            "text": "Set up additional Direct Connect connections from the on-premises data center to the other two Regions. F. Create a private VI G. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions. H. Create a public VI I. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions. J. Use VPC peering to establish a connection between the VPCs across the Region K. Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AE",
        "explanation": "A Direct Connect gateway allows you to connect multiple VPCs across different Regions to a Direct Connect connection1. A public VIF allows you to access AWS public services such as EC21. A Site-to-Site VPN connection over the public VIF provides encryption and redundancy for the traffic between the on-premises data center and the VPCs2. This solution is cheaper than setting up additional Direct Connect connections or using a private VIF with VPC peering.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "028928509302",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 134
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 132
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220394",
        "duplicate_sources": [
          "sap-c02_8_study_data.json",
          "sap-c02_6_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_d6b3f110689c",
        "number": 99,
        "text": "A company plans to migrate a three-tiered web application from an on-premises data center to AWS The company developed the Ui by using server-side\nJavaScript libraries The business logic and API tier uses a Python-based web framework The data tier runs on a MySQL database\nThe company custom built the application to meet business requirements The company does not want to\nre-architect the application The company needs a solution to replatform the application to AWS with the least possible amount of development The solution needs\nto be highly available and must reduce operational overhead\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Deploy the UI to a static website on Amazon S3 Use Amazon CloudFront to deliver the website Build the business logic in a Docker image Store the image in AmazonElastic Container Registry (Amazon ECR) Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the website with an Application Load Balancer in front Deploy the datalayer to an Amazon Aurora MySQL DB cluster",
            "letter": "A"
          },
          {
            "text": "Build the UI and business logic in Docker images Store the images in Amazon Elastic Container Registry (Amazon ECR) Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the UI and business logic applications with an Application Load Balancer in front Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance",
            "letter": "B"
          },
          {
            "text": "Deploy the UI to a static website on Amazon S3 Use Amazon CloudFront to deliver the website Convert the business logic to AWS Lambda functions Integrate the functions with Amazon API Gateway Deploy the data layer to an Amazon Aurora MySQL DB cluster",
            "letter": "C"
          },
          {
            "text": "Build the UI and business logic in Docker images Store the images in Amazon Elastic Container Registry (Amazon ECR) Use Amazon Elastic Kubernetes Service(Amazon EKS) with Fargate profiles to host the UI and business logic Use AWS Database Migration Service (AWS DMS) to migrate the data layer to Amazon DynamoDB",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "This solution utilizes Amazon S3 and CloudFront to deploy the UI as a static website, which can be done with minimal development effort. The business logic and API tier can be containerized in a Docker image and stored in Amazon Elastic Container Registry (ECR) and run on Amazon Elastic Container Service (ECS) with the Fargate launch type, which allows the application to be highly available with minimal operational overhead. The data layer can be deployed on an Amazon Aurora MySQL DB cluster which is a fully managed relational database service. Amazon Aurora provides high availability and performance for the data layer without the need for managing the underlying infrastructure.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d6b3f110689c",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 139
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220401"
      }
    },
    {
      "question": {
        "id": "sap-c02_8ff5a949fff4",
        "number": 100,
        "text": "A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a separate member\naccount for development and a separate member account for production. Consolidated billing is linked to the management account. In the management account, a\nsolutions architect needs to create an 1AM user that can stop or terminate resources in both member accounts.\nWhich solution will meet this requirement?",
        "options": [
          {
            "text": "Create an IAM user and a cross-account role in the management accoun",
            "letter": "A"
          },
          {
            "text": "Configure the cross-account role with least privilege access to the member accounts.",
            "letter": "B"
          },
          {
            "text": "Create an IAM user in each member accoun",
            "letter": "C"
          },
          {
            "text": "In the management account, create a cross-account role that has least privilege acces",
            "letter": "D"
          },
          {
            "text": "Grant the IAM users access to the cross-account role by using a trust policy. F. Create an IAM user in the management accoun G. In the member accounts, create an IAM group that has least privilege acces H. Add the IAM user from the management account to each IAM group in the member accounts. I. Create an IAM user in the management accoun J. In the member accounts, create cross-account roles that have least privilege acces K. Grant the IAM user access to the roles by using a trust policy.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "Cross account role should be created in destination(member) account. The role has trust entity to master account.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "8ff5a949fff4",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 143
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220407"
      }
    },
    {
      "question": {
        "id": "sap-c02_888a5c98feb1",
        "number": 101,
        "text": "A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data\ntransformations and aggregations. After the company performs E TL processes, the company stores the results in Amazon Redshift tables. The company sells this\ndata to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using\nFTP. The number of data customers has grown significantly. Management of the data customers has become difficult.\nThe company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the\nidentities of the customers before the company shares data.\nThe customers also need access to the most recent data when the company publishes the data. Which solution will meet these requirements with the LEAST\noperational overhead?",
        "options": [
          {
            "text": "Use AWS Data Exchange for APIs to share data with customer",
            "letter": "A"
          },
          {
            "text": "Configure subscription verification In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshif",
            "letter": "B"
          },
          {
            "text": "Require the data customers to subscribe to the data product In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift",
            "letter": "C"
          },
          {
            "text": "cluste",
            "letter": "D"
          },
          {
            "text": "Configure subscription verificatio F. Require the data customers to subscribe to the data product. G. Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodicall H. Use AWS Data Exchange for S3 to share data with customers. I. Configure subscription verificatio J. Require the data customers to subscribe to the data product Publish the Amazon Redshift data to an Open Data on AWS Data Exchang K. Require the customers to subscribe to the data product in AWS Data Exchang L. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "The company should download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically and use AWS Data Exchange for S3 to share data with customers. The company should configure subscription verification and require the data customers to subscribe to the data product. This solution will meet the requirements with the least operational overhead because AWS Data Exchange for S3 is a feature that enables data subscribers to access third-party data files directly from data providersâ€™ Amazon S3 buckets. Subscribers can easily use these files for their data analysis with AWS services without needing to create or Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "888a5c98feb1",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 148
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220414"
      }
    },
    {
      "question": {
        "id": "sap-c02_c96833847514",
        "number": 102,
        "text": "A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and an Amazon\nAurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already deployed in two Regions.\nCompany policy states that critical applications must have application tier components and data tier components deployed across two Regions. The RTO and RPO\nmust be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant with company policy.\nWhich combination of steps will meet these requirements? (Choose two.)",
        "options": [
          {
            "text": "Add another Region to the Aurora MySQL DB cluster",
            "letter": "A"
          },
          {
            "text": "Add another Region to each table in the Aurora MySQL DB cluster",
            "letter": "B"
          },
          {
            "text": "Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster",
            "letter": "C"
          },
          {
            "text": "Convert the existing DynamoDB table to a global table by adding another Region to its configuration",
            "letter": "D"
          },
          {
            "text": "Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AD",
        "explanation": "The company should use Amazon Aurora global database and Amazon DynamoDB global table to deploy the data tier components across two Regions. Amazon Aurora global database is a feature that allows a single Aurora database to span multiple AWS Regions, enabling low-latency global reads and fast recovery from Region-wide outages1. Amazon DynamoDB global table is a feature that allows a single DynamoDB table to span multiple AWS Regions, enabling low-latency global reads and writes and fast recovery from Region-wide outages2. References: https://aws.amazon.com/rds/aurora/global-database/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html https://aws.amazon.com/route53/application-recovery-controller/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "c96833847514",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 150
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220420"
      }
    },
    {
      "question": {
        "id": "sap-c02_8b3a790f519c",
        "number": 103,
        "text": "A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service (Amazon ECS) on a set\nof Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.\nThe company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the new image\nversion receives a unique tag.\nThe company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically delete new image\ntags that have Critical or High severity findings. The solution also must notify the development team when such a deletion occurs.\nWhich solution meets these requirements?",
        "options": [
          {
            "text": "Configure scan on push on the repository Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity finding",
            "letter": "A"
          },
          {
            "text": "Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS).",
            "letter": "B"
          },
          {
            "text": "Configure scan on push on the repository Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queu",
            "letter": "C"
          },
          {
            "text": "Invoke an AWS Lambda function when a new message is added to the SQS queu",
            "letter": "D"
          },
          {
            "text": "Use the Lambda function to delete the image tag for images that have Critical or High seventy finding F. Notify the development team by using Amazon Simple Email Service (Amazon SES). G. Schedule an AWS Lambda function to start a manual image scan every hou H. Configure Amazon EventBridge to invoke another Lambda function when a scan is complet I. Use the second Lambda function to delete the image tag for images that have Critical or High severity finding J. Notify the development team by using Amazon Simple Notification Service (Amazon SNS). K. Configure periodic image scan on the repositor L. Configure scan results to be added lo an Amazon Simple Queue Service (Amazon SQS) queu M. Invoke an AWS Step Functions state machine when a new message is added to the SQS queu Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) N. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity finding O. Notify the development team by using Amazon Simple Email Service (Amazon SES).",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr-eventbridge.html \"Activating an AWS Step Functions state machine\" https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "8b3a790f519c",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 152
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220428"
      }
    },
    {
      "question": {
        "id": "sap-c02_ab2700a2562d",
        "number": 104,
        "text": "A company has an on-premises Microsoft SOL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to\nmore robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS.\nWhich solution meets these requirements MOST cost-effectively?",
        "options": [
          {
            "text": "Create a new S3 bucke",
            "letter": "A"
          },
          {
            "text": "Deploy an AWS Storage Gateway file gateway within the VPC that Is connected to the Direct Connect connectio",
            "letter": "B"
          },
          {
            "text": "Create a new SMB file shar",
            "letter": "C"
          },
          {
            "text": "Write nightly database exports to the new SMB file share.",
            "letter": "D"
          },
          {
            "text": "Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connectio F. Create a new SMB file shar G. Write nightly database exports to an SMB file share on the Amazon FSx file syste H. Enable nightly backups. I. Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connectio J. Create a new SMB file shar K. Write nightly database exports to an SMB file share on the Amazon FSx file syste L. Enable nightly backups. M. Create a new S3 bucke N. Deploy an AWS Storage Gateway volume gateway within the VPC that Is connected to the Direct Connect connectio O. Create a new SMB file shar P. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://docs.aws.amazon.com/filegateway/latest/files3/CreatingAnSMBFileShare.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ab2700a2562d",
        "sources": [
          {
            "file": "sap-c02_6_study_data.json",
            "question_number": 157
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.220433"
      }
    },
    {
      "question": {
        "id": "sap-c02_3e64b8f8ea32",
        "number": 105,
        "text": "A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud\nspending with the head of each business unit. The company uses AWS Organizations lo manage the separate AWS accounts for each business unit. The existing\ntagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business\nunit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold.\nWhich solution is the MOST cost-effective way to meet these requirements?",
        "options": [
          {
            "text": "Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owne",
            "letter": "A"
          },
          {
            "text": "Add each business unit to an Amazon SNS topic for each aler",
            "letter": "B"
          },
          {
            "text": "Use Cost Explorer in each account to create monthly reports for each business unit.",
            "letter": "C"
          },
          {
            "text": "Configure AWS Budgets in the organization's master account and configure budget alerts that are grouped by application, environment, and owne",
            "letter": "D"
          },
          {
            "text": "Add each business unit to an Amazon SNS topic for each aler F. Use Cost Explorer in the organization's master account to create monthly reports for each business unit. G. Configure AWS Budgets in each account and configure budget alerts lhat are grouped by application, environment, and owne H. Add each business unit to an Amazon SNS topic for each aler I. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each businessunit. J. Enable AWS Cost and Usage Reports in the organization's master account and configure reports grouped by application, environment, and owne K. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit's email list.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Configure AWS Budgets in the organizationâ‚¬â„¢s master account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organizationâ‚¬â„¢s master account to create monthly reports for each business unit. https://aws.amazon.com/about-aws/whats-new/2019/07/introducing-aws-budgets-reports/#:~:text=AWS%20Bud",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "3e64b8f8ea32",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 1
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223305"
      }
    },
    {
      "question": {
        "id": "sap-c02_0373955577f8",
        "number": 106,
        "text": "An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the connections to the\ndatabase and could not re-establish the connections. After a restart of the application, the application re-established the connections.\nA solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an Amazon Aurora MySQL Serverless v1 DB instanc",
            "letter": "A"
          },
          {
            "text": "Migrate the RDS DB instance to the Aurora Serverless v1 DB instanc",
            "letter": "B"
          },
          {
            "text": "Update the connection settings in the application to point to the Aurora reader endpoint.",
            "letter": "C"
          },
          {
            "text": "Create an RDS prox",
            "letter": "D"
          },
          {
            "text": "Configure the existing RDS endpoint as a targe F. Update the connection settings in the application to point to the RDS proxy endpoint. G. Create a two-node Amazon Aurora MySQL DB cluste H. Migrate the RDS DB instance to the Aurora DB cluste I. Create an RDS prox J. Configure the existing RDS endpoint as a targe K. Update the connection settings in the application to point to the RDS proxy endpoint. L. Create an Amazon S3 bucke M. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon Athena to use the S3 bucket as a data stor N. Install the latest Open Database Connectivity (ODBC) driver for the applicatio O. Update the connection settings in the application to point to the Athena endpoint",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Amazon RDS Proxy is a fully managed database proxy service for Amazon Relational Database Service (RDS) that makes applications more scalable, resilient, and secure. It allows applications to pool and share connections to an RDS database, which can help reduce database connection overhead, improve scalability, and provide automatic failover and high availability.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "0373955577f8",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 2
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 2
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223319",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_f06a92700e31",
        "number": 107,
        "text": "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input\nfiles through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can\ntake up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with\nthe number of files rapidly declining after business hours.\nWhat is the MOST cost-effective migration recommendation?",
        "options": [
          {
            "text": "Create a queue using Amazon SQ",
            "letter": "A"
          },
          {
            "text": "Configure the existing web server to publish to the new queue.When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the file",
            "letter": "B"
          },
          {
            "text": "Store the processed files in an Amazon S3 bucket.",
            "letter": "C"
          },
          {
            "text": "Create a queue using Amazon",
            "letter": "D"
          },
          {
            "text": "Configure the existing web server to publish to the new queu F. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the file G. Store the processed files in Amazon EF H. Shut down the EC2 instance after the task is complete. I. Create a queue using Amazon M J. Configure the existing web server to publish to the new queue.When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the file K. Store the processed files in Amazon EFS. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) L. Create a queue using Amazon SO M. Configure the existing web server to publish to the new queu N. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the file O. Scale the EC2 instances based on the SOS queue lengt P. Store the processed files in an Amazon S3 bucket.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f06a92700e31",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 3
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223327"
      }
    },
    {
      "question": {
        "id": "sap-c02_d6904fcda95f",
        "number": 108,
        "text": "A company has migrated its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web\napplication. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and an\nAmazon RDS for PostgreSQL database.\nWhen forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team member then logs in\nand processes each form. The team member performs data validation on the form and extracts relevant data before entering the information into another system\nthat uses an API.\nA solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction, minimize time to market, and\nminimize long-term operational overhead.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Develop custom libraries to perform optical character recognition (OCR) on the form",
            "letter": "A"
          },
          {
            "text": "Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tie",
            "letter": "B"
          },
          {
            "text": "Use this tier to process the forms when forms are uploade",
            "letter": "C"
          },
          {
            "text": "Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB tabl",
            "letter": "D"
          },
          {
            "text": "Submit the data to the target system's AP F. Host the new application tier on EC2 instances. G. Extend the system with an application tier that uses AWS Step Functions and AWS Lambd H. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploade I. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tie J. Submit the data to the target system's API. K. Host a new application tier on EC2 instance L. Use this tier to call endpoints that host artificial intelligence and machine learning (Al/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the form M. Store the output in Amazon ElastiCach N. Parse this output by extracting the data that is required within the application tie O. Submit the data to the target system's API. P. Extend the system with an application tier that uses AWS Step Functions and AWS Lambd Q. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploade R. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tie S. Submit the data to the target system's API.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API. This solution meets the requirements of accurate form extraction, minimal time to market, and minimal long-term operational overhead. Amazon Textract and Amazon Comprehend are fully managed and serverless services that can perform OCR and extract relevant data from the forms, which eliminates the need to develop custom libraries or train and host models. Using AWS Step Functions and Lambda allows for easy automation of the process and the ability to scale as needed.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d6904fcda95f",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 4
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223335"
      }
    },
    {
      "question": {
        "id": "sap-c02_76a4819dae0a",
        "number": 109,
        "text": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit\nGateway for VPC connectivity across accounts.\nIn an AWS application account, the company's application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's\ndatabase administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database\nadministrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed in the application account.\nThe application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing\nthe secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A\nsolutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the\nsecrets.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA accoun",
            "letter": "A"
          },
          {
            "text": "In the DBA account, create an IAM role that is named DBA-Admi",
            "letter": "B"
          },
          {
            "text": "Grant the role the required permissions to access the shared secret",
            "letter": "C"
          },
          {
            "text": "Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
            "letter": "D"
          },
          {
            "text": "In the application account, create an IAM role that is named DBA-Secre F. Grant the role the required permissions to access the secret G. In the DBA account, create an IAM role that is named DBA-Admi H. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application accoun I. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets. J. In the DBA account, create an IAM role that is named DBA-Admi K. Grant the role the required permissions to access the secrets and the default AWS managed key in the application accoun L. In the application account, attach resource-based policies to the key to allow access from the DBA accoun M. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) N. In the DBA account, create an IAM role that is named DBA-Admi O. Grant the role the required permissions to access the secrets in the application accoun P. Attach an SCP to the application account to allow access to the secrets from the DBA accoun Q. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "Option B is correct because creating an IAM role in the application account that has permissions to access the secrets and creating an IAM role in the DBA account that has permissions to assume the role in the application account eliminates the need to manually share the secrets. This approach uses cross-account IAM roles to grant access to the secrets in the application account. The database administrators can assume the role in the application account from their EC2 instance in the DBA account and retrieve the secrets without having to store them locally or share them manually2 References: 1: https://docs.aws.amazon.com/ram/latest/userguide/what-is.html 2: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html 3: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html : https://docs.aws.amazon.com/secretsmanager/latest/userguide/tutorials_basic.html : https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "76a4819dae0a",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 5
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223343"
      }
    },
    {
      "question": {
        "id": "sap-c02_4cd4f628f612",
        "number": 110,
        "text": "A company has 50 AWS accounts that are members of an organization in AWS Organizations Each account contains multiple VPCs The company wants to use\nAWS Transit Gateway to establish connectivity between the VPCs in each member account Each time a new member account is created, the company wants to\nautomate the process of creating a new VPC and a transit gateway attachment.\nWhich combination of steps will meet these requirements? (Select TWO)",
        "options": [
          {
            "text": "From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager",
            "letter": "A"
          },
          {
            "text": "Prom the management account, share the transit gateway with member accounts by using an AWS Organizations SCP",
            "letter": "B"
          },
          {
            "text": "Launch an AWS CloudFormation stack set from the management account that automatical^/ creates a new VPC and a VPC transit gateway attachment in a member accoun Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "C"
          },
          {
            "text": "Associate the attachment with the transit gateway in the management account by using the transit gateway ID.",
            "letter": "D"
          },
          {
            "text": "Launch an AWS CloudFormation stack set from the management account that automatical^ creates a new VPC and a peering transit gateway attachment in a member accoun F. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role. G. From the management account, share the transit gateway with member accounts by using AWS Service Catalog",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "https://aws.amazon.com/blogs/mt/self-service-vpcs-in-aws-control-tower-using-aws-service-catalog/ https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit- gateways.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-transitgatewayattachme",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4cd4f628f612",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 8
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223359"
      }
    },
    {
      "question": {
        "id": "sap-c02_6469b82a7dad",
        "number": 111,
        "text": "A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes networking and\nsecurity strategies. The company has set up an AWS Direct Connection connection in a central network account.\nThe company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the resources on AWS\nseamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to the internet through its on-premises\ndata center.\nWhich combination of steps will meet these requirements? (Choose three.)",
        "options": [
          {
            "text": "Create a Direct Connect gateway in the central accoun",
            "letter": "A"
          },
          {
            "text": "In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway.",
            "letter": "B"
          },
          {
            "text": "Create a Direct Connect gateway and a transit gateway in the central network accoun",
            "letter": "C"
          },
          {
            "text": "Attach the transit gateway to the Direct Connect gateway by using a transit VIF.",
            "letter": "D"
          },
          {
            "text": "Provision an internet gatewa F. Attach the internet gateway to subnet G. Allow internet traffic through the gateway. H. Share the transit gateway with other account I. Attach VPCs to the transit gateway. J. Provision VPC peering as necessary. K. Provision only private subnet L. Open the necessary route on the transit gateway and customer gatewayto allow outbound internet traffic from AWS to flow through NAT services that run in the data center.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BD",
        "explanation": "Option A is incorrect because creating a Direct Connect gateway in the central account and creating an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway does not enable active-passive failover between the regions. A Direct Connect gateway is a globally available resource that enables you to connect your AWS Direct Connect connection over a private virtual interface (VIF) to one or more VPCs in any AWS Region. A virtual private gateway is the VPN concentrator on the Amazon side of a VPN connection. You can associate a Direct Connect gateway with either a transit gateway or a virtual private gateway. However, a Direct Connect gateway does not provide any load balancing or failover capabilities by itself1 Option B is correct because creating a Direct Connect gateway and a transit gateway in the central network account and attaching the transit gateway to the Direct Connect gateway by using a transit VIF meets the requirement of enabling the corporate network to access the resources on AWS seamlessly and also to communicate with all the VPCs. A transit VIF is a type of private VIF that you can use to connect your AWS Direct Connect connection to a transit gateway or a Direct Connect gateway. A transit gateway is a network transit hub that you can use to interconnect your VPCs and on-premises networks. By using a transit VIF, you can route traffic between your on-premises network and multiple VPCs across different AWS accounts and Regions through a single connection23 Option C is incorrect because provisioning an internet gateway, attaching the internet gateway to subnets, and allowing internet traffic through the gateway does not meet the requirement of routing cloud resources to the internet through its on-premises data center. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. By using an internet gateway, you are routing cloud resources directly to the internet, not through your on-premises data center. Option D is correct because sharing the transit gateway with other accounts and attaching VPCs to the transit gateway meets the requirement of enabling the corporate network to access the resources on AWS seamlessly and also to communicate with all the VPCs. You can share your transit gateway with other AWS accounts within the same organization by using AWS Resource Access Manager (AWS RAM). This allows you to centrally manage connectivity from multiple accounts without having to create individual peering connections between VPCs or duplicate network appliances in each account. You can attach VPCs from different accounts and Regions to your shared transit gateway and enable routing between them. Option E is incorrect because provisioning VPC peering as necessary does not meet the requirement of enabling the corporate network to access the resources on AWS seamlessly and also to communicate with all the VPCs. VPC peering is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single Region. However, VPC peering does not allow you to route traffic from your on-premises network to your VPCs or between multiple Regions. You would need to create multiple VPN connections or Direct Connect connections for each VPC peering connection, which increases operational complexity and costs. Option F is correct because provisioning only private subnets, opening the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center meets the requirement of routing cloud resources to the internet through its on- premises data center. A private subnet is a subnet thatâ€™s associated with a route table that has no route to an internet gateway. Instances in a private subnet can communicate with other instances in the same VPC but cannot access resources on the internet directly. To enable outbound internet access from instances in private subnets, you can use NAT devices such as NAT gateways or NAT instances that are deployed in public subnets. A public subnet is a subnet thatâ€™s associated with a route table that has a route to an internet gateway. Alternatively, you can use your on-premises data center as a NAT device by configuring routes on your transit gateway and customer gateway that direct outbound internet traffic from your private subnets through your VPN connection or Direct Connect connection. This way, you can route cloud resources to the internet through your on-premises data center instead of using an internet gateway. References: 1: https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html 2: https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-transit-virtual-interfaces.html 3: https://docs.aws.amazon.com/vpc/latest/tgw/what-is- transit-gateway.html : https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html : https://docs.aws.amazon.com/vpc/latest/tgw/tgw- sharing.html : https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html : https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html : https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario3.html : https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html : Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6469b82a7dad",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 9
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 10
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223365",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_59feb820c140",
        "number": 112,
        "text": "A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and\nassets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design\nteam can access.\nAfter the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A\nsolutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted\nchanges.\nWhich combination of steps will meet these requirements? (Select THREE.)",
        "options": [
          {
            "text": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket.",
            "letter": "A"
          },
          {
            "text": "In the development account, create a new IAM policy that allows read and write access to the S3 bucket.",
            "letter": "B"
          },
          {
            "text": "In the production account, create a rol",
            "letter": "C"
          },
          {
            "text": "Attach the new policy to the rol",
            "letter": "D"
          },
          {
            "text": "Define the development account as a trusted entity. F. In the development account, create a rol G. Attach the new policy to the rol H. Define the production account as a trusted entity. I. In the development account, create a group that contains all the IAM users of the design tea J. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account. K. In the development account, create a group that contains all tfje IAM users of the design tea L. Attach a different IAM policy to the group to allow the sts;AssumeRole action on the role in the development account.",
            "letter": "E"
          },
          {
            "text": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket. The policy grants the necessary permissions to access the assets in the production S3 bucket.",
            "letter": "A"
          },
          {
            "text": "In the production account, create a role. Attach the new policy to the role. Define the development account as a trusted entity. By creating a role and attaching the policy, and then defining the development account as a trusted entity, the development account can assume the role and access the production S3 bucket with the read and write permissions.",
            "letter": "C"
          },
          {
            "text": "In the development account, create a group that contains all the IAM users of the design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account. The IAM policy attached to the group allows the design team members to assume the role created in the production account, thereby giving them access to the production S3 bucket. Step 1: Create a role in the Production Account; create the role in the Production account and specify the Development account as a trusted entity. You also limit the role permissions to only read and write access to the productionapp bucket. Anyone granted permission to use the role can read and write to the productionapp bucket. Step 2: Grant access to the role Sign in as an administrator in the Development account and allow the AssumeRole action on the UpdateApp role in the Production account. So, recap, production account you create the policy for S3, and you set development account as a trusted entity. Then on the development account you allow the sts:assumeRole action on the role in production account. https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with- roles.html",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "ACE",
        "explanation": "A. In the production account, create a new IAM policy that allows read and write access to the S3 bucket. The policy grants the necessary permissions to access the assets in the production S3 bucket. C. In the production account, create a role. Attach the new policy to the role. Define the development account as a trusted entity. By creating a role and attaching the policy, and then defining the development account as a trusted entity, the development account can assume the role and access the production S3 bucket with the read and write permissions. E. In the development account, create a group that contains all the IAM users of the design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account. The IAM policy attached to the group allows the design team members to assume the role created in the production account, thereby giving them access to the production S3 bucket. Step 1: Create a role in the Production Account; create the role in the Production account and specify the Development account as a trusted entity. You also limit the role permissions to only read and write access to the productionapp bucket. Anyone granted permission to use the role can read and write to the productionapp bucket. Step 2: Grant access to the role Sign in as an administrator in the Development account and allow the AssumeRole action on the UpdateApp role in the Production account. So, recap, production account you create the policy for S3, and you set development account as a trusted entity. Then on the development account you allow the sts:assumeRole action on the role in production account. https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with- roles.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "59feb820c140",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 10
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223375"
      }
    },
    {
      "question": {
        "id": "sap-c02_03ad62bfff8b",
        "number": 113,
        "text": "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode.\nA recent RDS database failover test caused a 40-second outage to the application A solutions architect needs to design a solution to reduce the outage time to\nless than 20 seconds.\nWhich combination of steps should the solutions architect take to meet these requirements? (Select THREE.)",
        "options": [
          {
            "text": "Use Amazon ElastiCache for Memcached in front of the database",
            "letter": "A"
          },
          {
            "text": "Use Amazon ElastiCache for Redis in front of the database.",
            "letter": "B"
          },
          {
            "text": "Use RDS Proxy in front of the database",
            "letter": "C"
          },
          {
            "text": "Migrate the database to Amazon Aurora MySQL",
            "letter": "D"
          },
          {
            "text": "Create an Amazon Aurora Replica F. Create an RDS for MySQL read replica",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CDE",
        "explanation": "Migrate the database to Amazon Aurora MySQL. - Create an Amazon Aurora Replica. - Use RDS Proxy in front of the database. - These options are correct because they address the requirement of reducing the failover time to less than 20 seconds. Migrating to Amazon Aurora MySQL and creating an Aurora replica can reduce the failover time to less than 20 seconds. Aurora has a built-in, fault-tolerant storage system that can automatically detect and repair failures. Additionally, Aurora has a feature called \"Aurora Global Database\" which allows you to create read-only replicas across multiple AWS regions which can further help to reduce the failover time. Creating an Aurora replica can also help to reduce the failover time as it can take over as the primary DB instance in case of a failure. Using RDS proxy can also help to reduce the failover time as it can route the queries to the healthy DB instance, it also helps to balance the load across multiple DB instances.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "03ad62bfff8b",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 15
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223384"
      }
    },
    {
      "question": {
        "id": "sap-c02_4333e1b884c7",
        "number": 114,
        "text": "A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores\nthe image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.\nThe Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda\nfunction to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment\nvariables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These\nchanges cause interruptions for users.\nA solutions architect needs to simplify this process to minimize disruption to users. Which solution will meet these requirements with the LEAST operational\noverhead?",
        "options": [
          {
            "text": "Directly modify the environment variables of the published Lambda function versio",
            "letter": "A"
          },
          {
            "text": "Use theSLATEST version to test image processing parameters.",
            "letter": "B"
          },
          {
            "text": "Create an Amazon DynamoDB table to store the image processing parameter",
            "letter": "C"
          },
          {
            "text": "Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table.",
            "letter": "D"
          },
          {
            "text": "Directly code the image processing parameters within the Lambda function and remove the environment variable F. Publish a new function version when the company updates the parameters. G. Create a Lambda function alia H. Modify the client application to use the function alias AR I. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "A Lambda function alias allows you to point to a specific version of a function and also can be updated to point to a new version of the function without modifying the client application. This way, the company can test different versions of the function with different environment variables and, once the optimal parameters are found, update the alias to point to the new version, without the need to update the client application. By using this approach, the company can simplify the process of updating the environment variables, minimize disruption to users, and reduce the operational overhead. Reference: AWS Lambda documentation: https://aws.amazon.com/lambda/ AWS Lambda Aliases documentation: https://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.html AWS Lambda versioning and aliases documentation: https://aws.amazon.com/blogs/compute/versioning-aliases-in-aws-lambda/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4333e1b884c7",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 19
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 17
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223396",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_a8340e00724e",
        "number": 115,
        "text": "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer.\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\nThe company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate\ntraffic to the application.\nHow should a solutions architect configure the web ACLs to meet these requirements?",
        "options": [
          {
            "text": "Set the action of the web ACL rules to Coun",
            "letter": "A"
          },
          {
            "text": "Enable AWS WAF logging Analyze the requests for false positives Modify the rules to avoid any false positive Over time change the action of the web ACL rules from Count to Block.",
            "letter": "B"
          },
          {
            "text": "Use only rate-based rules in the web ACL",
            "letter": "C"
          },
          {
            "text": "and set the throttle limit as high as possible Temporarily block all requests that exceed the limi",
            "letter": "D"
          },
          {
            "text": "Define nested rules to narrow the scope of the rate tracking. F. Set the action o' the web ACL rules to Bloc G. Use only AWS managed rule groups in the web ACLs Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs. H. Use only custom rule groups in the web ACL I. and set the action to Allow Enable AWS WAF logging Analyze the requests tor false positives Modify the rules to avoid any false positive Over time, change the action of the web ACL rules from Allow to Block.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/waf-analyze-count-action-rules/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "a8340e00724e",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 21
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223402"
      }
    },
    {
      "question": {
        "id": "sap-c02_37f8fcfb385d",
        "number": 116,
        "text": "A company plans to refactor a monolithic application into a modern application designed deployed or AWS. The CLCD pipeline needs to be upgraded to support\nthe modem design for the application with the following requirements\nâ€¢ It should allow changes to be released several times every hour.\n* It should be able to roll back the changes as quickly as possible Which design will meet these requirements?",
        "options": [
          {
            "text": "Deploy a Cl-CD pipeline that incorporates AMIs to contain the application and their configurationsDeploy the application by replacing Amazon EC2 instances",
            "letter": "A"
          },
          {
            "text": "Specify AWS Elastic Beanstak to sage in a secondary environment as the deployment target for the CI/CD pipeline of the applicatio",
            "letter": "B"
          },
          {
            "text": "To deploy swap the staging and production environment URLs.",
            "letter": "C"
          },
          {
            "text": "Use AWS Systems Manager to re-provision the infrastructure for each deployment Update the Amazon EC2 user data to pull the latest code art-fact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment",
            "letter": "D"
          },
          {
            "text": "Roll out At application updates as pan of an Auto Scaling event using prebuilt AMI F. Use new versions of the AMIs to add instances, and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "It is the fastest when it comes to rollback and deploying changes every hour",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "37f8fcfb385d",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 23
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223412"
      }
    },
    {
      "question": {
        "id": "sap-c02_ccda31362d29",
        "number": 117,
        "text": "A finance company is running its business-critical application on current-generation Linux EC2 instances The application includes a self-managed MySQL\ndatabase performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during\nthe final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure\nto meet the increased demand.\nWhich of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?",
        "options": [
          {
            "text": "Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.",
            "letter": "A"
          },
          {
            "text": "Performing a one-time migration of the database cluster to Amazon RD",
            "letter": "B"
          },
          {
            "text": "and creating several additional read replicas to handle the load during end of month",
            "letter": "C"
          },
          {
            "text": "Using Amazon CioudWatch with AWS Lambda to change the typ",
            "letter": "D"
          },
          {
            "text": "size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric F. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "In this scenario, the Amazon EC2 instances are in an Auto Scaling group already which means that the database read operations is the possible bottleneck especially during the month-end wherein the reports are generated. This can be solved by creating RDS read replicas.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ccda31362d29",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 25
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223417"
      }
    },
    {
      "question": {
        "id": "sap-c02_fd8b8d2291f8",
        "number": 118,
        "text": "A financial services company receives a regular data feed from its credit card servicing partner Approximately 5.1 records are sent every 15 minutes in plaintext,\ndelivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data\nThe company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to\nremove and merge specific fields, and then transform the record into JSON format Additionally, extra feeds are likely to be added in the future, so any design\nneeds to be easily expandable.\nWhich solutions will meet these requirements?",
        "options": [
          {
            "text": "Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queu",
            "letter": "A"
          },
          {
            "text": "Trigger another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Trigger a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.",
            "letter": "B"
          },
          {
            "text": "Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queu",
            "letter": "C"
          },
          {
            "text": "Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains message Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "D"
          },
          {
            "text": "Have the application process each record, and transform the record into JSON forma F. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance. G. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to matc H. Trigger an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirement I. Define the output format as JSO J. Once complete, have the ETL job send the results to another S3 bucket for internal processing. K. Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to matc L. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETLjob to transform the entire record according to the processing and transformation requirement M. Define the output format as JSO N. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "You can use a Glue crawler to populate the AWS Glue Data Catalog with tables. The Lambda function can be triggered using S3 event notifications when object create events occur. The Lambda function will then trigger the Glue ETL job to transform the records masking the sensitive data and modifying the output format to JSON. This solution meets all requirements.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "fd8b8d2291f8",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 26
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223425"
      }
    },
    {
      "question": {
        "id": "sap-c02_e47f3b301f8d",
        "number": 119,
        "text": "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2. Amazon S3 and Amazon DynamoDB. The\ndevelopers account resides In a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account:\nWhen this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy. What should the solutions\narchitect do to eliminate the developers' ability to use services outside the scope of this policy?",
        "options": [
          {
            "text": "Create an explicit deny statement for each AWS service that should be constrained",
            "letter": "A"
          },
          {
            "text": "Remove the Full AWS Access SCP from the developer account's OU",
            "letter": "B"
          },
          {
            "text": "Modify the Full AWS Access SCP to explicitly deny all services",
            "letter": "C"
          },
          {
            "text": "Add an explicit deny statement using a wildcard to the end of the SCP",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "e47f3b301f8d",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 31
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223430"
      }
    },
    {
      "question": {
        "id": "sap-c02_e2935c934e1d",
        "number": 120,
        "text": "A company is using AWS Organizations lo manage multiple AWS accounts For security purposes, the company requires the creation of an Amazon Simple\nNotification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts\nA solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks Trusted\naccess has been enabled in Organizations\nWhat should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
        "options": [
          {
            "text": "Create a stack set in the Organizations member account",
            "letter": "A"
          },
          {
            "text": "Use service-managed permission",
            "letter": "B"
          },
          {
            "text": "Set deployment options to deploy to an organizatio",
            "letter": "C"
          },
          {
            "text": "Use CloudFormation StackSets drift detection.",
            "letter": "D"
          },
          {
            "text": "Create stacks in the Organizations member account F. Use self-service permission Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) G. Set deployment options to deploy to an organizatio H. Enable the CloudFormation StackSets automatic deployment. I. Create a stack set in the Organizations management account Use service-managed permission J. Set deployment options to deploy to the organizatio K. Enable CloudFormation StackSets automatic deployment. L. Create stacks in the Organizations management accoun M. Use service-managed permission N. Set deployment options to deploy to the organizatio O. Enable CloudFormation StackSets drift detection.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-ac",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "e2935c934e1d",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 36
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223436"
      }
    },
    {
      "question": {
        "id": "sap-c02_99a87bb587b2",
        "number": 121,
        "text": "A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the company's data\ncenter.\nThe system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes and 2 hours to\nfinish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5 minutes and have no\nSLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to meet SLAs. However, user jobs can be delayed.\nA solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-term commitments.\nThe solution must maintain high availability and must not affect the SLAs.\nWhich solution will meet these requirements MOST cost-effectively?",
        "options": [
          {
            "text": "Split the 12 instances across two Availability Zones in the chosen AWS Regio",
            "letter": "A"
          },
          {
            "text": "Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservation",
            "letter": "B"
          },
          {
            "text": "Run four instances in each Availability Zone as Spot Instances.",
            "letter": "C"
          },
          {
            "text": "Split the 12 instances across three Availability Zones in the chosen AWS Regio",
            "letter": "D"
          },
          {
            "text": "In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservation F. Run the remaining instances as Spot Instances. G. Split the 12 instances across three Availability Zones in the chosen AWS Regio H. Run two instances in each Availability Zone as On-Demand Instances with a Savings Pla I. Run two instances in each Availability Zone as Spot Instances. J. Split the 12 instances across three Availability Zones in the chosen AWS Regio K. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservation L. Run one instance in each Availability Zone as a Spot Instance.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "By splitting the 12 instances across three Availability Zones, the system can maintain high availability and availability of resources in case of a failure. Option D also uses a combination of On-Demand Instances with Capacity Reservations and Spot Instances, which allows for scheduled jobs to be run on the On-Demand instances with guaranteed capacity, while also taking advantage of the cost savings from Spot Instances for the user jobs which have lower SLA requirements.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "99a87bb587b2",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 41
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223443"
      }
    },
    {
      "question": {
        "id": "sap-c02_6302ceead740",
        "number": 122,
        "text": "A financial services company in North America plans to release a new online web application to its customers on AWS . The company will launch the application in\nthe us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet user traffic. The company also\nwants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-passive failover.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create a VPC in us-east-1 and a VPC in us-west-1 Configure VPC peering In the us-east-1 VP",
            "letter": "A"
          },
          {
            "text": "create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs Place the Auto Scaling group behind the ALB.",
            "letter": "B"
          },
          {
            "text": "Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VP",
            "letter": "C"
          },
          {
            "text": "create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VP",
            "letter": "D"
          },
          {
            "text": "Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC Place the Auto Scaling group behind the ALB Set up the same configuration in the us-west-1 VP F. Create an Amazon Route 53 hosted zone Create separate records for each ALB Enable health checks to ensure high availability between Regions. G. Create a VPC in us-east-1 and a VPC in us-west-1 In the us-east-1 VP H. create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC Place the Auto Scaling group behind the ALB Set up the same configuration in the us-west-1 VPC Create an Amazon Route 53 hosted zon I. Create separate records for each ALB Enable health checks and configure a failover routing policy for each record. J. Create a VPC in us-east-1 and a VPC in us-west-1 Configure VPC peering In the us-east-1 VP K. create an Application Load Balancer (ALB) that extends across multiple Availability Zones in Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs Place the Auto Scaling group behind the ALB Create an Amazon Route 53 host.. Create a record for the ALB.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "it's the one that handles failover while B (the one shown as the answer today) it almost the same but does not handle failover.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6302ceead740",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 42
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 41
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223451",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_18fd726e42ad",
        "number": 123,
        "text": "A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The\nEC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).\nThe company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\ncompany uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.\nA solutions architect must configure the application so that itis highly available and fault tolerant. Which solution meets these requirements?",
        "options": [
          {
            "text": "Provision a full, secondary application deployment in a different AWS Regio",
            "letter": "A"
          },
          {
            "text": "Update the Route 53 A record to be a failover recor",
            "letter": "B"
          },
          {
            "text": "Add both of the CloudFront distributions as value",
            "letter": "C"
          },
          {
            "text": "Create Route 53 health checks.",
            "letter": "D"
          },
          {
            "text": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Regio F. Update the CloudFront distribution, and create a second origin for the new AL G. Create an origin group for the two origin H. Configure one origin as primary and one origin as secondary. I. Provision an Auto Scaling group and EC2 instances in a different AWS Regio J. Create a second target for the new Auto Scaling group in the AL K. Set up the failover routing algorithm on the ALB. L. Provision a full, secondary application deployment in a different AWS Regio M. Create a second CloudFront distribution, and add the new application setup as an origi N. Create an AWS Global Accelerator accelerato O. Add both of the CloudFront distributions as endpoints.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.h https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable, or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "18fd726e42ad",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 46
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223457"
      }
    },
    {
      "question": {
        "id": "sap-c02_31a2668fb4a0",
        "number": 124,
        "text": "A company that has multiple AWS accounts is using AWS Organizations. The companyâ€™s AWS accounts host VPCs, Amazon EC2 instances, and containers.\nThe companyâ€™s compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and\nsend information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-related resources with a key of\nâ€œcostCenterâ€ and a value or â€œcomplianceâ€.\nThe company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance teamâ€™s AWS\naccount. The cost calculation must be as accurate as possible.\nWhat should a solutions architect do to meet these requirements?",
        "options": [
          {
            "text": "In the management account of the organization, activate the costCenter user-defined ta",
            "letter": "A"
          },
          {
            "text": "Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management accoun",
            "letter": "B"
          },
          {
            "text": "Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.",
            "letter": "C"
          },
          {
            "text": "In the member accounts of the organization, activate the costCenter user-defined ta",
            "letter": "D"
          },
          {
            "text": "Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management accoun F. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources. G. In the member accounts of the organization activate the costCenter user-defined ta H. From the management account, schedule a monthly AWS Cost and Usage Repor I. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources. J. Create a custom report in the organization view in AWS Trusted Adviso K. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance teamâ€™s AWS account.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/configurecostallocreport.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "31a2668fb4a0",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 49
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223469"
      }
    },
    {
      "question": {
        "id": "sap-c02_0edb3d39aa21",
        "number": 125,
        "text": "A company has purchased appliances from different vendors. The appliances all have loT sensors. The sensors send status information in the vendors' proprietary\nformats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application\nparses all the JSON records and stores the records in a relational database for analysis.\nThe company needs to design a new data analysis solution that can deliver faster and optimize costs. Which solution will meet these requirements?",
        "options": [
          {
            "text": "Connect the loT sensors to AWS loT Cor",
            "letter": "A"
          },
          {
            "text": "Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the file",
            "letter": "B"
          },
          {
            "text": "Use Amazon Athena and Amazon OuickSight for analysis.",
            "letter": "C"
          },
          {
            "text": "Migrate the application server to AWS Fargate, which will receive the information from loT sensors and parse the information into a relational forma",
            "letter": "D"
          },
          {
            "text": "Save the parsed information to Amazon Redshift for analysis. F. Create an AWS Transfer for SFTP serve G. Update the loT sensor code to send the information as a .csv file through SFTP to the serve H. Use AWS Glue to catalog the file I. Use Amazon Athena for analysis. J. Use AWS Snowball Edge to collect data from the loT sensors directly to perform local analysis.Periodically collect the data into Amazon Redshift to perform global analysis.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis. This solution meets the requirement of faster analysis and cost Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "0edb3d39aa21",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 50
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223475"
      }
    },
    {
      "question": {
        "id": "sap-c02_7dca1a45cbc2",
        "number": 126,
        "text": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced\nan issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an alias for every new deployed version of the Lambda functio",
            "letter": "A"
          },
          {
            "text": "Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.",
            "letter": "B"
          },
          {
            "text": "Deploy the application into a new CloudFormation stac",
            "letter": "C"
          },
          {
            "text": "Use an Amazon Route 53 weighted routing policy to distribute the load.",
            "letter": "D"
          },
          {
            "text": "Create a version for every new deployed Lambda functio F. Use the AWS CLIupdate-function-configuration command with the routing-config parameter to distribute the load. G. Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias- https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "7dca1a45cbc2",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 53
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223480"
      }
    },
    {
      "question": {
        "id": "sap-c02_7d9122f3d75b",
        "number": 127,
        "text": "A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the\ncompany's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and\nwants to use AWS.\nThe company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The\nmeasured upload speed of the company's internet connection is 100 Mbps, and multiple departments share the connection.\nWhich solution will meet these requirements MOST cost-effectively?",
        "options": [
          {
            "text": "Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Consol",
            "letter": "A"
          },
          {
            "text": "Configure the devices with a destination S3 bucke",
            "letter": "B"
          },
          {
            "text": "Copy the data to the device",
            "letter": "C"
          },
          {
            "text": "Ship the devices back to AWS.",
            "letter": "D"
          },
          {
            "text": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Regio F. Transfer the data over a VPN connection into the Region to store the data in Amazon S3. G. Create a VPN connection between the on-premises network storage and the nearest AWS Region.Transfer the data over the VPN connection. H. Deploy an AWS Storage Gateway file gateway on premise I. Configure the file gateway with a destination S3 bucke J. Copy the data to the file gateway.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "This solution will meet the requirements of the company as it provides a secure, cost-effective and fast way of transferring large data sets from on-premises to AWS. Snowball Edge devices encrypt the data during transfer, and the devices are shipped back to AWS for import into S3. This option is more cost effective than using Direct Connect or VPN connections as it does not require the company to pay for long-term dedicated connections.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "7d9122f3d75b",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 58
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223486"
      }
    },
    {
      "question": {
        "id": "sap-c02_bc2cbf83f888",
        "number": 128,
        "text": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1\nRegions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs\nto support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.\nThe company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct\nConnect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that\nis configured to route all inter-VPC traffic within that Region.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create a private VIF from the DX-A connection into a Direct Connect gatewa",
            "letter": "A"
          },
          {
            "text": "Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availabilit",
            "letter": "B"
          },
          {
            "text": "Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gatewa",
            "letter": "C"
          },
          {
            "text": "Peer the transit gatewayswith each other to support cross-Region routing.",
            "letter": "D"
          },
          {
            "text": "Create a transit VIF from the DX-A connection into a Direct Connect gatewa F. Associate the eu-west-1 transit gateway with this Direct Connect gatewa G. Create a transit VIF from the DX-B connection into a separate Direct Connect gatewa H. Associate the us-east-1 transit gateway with this separate Direct Connect gatewa I. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing. J. Create a transit VIF from the DX-A connection into a Direct Connect gatewa K. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availabilit L. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gatewa M. Configure the Direct Connect gateway to route traffic between the transit gateways. N. Create a transit VIF from the DX-A connection into a Direct Connect gatewa Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) O. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availabilit P. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gatewa Q. Peer the transit gateways with each other to support cross-Region routing.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "in this solution, two transit VIFs are created - one from the DX-A connection and one from the DX-B connection - into the same Direct Connect gateway for high availability. Both the eu-west-1 and us-east-1 transit gateways are then associated with this Direct Connect gateway. The transit gateways are then peered with each other to support cross-Region routing. This solution meets the requirements of the company by creating a highly available connection between the on- premises data center and the VPCs in both the eu-west-1 and us-east-1 regions, and by enabling direct traffic routing between VPCs in those regions.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "bc2cbf83f888",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 62
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223493"
      }
    },
    {
      "question": {
        "id": "sap-c02_c9c2811386ce",
        "number": 129,
        "text": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition\nsoftware for categorization.\nThe website contains stat c content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto\nScaling group for the web application and EC2\ninstances running in an Auto Scaling group to process an Amazon SQS queue The company wants to\nre-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.\nWhich solution meets these requirements?",
        "options": [
          {
            "text": "Use Amazon ECS containers for the web application and Spot Instances for the Auto Scaling group that processes the SQS queu",
            "letter": "A"
          },
          {
            "text": "Replace the custom software with Amazon Recognition to categorize the videos.",
            "letter": "B"
          },
          {
            "text": "Store the uploaded videos n Amazon EFS and mount the file system to the EC2 instances for Te web applicatio",
            "letter": "C"
          },
          {
            "text": "Process the SOS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
            "letter": "D"
          },
          {
            "text": "Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notifications to publish events to the SQS queue Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos. F. Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue Replace the custom software with Amazon Rekognition to categorize the videos.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Option C is correct because hosting the web application in Amazon S3, storing the uploaded videos in Amazon S3, and using S3 event notifications to publish events to the SQS queue reduces the operational overhead of managing EC2 instances and EBS volumes. Amazon S3 can serve static content such as HTML, CSS, JavaScript, and media files directly from S3 buckets. Amazon S3 can also trigger AWS Lambda functions through S3 event notifications when new objects are created or existing objects are updated or deleted. AWS Lambda can process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos. This solution eliminates the need for custom recognition software and third-party dependencies345 References: 1: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html 2: https://aws.amazon.com/efs/pricing/ 3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html 4: https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html 5: https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html 6: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "c9c2811386ce",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 63
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223499"
      }
    },
    {
      "question": {
        "id": "sap-c02_29656f9d22de",
        "number": 130,
        "text": "A company is processing videos in the AWS Cloud by using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video. Several EC2\ninstances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.\nThe company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the\nvisibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in\nthe dead-letter queue.\nSeveral times during the day, the development team receives notification that messages are in the dead-letter queue and that videos have not been processed\nproperly. An investigation finds no errors in the application logs.\nHow can the company solve this problem?",
        "options": [
          {
            "text": "Turn on termination protection for the EC2 instances.",
            "letter": "A"
          },
          {
            "text": "Update the visibility timeout for the SOS queue to 3 hours.",
            "letter": "B"
          },
          {
            "text": "Configure scale-in protection for the instances during processing.",
            "letter": "C"
          },
          {
            "text": "Update the redrive policy and set maxReceiveCount to 0.",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "The best solution for this problem is to update the visibility timeout for the SQS queue to 3 hours. This is because when the visibility timeout is set to 1 hour, it means that if the EC2 instance doesn't process the message within an hour, it will be moved to the dead-letter queue. By increasing the visibility timeout to 3 hours, this should give the EC2 instance enough time to process the message before it gets moved to the dead-letter queue. Additionally, configuring scale-in protection for the EC2 instances during processing will help to ensure that the instances are not terminated while the messages are being processed.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "29656f9d22de",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 68
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223505"
      }
    },
    {
      "question": {
        "id": "sap-c02_91191226692f",
        "number": 131,
        "text": "A company is planning to host a web application on AWS and works to load balance the traffic across a group of Amazon EC2 instances. One of the security\nrequirements is to enable end-to-end encryption in transit between the client and the web server.\nWhich solution will meet this requirement?",
        "options": [
          {
            "text": "Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the AL",
            "letter": "A"
          },
          {
            "text": "Export the SSL certificate and install it on each EC2 instanc",
            "letter": "B"
          },
          {
            "text": "Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "C"
          },
          {
            "text": "Associate the EC2 instances with a target grou",
            "letter": "D"
          },
          {
            "text": "Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon CloudFront distribution and configure It to use the SSL certificat F. Set CloudFront to use the target group as the origin server G. Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the AL H. Provision athird-party SSL certificate and install it on each EC2 instanc I. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances. J. Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instanc K. Configure the NLB to listen on port 443 and to forward traffic to port 443 on the instances.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "Option A is correct because placing the EC2 instances behind an Application Load Balancer (ALB) and associating an SSL certificate from AWS Certificate Manager (ACM) with the ALB enables encryption in transit between the client and the ALB. Exporting the SSL certificate and installing it on each EC2 instance enables encryption in transit between the ALB and the web server. Configuring the ALB to listen on port 443 and to forward traffic to port 443 on the instances ensures that HTTPS is used for both connections. This solution achieves end-to-end encryption in transit for the web applicatio1n2 References: 1: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html 2: https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html 3: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer- target-groups.html : https://aws.amazon.com/certificate-manager/faqs/ : https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "91191226692f",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 71
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223511"
      }
    },
    {
      "question": {
        "id": "sap-c02_b1b725ca11e9",
        "number": 132,
        "text": "A large company is running a popular web application. The application runs on several Amazon EC2 Linux Instances in an Auto Scaling group in a private subnet.\nAn Application Load Balancer is targeting the Instances In the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager Is configured,\nand AWS Systems Manager Agent is running on all the EC2 instances.\nThe company recently released a new version of the application Some EC2 instances are now being marked as unhealthy and are being terminated As a result,\nthe application is running at reduced capacity A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from\nthe application, but the logs are inconclusive\nHow should the solutions architect gain access to an EC2 instance to troubleshoot the issue1?",
        "options": [
          {
            "text": "Suspend the Auto Scaling group's HealthCheck scaling proces",
            "letter": "A"
          },
          {
            "text": "Use Session Manager to log in to an instance that is marked as unhealthy",
            "letter": "B"
          },
          {
            "text": "Enable EC2 instance termination protection Use Session Manager to log In to an instance that is marked as unhealthy.",
            "letter": "C"
          },
          {
            "text": "Set the termination policy to Oldestinstance on the Auto Scaling grou",
            "letter": "D"
          },
          {
            "text": "Use Session Manager to log in to an instance that is marked as unhealthy F. Suspend the Auto Scaling group's Terminate proces G. Use Session Manager to log in to an instance thatis marked as unhealthy",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b1b725ca11e9",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 76
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223520"
      }
    },
    {
      "question": {
        "id": "sap-c02_d0f11b780161",
        "number": 133,
        "text": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda\nfunction. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support\nfailover to another AWS Region.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.",
            "letter": "A"
          },
          {
            "text": "Create an Amazon Simple Queue Service (Amazon SQS) queu Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "B"
          },
          {
            "text": "Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda functio",
            "letter": "C"
          },
          {
            "text": "Configure the Lambda function to pull messages from the queue for processing.",
            "letter": "D"
          },
          {
            "text": "Deploy the Lambda function to the us-west-2 Regio F. Create an API Gateway endpoint in us-west-2 to direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints. G. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Regio H. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "This solution allows for deploying the Lambda function and API Gateway endpoint to another region, providing a failover option in case of any issues in the primary region. Using Route 53's failover routing policy allows for automatic routing of traffic to the healthy endpoint, ensuring that the application is available even in case of issues in one region. This solution provides a cost-effective and simple way to implement failover while minimizing operational overhead.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d0f11b780161",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 81
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223527"
      }
    },
    {
      "question": {
        "id": "sap-c02_113aba2cbfda",
        "number": 134,
        "text": "A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved resources and that\nengineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to enforce the new restriction on the IAM\nrole that the engineers use for access.\nWhat should the solutions architect do to create the solution?",
        "options": [
          {
            "text": "Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket.Update the IAM policy for the engineers' IAM role to only allow access to Amazon S3 and AWS CloudFormatio",
            "letter": "A"
          },
          {
            "text": "Use AWS CloudFormation templates to provision resources.",
            "letter": "B"
          },
          {
            "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormatio",
            "letter": "C"
          },
          {
            "text": "Use AWS CloudFormation templates to create stacks with approved resources.",
            "letter": "D"
          },
          {
            "text": "Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation action F. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service rol G. Assign the IAM service role to AWS CloudFormation during stack creation. H. Provision resources in AWS CloudFormation stack I. Update the IAM policy for the engineers' IAM role to only allow access to their own AWS CloudFormation stack.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-best-practices.html#use-iam-to-c https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "113aba2cbfda",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 89
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 75
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223537",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_9a9189ca0901",
        "number": 135,
        "text": "A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching.\nManagement requires a single report showing the patch status of all the servers and instances.\nWhich set of actions should a solutions architect take to meet these requirements?",
        "options": [
          {
            "text": "Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instance",
            "letter": "A"
          },
          {
            "text": "Use Systems Manager to generate patch compliance reports.",
            "letter": "B"
          },
          {
            "text": "Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instance",
            "letter": "C"
          },
          {
            "text": "Use Amazon OuickSight integration with OpsWorks to generate patch compliance reports.",
            "letter": "D"
          },
          {
            "text": "Use an Amazon EventBridge (Amazon CloudWatch Events) rule to apply patches by scheduling an AWS Systems Manager patch remediation jo F. Use Amazon Inspector to generate patch compliance reports. G. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instance H. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "9a9189ca0901",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 93
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 77
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223542",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_213d3a85556c",
        "number": 136,
        "text": "A company has a legacy monolithic application that is critical to the company's business. The company hosts the application on an Amazon EC2 instance that runs\nAmazon Linux 2. The company's application team receives a directive from the legal department to back up the data from the instance's encrypted Amazon Elastic\nBlock Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application\nmust continue to serve the users.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.",
            "letter": "A"
          },
          {
            "text": "Create an image of the instance with the reboot option turned o",
            "letter": "B"
          },
          {
            "text": "Launch a new EC2 instance from the imag",
            "letter": "C"
          },
          {
            "text": "Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.",
            "letter": "D"
          },
          {
            "text": "Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3. F. Create an image of the instanc G. Launch a new EC2 instance from the imag H. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (DLM) will meet the requirements because it allows you to create a backup of the volume without the need to access the instance or its SSH key pair. Additionally, DLM allows you to schedule the backups to occur at specific intervals and also enables you to copy the snapshots to an S3 bucket. This approach will not impact the running application as the backup is performed on the EBS volume level.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "213d3a85556c",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 102
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223555"
      }
    },
    {
      "question": {
        "id": "sap-c02_f65110c34a6e",
        "number": 137,
        "text": "A company used Amazon EC2 instances to deploy a web fleet to host a blog site The EC2 instances are behind an Application Load Balancer (ALB) and are\nconfigured in an Auto ScaSng group The web application stores all blog content on an Amazon EFS volume.\nThe company recently added a feature 'or Moggers to add video to their posts, attracting 10 times the previous user traffic At peak times of day. users report\nbuffering and timeout issues while attempting to reach the site or watch videos\nWhich is the MOST cost-efficient and scalable deployment that win resolve the issues for users?",
        "options": [
          {
            "text": "Reconfigure Amazon EFS to enable maximum I/O.",
            "letter": "A"
          },
          {
            "text": "Update the Nog site to use instance store volumes tor storag",
            "letter": "B"
          },
          {
            "text": "Copy the site contents to the volumes atlaunch and to Amazon S3 al shutdown.",
            "letter": "C"
          },
          {
            "text": "Configure an Amazon CloudFront distributio",
            "letter": "D"
          },
          {
            "text": "Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3. F. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-connection-fails/ Using an Amazon S3 bucket Using a MediaStore container or a MediaPackage channel Using an Application Load Balancer Using a Lambda function URL Using Amazon EC2 (or another custom origin) Using CloudFront origin groups https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-to-load-balancer.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f65110c34a6e",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 107
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223560"
      }
    },
    {
      "question": {
        "id": "sap-c02_e1689a446b0d",
        "number": 138,
        "text": "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a\nNOSQL MongoDB database to store subscriber data.\nThe company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company\ncannot make changes to the application\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "use an Amazon Aurora DB cluster as the database for the subscriber dat",
            "letter": "A"
          },
          {
            "text": "Deploy Amazon EC2instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
            "letter": "B"
          },
          {
            "text": "Use MongoDB on Amazon EC2 instances as the database for the subscriber dat",
            "letter": "C"
          },
          {
            "text": "Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.",
            "letter": "D"
          },
          {
            "text": "Configure Amazon DocumentD3 (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber dat F. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application. G. Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber dat H. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "On-demand capacity mode is the function of Dynamodb. https://aws.amazon.com/blogs/news/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-ama Amazon DocumentDB Elastic Clusters https://aws.amazon.com/blogs/news/announcing-amazon-documentdb-elastic-clusters/ Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application. This will provide high availability and scalability, while allowing the company to retain the same database structure as the original application. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "e1689a446b0d",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 113
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223571"
      }
    },
    {
      "question": {
        "id": "sap-c02_da068e7a6381",
        "number": 139,
        "text": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly\nthrough a simple API over HTTPS. The solution must scale automatically in response to demand.\nWhich solutions meet these requirements? (Choose two.)",
        "options": [
          {
            "text": "Create an Amazon API Gateway REST AP",
            "letter": "A"
          },
          {
            "text": "Configure this API with direct integrations to DynamoDB by using API Gatewayâ€™s AWS integration type.",
            "letter": "B"
          },
          {
            "text": "Create an Amazon API Gateway HTTP AP",
            "letter": "C"
          },
          {
            "text": "Configure this API with direct integrations to Dynamo DB by using API Gatewayâ€™s AWS integration type.",
            "letter": "D"
          },
          {
            "text": "Create an Amazon API Gateway HTTP AP F. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables. G. Create an accelerator in AWS Global Accelerato H. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables. I. Create a Network Load Balance J. Configure listener rules to forward requests to the appropriate AWS Lambda functions",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.htm",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "da068e7a6381",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 116
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223578"
      }
    },
    {
      "question": {
        "id": "sap-c02_6aecb6a333f5",
        "number": 140,
        "text": "A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and uses Amazon\nCloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.\nThe company wants to create a CSV report every 2 weeks to show each API Lambda functionâ€™s recommended configured memory, recommended cost, and the\nprice difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.\nWhich solution will meet these requirements with the LEAST development time?",
        "options": [
          {
            "text": "Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week penod_ Collate the data into tabular forma",
            "letter": "A"
          },
          {
            "text": "Store the data as a_csvfile in an S3 bucke",
            "letter": "B"
          },
          {
            "text": "Create an Amazon Eventaridge rule to schedule the Lambda function to run every 2 weeks.",
            "letter": "C"
          },
          {
            "text": "Opt in to AWS Compute Optimize",
            "letter": "D"
          },
          {
            "text": "Create a Lambda function that calls the ExportLambdaFunctionRecommendatlons operatio F. Export the _csv file to an S3 bucke G. Create an Amazon Eventaridge rule to schedule the Lambda function to run every 2 weeks. H. Opt in to AWS Compute Optimize I. Set up enhanced infrastructure metric J. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a _csvfile_ Store the file in an S3 bucket every 2 weeks. K. Purchase the AWS Business Support plan for the production accoun L. Opt in to AWS Compute Optimizer for AWS Trusted Advisor check M. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a _csvfile_ Store the file in an S3 bucket every 2 weeks.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommend",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "6aecb6a333f5",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 121
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223590"
      }
    },
    {
      "question": {
        "id": "sap-c02_ee0949903d6d",
        "number": 141,
        "text": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common\nsecurity group rules for the AWS accounts in the organization.\nThe company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to\nand from the company's on-premises network.\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\nDevelopers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently,\nthe security team notifies the owners of the other AWS accounts when changes are made to the allow list.\nThe solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
        "options": [
          {
            "text": "Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS accoun",
            "letter": "A"
          },
          {
            "text": "Deploy an AWS Lambda function in each AWS accoun",
            "letter": "B"
          },
          {
            "text": "Configure the Lambda function to run every time an SNS topic receives a messag",
            "letter": "C"
          },
          {
            "text": "Configure the Lambda function to take an IP address as input and add it to a list of security groups in the accoun",
            "letter": "D"
          },
          {
            "text": "Instruct the security team to distribute changes by publishing messages to its SNS topic. F. Create new customer-managed prefix lists in each AWS account within the organizatio G. Populate the prefix lists in each account with all internal CIDR range H. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security group I. Instruct the security team to share updates with each AWS account owner. J. Create a new customer-managed prefix list in the security team's AWS accoun K. Populate the customer-managed prefix list with all internal CIDR range L. Share the customer-managed prefix listwith the organization by using AWS Resource Access Manage M. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups. N. Create an IAM role in each account in the organizatio O. Grant permissions to update security groups.Deploy an AWS Lambda function in the security team's AWS accoun P. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Create a new customer-managed prefix list in the security teamâ€™s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups. This solution meets the requirements with the least amount of operational overhead as it requires the security team to create and maintain a single customer-managed prefix list, and share it with the organization using AWS Resource Access Manager. The owners of each AWS account are then responsible for allowing the prefix list in their security groups, which eliminates the need for the security team to manually notify each account owner when changes are made. This solution also eliminates the need for a separate AWS Lambda function in each account, reducing the overall complexity of the solution.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ee0949903d6d",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 123
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223598"
      }
    },
    {
      "question": {
        "id": "sap-c02_c5d080501134",
        "number": 142,
        "text": "A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is\nrunning a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the\ncompanyâ€™s customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.\nThe company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and\nvalidated the SES domain. The company has lifted the SES limits.\nWhat should the company do to modify the application to send email messages from Amazon SES?",
        "options": [
          {
            "text": "Configure the application to connect to Amazon SES by using TLS Wrappe",
            "letter": "A"
          },
          {
            "text": "Create an IAM role that has ses:SendEmail and ses:SendRawEmail permission",
            "letter": "B"
          },
          {
            "text": "Attach the IAM role to an Amazon EC2 instance.",
            "letter": "C"
          },
          {
            "text": "Configure the application to connect to Amazon SES by using STARTTL",
            "letter": "D"
          },
          {
            "text": "Obtain Amazon SES SMTP credential F. Use the credentials to authenticate with Amazon SES. G. Configure the application to use the SES API to send email message H. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permission I. Use the IAM role as a service role for Amazon SES. J. Configure the application to use AWS SDKs to send email message K. Create an IAM user for Amazon SE L. Generate API access key M. Use the access keys to authenticate with Amazon SES.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "To set up a STARTTLS connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 25, 587, or 2587, issues an EHLO command, and waits for the server to announce that it supports the STARTTLS SMTP extension. The client then issues the STARTTLS command, initiating TLS negotiation. When negotiation is complete, the client issues an EHLO command over the new encrypted connection, and the SMTP session proceeds normally To set up a TLS Wrapper connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 465 or 2465. The server presents its certificate, the client issues an EHLO command, and the SMTP session proceeds normally. https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "c5d080501134",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 126
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223606"
      }
    },
    {
      "question": {
        "id": "sap-c02_62a7747ecb9c",
        "number": 143,
        "text": "A company is running an application in the AWS Cloud. Recent application metrics show inconsistent\nresponse times and a significant increase in error rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services\nsynchronously by directly invoking an AWS Lambda function.\nA solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.",
            "letter": "A"
          },
          {
            "text": "Use an AWS Step Functions state machine to pass events to the Lambda function.",
            "letter": "B"
          },
          {
            "text": "Use an Amazon EventBridge rule to pass events to the Lambda function.",
            "letter": "C"
          },
          {
            "text": "Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function.",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "Using an SQS queue to store events and invoke the Lambda function will decouple the third-party service calls and ensure that all the calls are eventually completed. SQS allows you to store messages in a queue and process them asynchronously, which eliminates the need for the application to wait for a response from the third-party service. The messages will be stored in the SQS queue until they are processed by the Lambda function, even if the Lambda function is currently unavailable or busy. This will ensure that all the calls are eventually completed, even if there are delays or errors. AWS Step Functions state machines can also be used to pass events to the Lambda function, but it would require additional management and configuration to set up the state machine, which would increase operational overhead. Amazon EventBridge rule can also be used to pass events to the Lambda function, but it would not provide the same level of decoupling and reliability as SQS. Using Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function, is similar to SQS, but SNS is a publish-subscribe messaging service and SQS is a queue service. SNS is used for sending messages to multiple recipients, SQS is used for sending messages to a single recipient, so SQS is more appropriate for this use case. References: AWS SQS AWS Step Functions AWS EventBridge AWS SNS",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "62a7747ecb9c",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 127
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223611"
      }
    },
    {
      "question": {
        "id": "sap-c02_403eee3358b2",
        "number": 144,
        "text": "A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis indicates that the\nissue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The\nFSx for Windows File Server file system is configured with 10 TB of storage.\nThe solutions architect discovers that the file system has reached its maximum capacity. The solutions architect must ensure that users can regain access. The\nsolution also must prevent the problem from occurring again.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Remove old user profiles to create spac",
            "letter": "A"
          },
          {
            "text": "Migrate the user profiles to an Amazon FSx for Lustre file system.",
            "letter": "B"
          },
          {
            "text": "Increase capacity by using the update-file-system comman",
            "letter": "C"
          },
          {
            "text": "Implement an Amazon CloudWatch metric that monitors free spac",
            "letter": "D"
          },
          {
            "text": "Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required. F. Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatc G. Use AWS Step Functions to increase the capacity as required. H. Remove old user profiles to create spac I. Create an additional FSx for Windows File Server file system.Update the user profile redirection for 50% of the users to use the new file system.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "It can prevent the issue from happening again by monitoring the file system with the FreeStorageCapacity metric in Amazon CloudWatch and using Amazon EventBridge to invoke an AWS Lambda function to increase the capacity as required. This ensures that the file system always has enough free space to store user profiles and avoids reaching maximum capacity.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "403eee3358b2",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 130
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223622"
      }
    },
    {
      "question": {
        "id": "sap-c02_48a1125f5e1b",
        "number": 145,
        "text": "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\ngroups and a single instance type.\nRecently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to\nimprove the overall reliability of the workload.\nWhich solution will meet this requirement?",
        "options": [
          {
            "text": "Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.",
            "letter": "A"
          },
          {
            "text": "Create a new launch template version that uses attribute-based instance type selectio",
            "letter": "B"
          },
          {
            "text": "Configure the Auto Scaling group to use the new launch template version.",
            "letter": "C"
          },
          {
            "text": "Update the launch template Auto Scaling group to increase the number of placement groups.",
            "letter": "D"
          },
          {
            "text": "Update the launch template to use a larger instance type.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-instance-type-requirements.html#use-attribut",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "48a1125f5e1b",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 131
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223631"
      }
    },
    {
      "question": {
        "id": "sap-c02_1c7e67e5b684",
        "number": 146,
        "text": "A company runs a processing engine in the AWS Cloud The engine processes environmental data from logistics centers to calculate a sustainability index The\ncompany has millions of devices in logistics centers that are spread across Europe The devices send information to the processing engine through a RESTful API\nThe API experiences unpredictable bursts of traffic The company must implement a solution to process all data that the devices send to the processing engine\nData loss is unacceptable\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an Application Load Balancer (ALB) for the RESTful API Create an Amazon Simple Queue Service (Amazon SQS) queue Create a listener and a target group for the ALB Add the SQS queue as the target Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue",
            "letter": "A"
          },
          {
            "text": "Create an Amazon API Gateway HTTP API that implements the RESTful API Create an Amazon Simple Queue Service (Amazon SQS) queue Create an API Gateway service integration with the SQS queue Create an AWS Lambda function to process messages in the SQS queue",
            "letter": "B"
          },
          {
            "text": "Create an Amazon API Gateway REST API that implements the RESTful API Create a fleet of Amazon EC2 instances in an Auto Scaling group Create an API Gateway Auto Scaling group proxy integration Use the EC2 instances to process incoming data",
            "letter": "C"
          },
          {
            "text": "Create an Amazon CloudFront distribution for the RESTful API Create a data stream in Amazon Kinesis Data Streams Set the data stream as the origin for the distribution Create an AWS Lambda function to consume and process data in the data stream",
            "letter": "D"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "it will use the ALB to handle the unpredictable bursts of traffic and route it to the SQS queue. The SQS queue will act as a buffer to store incoming data temporarily and the container running in Amazon ECS with the Fargate launch type will process messages in the queue. This approach will ensure that all data is processed and prevent data loss.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "1c7e67e5b684",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 134
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223637"
      }
    },
    {
      "question": {
        "id": "sap-c02_e80a0e30f4fb",
        "number": 147,
        "text": "A company runs its sales reporting application in an AWS Region in the United States. The application uses an Amazon API Gateway Regional API and AWS\nLambda functions to generate on-demand reports from data in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 and\nis accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS service for the domain. Route 53 is\nconfigured with a simple routing policy to route traffic to the API Gateway API.\nIn the next 6 months, the company plans to expand operations to Europe. More than 90% of the database traffic is read-only traffic. The company has already\ndeployed an API Gateway API and Lambda functions in the new Region.\nA solutions architect must design a solution that minimizes latency for users who download reports. Which solution will meet these requirements?",
        "options": [
          {
            "text": "Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the primary database in the original Region to the database in the new Regio",
            "letter": "A"
          },
          {
            "text": "Change the Route 53 record to latency-based routing to connect to the API Gateway API.",
            "letter": "B"
          },
          {
            "text": "Use an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC) to replicate the primary database in the original Region to the database in the new Regio",
            "letter": "C"
          },
          {
            "text": "Change the Route 53 record to geolocation routing to connect to the API Gateway API.",
            "letter": "D"
          },
          {
            "text": "Configure a cross-Region read replica for the RDS database in the new Regio F. Change the Route 53 record to latency-based routing to connect to the API Gateway API. G. Configure a cross-Region read replica for the RDS database in the new Regio H. Change the Route 53 record to geolocation routing to connect to the API",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "The company should configure a cross-Region read replica for the RDS database in the new Region. The company should change the Route 53 record to latency- based routing to connect to the API Gateway API. This solution will meet the requirements because a cross-Region read replica is a feature that enables you to create a MariaDB, MySQL, Oracle, PostgreSQL, or SQL Server read replica in a different Region from the source DB instance. You can use cross-Region read replicas to improve availability and disaster recovery, scale out globally, or migrate an existing database to a new Region1. By creating a cross-Region read replica for the RDS database in the new Region, the company can have a standby copy of its primary database that can serve read-only traffic from users in Europe. A latency-based routing policy is a feature that enables you to route traffic based on the latency between your users and your resources. You can use latency-based routing to route traffic to the resource that provides the best latency2. By changing the Route 53 record to latency-based routing, the company can minimize latency for users who download reports by connecting them to the API Gateway API in the Region that provides the best response time. The other options are not correct because: Using AWS Database Migration Service (AWS DMS) to replicate the primary database in the original Region to the database in the new Region would not be as cost-effective or simple as using a cross-Region read replica. AWS DMS is a service that enables you to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to perform one-time migrations or continuous data replication with high availability and consolidate databases into a petabyte-scale data warehouse3. However, AWS DMS requires more configuration and management than creating a cross-Region read replica, which is fully Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "e80a0e30f4fb",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 139
          },
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 116
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223643",
        "duplicate_sources": [
          "sap-c02_7_study_data.json",
          "sap-c02_8_study_data.json"
        ]
      }
    },
    {
      "question": {
        "id": "sap-c02_b51d97464ef0",
        "number": 148,
        "text": "A company has built a high performance computing (HPC) cluster in AWS tor a tightly coupled workload that generates a large number of shared files stored in\nAmazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the\ncluster size to 1,000 EC2 instances, overall performance was well below expectations.\nWhich collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Select THREE.)",
        "options": [
          {
            "text": "Ensure the HPC cluster Is launched within a single Availability Zone.",
            "letter": "A"
          },
          {
            "text": "Launch the EC2 instances and attach elastic network interfaces in multiples of four.",
            "letter": "B"
          },
          {
            "text": "Select EC2 Instance types with an Elastic Fabric Adapter (EFA) enabled.",
            "letter": "C"
          },
          {
            "text": "Ensure the cluster Is launched across multiple Availability Zones.",
            "letter": "D"
          },
          {
            "text": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array. F. Replace Amazon EFS with Amazon FSx for Lustre.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "* A. High performance computing (HPC) workload cluster should be in a single AZ. * C. Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instances to accelerate High Performance Computing (HPC) * F. Amazon FSx for Lustre - Use it for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling. Cluster â€“ packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b51d97464ef0",
        "sources": [
          {
            "file": "sap-c02_7_study_data.json",
            "question_number": 142
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.223648"
      }
    },
    {
      "question": {
        "id": "sap-c02_b1a2c26bf798",
        "number": 149,
        "text": "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes\nstatic content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this\napplication in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.\nA solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement\nWindows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in\ntime.\nWhich solution will meet these requirements with the LEAST management overhead?",
        "options": [
          {
            "text": "Create an Amazon Elastic File System (Amazon EFS) file shar Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "A"
          },
          {
            "text": "Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance",
            "letter": "B"
          },
          {
            "text": "Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.",
            "letter": "C"
          },
          {
            "text": "Create a new AMI from the current EC2 instance that is runnin",
            "letter": "D"
          },
          {
            "text": "Create an Amazon FSx for Lustre file syste F. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance G. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system. H. Create an Amazon FSx for Windows File Server file syste I. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance J. Implement a user data script to install the application and mount the FSx for Windows File Server file syste K. Perform a seamless domain join to join the instance to the AD domain. L. Create a new AMI from the current EC2 instance that is runnin M. Create an Amazon Elastic File System (Amazon EFS) file syste N. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instance O. Perform a seamless domain join to join the instance to the AD domain.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_join_instance.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b1a2c26bf798",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 3
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227014"
      }
    },
    {
      "question": {
        "id": "sap-c02_8a58603b9c0f",
        "number": 150,
        "text": "Passing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\nA company runs an loT platform on AWS loT sensors in various locations send data to the company's Node js API servers on Amazon EC2 instances running\nbehind an Application Load Balancer The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume\nThe number of sensors the company has deployed in the field has increased over time and is expected to grow significantly The API servers are consistently\noverloaded and RDS metrics show high write latency\nWhich of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-\nefficient? {Select TWO.)",
        "options": [
          {
            "text": "Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS",
            "letter": "A"
          },
          {
            "text": "Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas",
            "letter": "B"
          },
          {
            "text": "Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data",
            "letter": "C"
          },
          {
            "text": "Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load",
            "letter": "D"
          },
          {
            "text": "Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CE",
        "explanation": "Option C is correct because leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data resolves the issues permanently and enable growth as new sensors are provisioned. Amazon Kinesis Data Streams is a serverless streaming data service that simplifies the capture, processing, and storage of data streams at any scale. Kinesis Data Streams can handle any amount of streaming data and process data from hundreds of thousands of sources with very low latency. AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. Lambda can be triggered by Kinesis Data Streams events and process the data records in real time. Lambda can also scale automatically based on the incoming data volume. By using Kinesis Data Streams and Lambda, the company can reduce the load on the API servers and improve the performance and scalability of the data ingestion and processing layer3 Option E is correct because re-architecting the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance resolves the issues permanently and enable growth as new sensors are provisioned. Amazon DynamoDB is a fully managed key-value and document database that delivers single- digit millisecond performance at any scale. DynamoDB supports auto scaling, which automatically adjusts read and write capacity based on actual traffic patterns. DynamoDB also supports on-demand capacity mode, which instantly accommodates up to double the previous peak traffic on a table. By using DynamoDB instead of RDS MySQL DB instance, the company can eliminate high write latency and improve scalability and performance of the database tier. References: 1: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html 2: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html 3: https://docs.aws.amazon.com/streams/latest/dev/introduction.html : https://docs.aws.amazon.com/lambda/latest/dg/welcome.html : https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html : https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html :",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "8a58603b9c0f",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 6
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227064"
      }
    },
    {
      "question": {
        "id": "sap-c02_f3011b46ba13",
        "number": 151,
        "text": "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process\nof the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors,\nanother CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the\napplication logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.\nHow can this be accomplished?",
        "options": [
          {
            "text": "Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda functio",
            "letter": "A"
          },
          {
            "text": "For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.",
            "letter": "B"
          },
          {
            "text": "Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify cod",
            "letter": "C"
          },
          {
            "text": "Rollback if Amazon CloudWatch alarms are triggered.",
            "letter": "D"
          },
          {
            "text": "Refactor the AWS CLI scripts into a single script that deploys the new Lambda versio F. When deployment is completed, the script tests execut G. If errors are detected, revert to the previous Lambda version. H. Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda versio I. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://aws.amazon.com/about-aws/whats-new/2017/11/aws-lambda-supports-traffic-shifting-and-phased-deploy Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "f3011b46ba13",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 8
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227094"
      }
    },
    {
      "question": {
        "id": "sap-c02_8dd5d840e789",
        "number": 152,
        "text": "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and configured the\nRCUs and WCUs on the DynamoDB table to\nmatch the expected peak load. The peak load occurs once a week for a 4-hour period and is double the average load. The application load is close to the average\nload tor the rest of the week. The access pattern includes many more writes to the table than reads of the table.\nA solutions architect needs to implement a solution to minimize the cost of the table. Which solution will meet these requirements?",
        "options": [
          {
            "text": "Use AWS Application Auto Scaling to increase capacity during the peak perio",
            "letter": "A"
          },
          {
            "text": "Purchase reserved RCUs and WCUs to match the average load.",
            "letter": "B"
          },
          {
            "text": "Configure on-demand capacity mode for the table.",
            "letter": "C"
          },
          {
            "text": "Configure DynamoDB Accelerator (DAX) in front of the tabl",
            "letter": "D"
          },
          {
            "text": "Reduce the provisioned read capacity to match the new peak load on the table. F. Configure DynamoDB Accelerator (DAX) in front of the tabl G. Configure on-demand capacity mode for the table.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "This solution meets the requirements by using Application Auto Scaling to automatically increase capacity during the peak period, which will handle the double the average load. And by purchasing reserved RCUs and WCUs to match the average load, it will minimize the cost of the table for the rest of the week when the load is close to the average.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "8dd5d840e789",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 9
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227111"
      }
    },
    {
      "question": {
        "id": "sap-c02_5388c4977240",
        "number": 153,
        "text": "A company is running an application in the AWS Cloud. The application runs on containers in an Amazon Elastic Container Service (Amazon ECS) cluster. The\nECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the\napplication must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost. Which solution will\nmeet these requirements with the LEAST amount of operational overhead?",
        "options": [
          {
            "text": "Provision an Aurora Replica in a different Region.",
            "letter": "A"
          },
          {
            "text": "Set up AWS DataSync for continuous replication of the data to a different Region.",
            "letter": "B"
          },
          {
            "text": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.",
            "letter": "C"
          },
          {
            "text": "Use Amazon Data Lifecycle Manager {Amazon DLM) to schedule a snapshot every 5 minutes.",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "Provision an Aurora Replica in a different Region will meet the requirement of the application being able to recover to a separate AWS Region in the event of an application failure, and no data can be lost, with the least amount of operational overhead.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "5388c4977240",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 10
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227139"
      }
    },
    {
      "question": {
        "id": "sap-c02_4d2d1daa70cc",
        "number": 154,
        "text": "A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where the application\nneeds to access an Amazon Aurora DB cluster. The EC2 instances are all associated with the same security group. The DB cluster is associated with its own\nsecurity group.\nThe solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB cluster.\nWhich combination of steps will meet these requirements? (Select TWO.)",
        "options": [
          {
            "text": "Add an inbound rule to the EC2 instances' security grou",
            "letter": "A"
          },
          {
            "text": "Specify the DB cluster's security group as the source over the default Aurora port.",
            "letter": "B"
          },
          {
            "text": "Add an outbound rule to the EC2 instances' security grou",
            "letter": "C"
          },
          {
            "text": "Specify the DB cluster's security group as the destination over the default Aurora port.",
            "letter": "D"
          },
          {
            "text": "Add an inbound rule to the DB cluster's security grou F. Specify the EC2 instances' security group as the source over the default Aurora port. G. Add an outbound rule to the DB cluster's security grou H. Specify the EC2 instances' security group as the destination over the default Aurora port. I. Add an outbound rule to the DB cluster's security grou J. Specify the EC2 instances' security group as the destination over the ephemeral ports.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AB",
        "explanation": "* B. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port. This allows the instances to make outbound connections to the DB cluster on the default Aurora port. C. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port. This allows connections to the DB cluster from the EC2 instances on the default Aurora port.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4d2d1daa70cc",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 12
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227155"
      }
    },
    {
      "question": {
        "id": "sap-c02_b1c7a609e6c4",
        "number": 155,
        "text": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the\nweb application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database.\nThe solutions architect has configured the database to have three read replicas.\nDuring testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The\nsolutions architect must improve the application's performance.\nWhich actions should the solutions architect take to meet these requirements? (Choose two.)",
        "options": [
          {
            "text": "Use the cluster endpoint of the Aurora database.",
            "letter": "A"
          },
          {
            "text": "Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.",
            "letter": "B"
          },
          {
            "text": "Use the Lambda Provisioned Concurrency feature.",
            "letter": "C"
          },
          {
            "text": "Move the code for opening the database connection in the Lambda function outside of the event handler.",
            "letter": "D"
          },
          {
            "text": "Change the API Gateway endpoint to an edge-optimized endpoint.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BD",
        "explanation": "Connect to RDS outside of Lambda handler method to improve performance https://awstut.com/en/2022/04/30/connect-to-rds-outside-of-lambda-handler-method- to-improve-performance-en Using RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these surges might cause issues due to oversubscribing connections or creating new connections at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in this pool. This approach avoids the memory Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b1c7a609e6c4",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 14
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227171"
      }
    },
    {
      "question": {
        "id": "sap-c02_eb53ed6b9596",
        "number": 156,
        "text": "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the\ncompany's AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all\nthe company's AWS accounts.\nThe company's security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Configure AWS Single Sign-On (AWS SSO) to connect to Active Directory by using SAML 2.0.Enable automatic provisioning by using the System for Cross- domain Identity Management (SCIM) v2.0 protoco",
            "letter": "A"
          },
          {
            "text": "Grant access to the AWS accounts by using attribute-based access controls (ABACs).",
            "letter": "B"
          },
          {
            "text": "Configure AWS Single Sign-On (AWS SSO) by using AWS SSO as an identity sourc",
            "letter": "C"
          },
          {
            "text": "Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protoco",
            "letter": "D"
          },
          {
            "text": "Grant access to the AWS accounts by using AWS SSO permission sets. F. In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provide Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) G. Provision IAM users that are mapped to the federated user H. Grant access that corresponds to appropriate groups in Active Director I. Grant access to the required AWS accounts by using cross-account IAM users. J. In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provide K. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Director L. Grant access to the required AWS accounts by using cross-account IAM roles.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "https://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-single-sign-on/",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "eb53ed6b9596",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 27
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227210"
      }
    },
    {
      "question": {
        "id": "sap-c02_686e12b2e114",
        "number": 157,
        "text": "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use\ntheir company email address to request an account. The company wants to ensure that developers are not launching costly services or running services\nunnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.\nWhich combination of steps will meet these requirements? (Choose three.)",
        "options": [
          {
            "text": "Create an SCP to set a fixed monthly account usage limi",
            "letter": "A"
          },
          {
            "text": "Apply the SCP to the developer accounts.",
            "letter": "B"
          },
          {
            "text": "Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process.",
            "letter": "C"
          },
          {
            "text": "Create an SCP to deny access to costly services and component",
            "letter": "D"
          },
          {
            "text": "Apply the SCP to the developer accounts. F. Create an IAM policy to deny access to costly services and component G. Apply the IAM policy to the developer accounts. H. Create an AWS Budgets alert action to terminate services when the budgeted amount is reached.Configure the action to terminate all services. I. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reache J. Invoke an AWS Lambda function to terminate all services.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "BC",
        "explanation": "Option A is incorrect because creating an SCP to set a fixed monthly account usage limit is not possible. SCPs are policies that specify the services and actions that users and roles can use in the member accounts of an AWS Organization. SCPs cannot enforce budget limits or prevent users from launching costly services or running services unnecessarily1 Option B is correct because using AWS Budgets to create a fixed monthly budget for each developerâ€™s account as part of the account creation process meets the requirement of giving developers a fixed monthly budget to limit their AWS costs. AWS Budgets allows you to plan your service usage, service costs, and instance reservations. You can create budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount2 Option C is correct because creating an SCP to deny access to costly services and components meets the requirement of ensuring that developers are not launching costly services or running services unnecessarily. SCPs can restrict access to certain AWS services or actions based on conditions such as region, resource tags, or request time. For example, an SCP can deny access to Amazon Redshift clusters or Amazon EC2 instances with certain instance types1 Option D is incorrect because creating an IAM policy to deny access to costly services and components is not sufficient to meet the requirement of ensuring that developers are not launching costly services or running services unnecessarily. IAM policies can only control access to resources within a single AWS account. If developers have multiple accounts or can create new accounts, they can bypass the IAM policy restrictions. SCPs can apply across multiple accounts within an AWS Organization and prevent users from creating new accounts that do not comply with the SCP rules3 Option E is incorrect because creating an AWS Budgets alert action to terminate services when the budgeted amount is reached is not possible. AWS Budgets alert actions can only perform one of the following actions: apply an IAM policy, apply an SCP, or send a notification through Amazon SNS. AWS Budgets alert actions cannot terminate services directly. Option F is correct because creating an AWS Budgets alert action to send an Amazon SNS notification when the budgeted amount is reached and invoking an AWS Lambda function to terminate all services meets the requirement of giving developers a fixed monthly budget to limit their AWS costs. AWS Budgets alert actions can send notifications through Amazon SNS when a budget threshold is breached. Amazon SNS can trigger an AWS Lambda function that can perform Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "686e12b2e114",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 33
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227221"
      }
    },
    {
      "question": {
        "id": "sap-c02_a9561c2488d9",
        "number": 158,
        "text": "A company's solutions architect is reviewing a new internally developed application in a sandbox AWS account The application uses an AWS Auto Scaling group\nof Amazon EC2 instances that have an IAM instance profile attached Part of the application logic creates and accesses secrets from AWS Secrets Manager The\ncompany has an AWS Lambda function that calls the application API to test the functionality The company also has created an AWS CloudTrail trail in the account\nThe application's developer has attached the SecretsManagerReadWnte AWS managed IAM policy to an IAM role The IAM role is associated with the instance\nprofile that is attached to the EC2 instances The solutions architect has invoked the Lambda function for testing\nThe solutions architect must replace the SecretsManagerReadWnte policy with a new policy that provides least privilege access to the Secrets Manager actions\nthat the application requires\nWhat is the MOST operationally efficient solution that meets these requirements?",
        "options": [
          {
            "text": "Generate a policy based on CloudTrail events for the IAM role Use the generated policy output to create a new IAM policy Use the newly generated IAM policy to replace the SecretsManagerReadWnte policy that is attached to the IAM role",
            "letter": "A"
          },
          {
            "text": "Create an analyzer in AWS Identity and Access Management Access Analyzer Use the IAM role's Access Advisor findings to create a new IAM policy Use the newly created IAM policy to replace the SecretsManagerReadWnte policy that is attached to the IAM role",
            "letter": "B"
          },
          {
            "text": "Use the aws cloudtrail lookup-events AWS CLI command to filter and export CloudTrail events that are related to Secrets Manager Use a new IAM policy that contains the actions from CloudTrail to replace the SecretsManagerReadWnte policy that is attached to the IAM role",
            "letter": "C"
          },
          {
            "text": "Use the IAM policy simulator to generate an IAM policy for the IAM role Use the newly generated IAM policy to replace the SecretsManagerReadWnte policy that is attached to the IAM role",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "The IAM policy simulator will generate a policy that contains only the necessary permissions for the application to access Secrets Manager, providing the least privilege necessary to get the job done. This is the most efficient solution as it will not require additional steps such as analyzing CloudTrail events or manually creating and testing an IAM policy. You can use the IAM policy simulator to generate an IAM policy for an IAM role by specifying the role and the API actions and resources that the application or service requires. The simulator will then generate an IAM policy that grants the least privilege access to those actions and resources. Once you have generated an IAM policy using the simulator, you can replace the existing SecretsManagerReadWnte policy that is attached to the IAM role with the newly generated policy. This will ensure that the application or service has the least privilege access to the Secrets Manager actions that it requires. You can access the IAM policy simulator through the IAM console, AWS CLI, and AWS SDKs. Here is the link for more information: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_simulator.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "a9561c2488d9",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 34
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227228"
      }
    },
    {
      "question": {
        "id": "sap-c02_323789d9c833",
        "number": 159,
        "text": "A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-\nmemory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the\ninfrastructure must be healthy and must be in an active state.\nA solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible\ndowntime.\nWhich combination of steps will meet these requirements? (Select THREE.)",
        "options": [
          {
            "text": "Use an Elastic Load Balancer to distribute traffic across multiple EC2 instance",
            "letter": "A"
          },
          {
            "text": "Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.",
            "letter": "B"
          },
          {
            "text": "Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances Ensure that the EC2 instances are configured in unlimited mode.",
            "letter": "C"
          },
          {
            "text": "Modify the DB instance to create a read replica in the same Availability Zon",
            "letter": "D"
          },
          {
            "text": "Promote the read replica to be the primary DB instance in failure scenarios. F. Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones. G. Create a replication group for the ElastiCache for Redis cluste H. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances. I. Create a replication group for the ElastiCache for Redis cluste J. Enable Multi-AZ on the cluster.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AD",
        "explanation": "Option A is correct because using an Elastic Load Balancer and an Auto Scaling group with a minimum capacity of two instances can improve the availability and scalability of the EC2 instances that host the application. The load balancer can distribute traffic across multiple instances and the Auto Scaling group can replace any unhealthy instances automatically1 Option D is correct because modifying the DB instance to create a Multi-AZ deployment that extends across two Availability Zones can improve the availability and durability of the RDS for MariaDB database. Multi-AZ deployments provide enhanced data protection and minimize downtime by automatically failing over to a standby replica in another Availability Zone in case of a planned or unplanned outage4 Option F is correct because creating a replication group for the ElastiCache for Redis cluster and enabling Multi-AZ on the cluster can improve the availability and fault tolerance of the in-memory data store. A replication group consists of a primary node and up to five read-only replica nodes that are synchronized with the primary node using asynchronous replication. Multi-AZ allows automatic failove to one of the replicas if the primary node fails or becomes unreachable6 References: 1: https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html 2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances-unlimited-mode.htm 3: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html 4: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html 5: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoScaling.html 6: https://docs.aws.amazon.com/AmazonElastiCache/latest/red- ug/Replication.Redis.Groups.html Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "323789d9c833",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 35
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227234"
      }
    },
    {
      "question": {
        "id": "sap-c02_36713b14005a",
        "number": 160,
        "text": "A company is using multiple AWS accounts The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A The company's applications\nand databases are running in Account B.\nA solutions architect win deploy a two-net application In a new VPC To simplify the configuration, the db.example com CNAME record set tor the Amazon RDS\nendpoint was created in a private hosted zone for Amazon Route 53.\nDuring deployment, the application failed to start. Troubleshooting revealed that db.example com is not resolvable on the Amazon EC2 instance The solutions\narchitect confirmed that the record set was created correctly in Route 53.\nWhich combination of steps should the solutions architect take to resolve this issue? (Select TWO )",
        "options": [
          {
            "text": "Deploy the database on a separate EC2 instance in the new VPC Create a record set for the instance's private IP in the private hosted zone",
            "letter": "A"
          },
          {
            "text": "Use SSH to connect to the application tier EC2 instance Add an RDS endpoint IP address to the/eto/resolv.conf file",
            "letter": "B"
          },
          {
            "text": "Create an authorization lo associate the private hosted zone in Account A with the new VPC In Account B",
            "letter": "C"
          },
          {
            "text": "Create a private hosted zone for the example.com domain m Account B Configure Route 53 replication between AWS accounts",
            "letter": "D"
          },
          {
            "text": "Associate a new VPC in Account B with a hosted zone in Account F. Delete the association authorization In Account A.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "CE",
        "explanation": "Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "36713b14005a",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 46
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227250"
      }
    },
    {
      "question": {
        "id": "sap-c02_1debc3470d3d",
        "number": 161,
        "text": "A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto\nScaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling\ngroup are set to zero. An Amazon RDS Multi-AZ DB instance stores the applicationâ€™s data. The DB instance has a read replica in the backup Region. The\napplication presents an endpoint to end users by using an Amazon Route 53 record.\nThe company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company\ndoes not have a large enough budget for an active-active strategy.\nWhat should a solutions architect recommend to meet these requirements?",
        "options": [
          {
            "text": "Reconfigure the applicationâ€™s Route 53 record with a latency-based routing policy that load balances traffic between the two ALB",
            "letter": "A"
          },
          {
            "text": "Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group value",
            "letter": "B"
          },
          {
            "text": "Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Regio",
            "letter": "C"
          },
          {
            "text": "Configure the CloudWatch alarm to invoke the Lambda function.",
            "letter": "D"
          },
          {
            "text": "Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group value F. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealth G. Update the applicationâ€™s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs. H. Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Regio I. Reconfigure the applicationâ€™s Route 53 record with a latency-based routing policy that load balances traffic between the two ALB J. Remove the read replic K. Replace the read replica with a standalone RDS DB instanc L. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3. M. Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted target N. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group value O. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Regio P. Configure the CloudWatch alarm to invoke the Lambda function.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values, and then configuring Route 53 with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. Finally, the application's Route 53 record should be updated with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs. This approach provides automatic failover to the backup region when a health check failure occurs, reducing the RTO to less than 15 minutes. Additionally, this approach is cost-effective as it does not require an active-active strategy.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "1debc3470d3d",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 50
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227266"
      }
    },
    {
      "question": {
        "id": "sap-c02_451d4f5b3c05",
        "number": 162,
        "text": "A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an\nAmazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index\nthat contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.\nThe company is concerned about ongoing costs and asks a solutions architect to recommend a new solution. Which solution will meet these requirements MOST\ncost-effectively?",
        "options": [
          {
            "text": "Replace all the data nodes with UltraWarm nodes to handle the expected capacit",
            "letter": "A"
          },
          {
            "text": "Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.",
            "letter": "B"
          },
          {
            "text": "Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacit",
            "letter": "C"
          },
          {
            "text": "Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the dat",
            "letter": "D"
          },
          {
            "text": "Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy. F. Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacit G. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the dat H. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storag I. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy. J. Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacit K. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "By reducing the number of data nodes in the cluster to 2 and adding UltraWarm nodes to handle the expected capacity, the company can reduce the cost of running the cluster. Additionally, configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data will ensure that the data is stored in the most cost-effective manner. Finally, transitioning the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy will ensure that the data is retained for compliance purposes, while also reducing the ongoing costs.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "451d4f5b3c05",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 52
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227272"
      }
    },
    {
      "question": {
        "id": "sap-c02_fe85d6afa86e",
        "number": 163,
        "text": "An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party\naccepts only one public CIDR block in each client's allow list.\nThe customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application\nLoad Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to\nthe private subnets.\nHow should a solutions architect ensure that the web application can continue to call the third-parly API after the migration?",
        "options": [
          {
            "text": "Associate a block of customer-owned public IP addresses to the VP",
            "letter": "A"
          },
          {
            "text": "Enable public IP addressing for public subnets in the VPC.",
            "letter": "B"
          },
          {
            "text": "Register a block of customer-owned public IP addresses in the AWS accoun",
            "letter": "C"
          },
          {
            "text": "Create Elastic IP addresses from the address block and assign them lo the NAT gateways in the VPC. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "D"
          },
          {
            "text": "Create Elastic IP addresses from the block of customer-owned IP addresse F. Assign the static Elastic IP addresses to the ALB. G. Register a block of customer-owned public IP addresses in the AWS accoun H. Set up AWS Global Accelerator to use Elastic IP addresses from the address bloc I. Set the ALB as the accelerator endpoint.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "When EC2 instances reach third-party API through internet, their privates IP addresses will be masked by NAT Gateway public IP address. https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-bring-your-own-ip-byoip-for-amaz",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "fe85d6afa86e",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 57
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227279"
      }
    },
    {
      "question": {
        "id": "sap-c02_9431b8d51159",
        "number": 164,
        "text": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1\nRegions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs\nto support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.\nThe company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct\nConnect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that\nis configured to route all inter-VPC traffic within that Region.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create a private VIF from the DX-A connection into a Direct Connect gatewa",
            "letter": "A"
          },
          {
            "text": "Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availabilit",
            "letter": "B"
          },
          {
            "text": "Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gatewa",
            "letter": "C"
          },
          {
            "text": "Peer the transit gatewayswith each other to support cross-Region routing.",
            "letter": "D"
          },
          {
            "text": "Create a transit VIF from the DX-A connection into a Direct Connect gatewa F. Associate the eu-west-1 transit gateway with this Direct Connect gatewa G. Create a transit VIF from the DX-B connection into a separate Direct Connect gatewa H. Associate the us-east-1 transit gateway with this separate Direct Connect gatewa I. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing. J. Create a transit VIF from the DX-A connection into a Direct Connect gatewa K. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availabilit L. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gatewa M. Configure the Direct Connect gateway to route traffic between the transit gateways. N. Create a transit VIF from the DX-A connection into a Direct Connect gatewa O. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availabilit P. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gatewa Q. Peer the transit gateways with each other to support cross-Region routing.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "in this solution, two transit VIFs are created - one from the DX-A connection and one from the DX-B connection - into the same Direct Connect gateway for high availability. Both the eu-west-1 and us-east-1 transit gateways are then associated with this Direct Connect gateway. The transit gateways are then peered with each other to support cross-Region routing. This solution meets the requirements of the company by creating a highly available connection between the on- premises data center and the VPCs in both the eu-west-1 and us-east-1 regions, and by enabling direct traffic routing between VPCs in those regions.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "9431b8d51159",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 60
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227286"
      }
    },
    {
      "question": {
        "id": "sap-c02_be16410eaac5",
        "number": 165,
        "text": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect\ncreated a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.\nThe solutions architect created the following IAM policy and attached it to an IAM role:\nDuring tests, me solutions architect was able to successfully get existing test objects m the S3 bucket However, attempts to upload a new object resulted in an\nerror message. The error message stated that me action was forbidden.\nWhich action must me solutions architect add to the IAM policy to meet all the requirements?",
        "options": [
          {
            "text": "Kms:GenerateDataKey Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "A"
          },
          {
            "text": "KmsGetKeyPolpcy",
            "letter": "B"
          },
          {
            "text": "kmsGetPubKKey",
            "letter": "C"
          },
          {
            "text": "kms:SKjn",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/ \"An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\" This error message indicates that your IAM user or role needs permission for the kms:GenerateDataKey action.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "be16410eaac5",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 62
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227291"
      }
    },
    {
      "question": {
        "id": "sap-c02_bb8db0bb4446",
        "number": 166,
        "text": "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the companyâ€™s data center. As part of the\nmigration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company\nthen wants to query and analyze the data.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises host",
            "letter": "A"
          },
          {
            "text": "Configure Data Exploration in AWS Migration Hu",
            "letter": "B"
          },
          {
            "text": "Use AWS Glue to perform an ETL job against the dat",
            "letter": "C"
          },
          {
            "text": "Query the data by using Amazon S3 Select.",
            "letter": "D"
          },
          {
            "text": "Export only the VM performance information from the on-premises host F. Directly import the required data into AWS Migration Hu G. Update any missing information in Migration Hu H. Query the data by using Amazon QuickSight. I. Create a script to automatically gather the server information from the on-premises host J. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hu K. Query the data directly in the Migration Hub console. L. Deploy the AWS Application Discovery Agent to each on-premises serve M. Configure Data Exploration in AWS Migration Hu N. Use Amazon Athena to run predefined queries against the data in Amazon S3.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "bb8db0bb4446",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 69
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227303"
      }
    },
    {
      "question": {
        "id": "sap-c02_4ec6b395cb76",
        "number": 167,
        "text": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda\nfunction. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support\nfailover to another AWS Region.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.",
            "letter": "A"
          },
          {
            "text": "Create an Amazon Simple Queue Service (Amazon SQS) queu",
            "letter": "B"
          },
          {
            "text": "Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda functio",
            "letter": "C"
          },
          {
            "text": "Configure the Lambda function to pull messages from the queue for processing.",
            "letter": "D"
          },
          {
            "text": "Deploy the Lambda function to the us-west-2 Regio F. Create an API Gateway endpoint in us-west-2 to direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints. G. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Regio H. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "This solution allows for deploying the Lambda function and API Gateway endpoint to another region, providing a failover option in case of any issues in the primary region. Using Route 53's failover routing policy allows for automatic routing of traffic to the healthy endpoint, ensuring that the application is available even in case of issues in one region. This solution provides a cost-effective and simple way to implement failover while minimizing operational overhead.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "4ec6b395cb76",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 70
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227309"
      }
    },
    {
      "question": {
        "id": "sap-c02_d6c59e699c4a",
        "number": 168,
        "text": "A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees\nwill access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible to the public.\nThe documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed\nof retrieval are not concerns of the company.\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\nWhich solution will meet these requirements at the LOWEST cost?",
        "options": [
          {
            "text": "Create an Amazon S3 bucke",
            "letter": "A"
          },
          {
            "text": "Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as defaul",
            "letter": "B"
          },
          {
            "text": "Configure the S3 bucket for website hostin",
            "letter": "C"
          },
          {
            "text": "Create an S3 interface endpoin",
            "letter": "D"
          },
          {
            "text": "Configure the S3 bucket to allow access only through that endpoint. F. Launch an Amazon EC2 instance that runs a web serve G. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks. H. Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived dat I. Use the Cold HDD (sc1) volume typ J. Configure the instance security groups to allow access only from private networks. K. Create an Amazon S3 bucke L. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as defaul M. Configure the S3 bucket for website hostin N. Create an S3 interface endpoin O. Configure the S3 bucket to allow access only through that endpoint.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "D",
        "explanation": "The S3 Glacier Deep Archive storage class is the lowest-cost storage class offered by Amazon S3, and it is designed for archival data that is accessed infrequently and for which retrieval time of several hours is acceptable. S3 interface endpoint for the VPC ensures that access to the bucket is only from resources within the VPC and this will meet the requirement of not being accessible to the public. And also, S3 bucket can be configured for website hosting, and this will allow employees to access the documents through the corporate intranet. Using an EC2 instance and a file system or block store would be more expensive and unnecessary because the number of requests to the data will be low and availability and speed of retrieval are not concerns. Additionally, using Amazon S3 bucket will provide durability, scalability and availability of data.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "d6c59e699c4a",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 78
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227324"
      }
    },
    {
      "question": {
        "id": "sap-c02_2ed86109c991",
        "number": 169,
        "text": "A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily.\nThe company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has\nestablished an AWS Direct Connect connection between the on-premises network and AWS.\nWhich data migration strategy should the company use?",
        "options": [
          {
            "text": "Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway.",
            "letter": "A"
          },
          {
            "text": "Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.",
            "letter": "B"
          },
          {
            "text": "Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).",
            "letter": "C"
          },
          {
            "text": "Use AWS DataSync to schedule a daily task lo replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS),",
            "letter": "D"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://aws.amazon.com/storagegateway/file/ https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-to-fsx-datasync.html https://docs.aws.amazon.com/systems-manager/latest/userguide/prereqs- operating-systems.html#prereqs-os-win",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "2ed86109c991",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 82
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227329"
      }
    },
    {
      "question": {
        "id": "sap-c02_2e71e733f44a",
        "number": 170,
        "text": "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the company can\nuse to share a common network across multiple accounts.\nThe company's infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to manage the network.\nIndividual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.\nWhich combination of actions should the solutions architect perform to meet these requirements? (Select TWO.)",
        "options": [
          {
            "text": "Create a transit gateway in the infrastructure account.",
            "letter": "A"
          },
          {
            "text": "Enable resource sharing from the AWS Organizations management account.",
            "letter": "B"
          },
          {
            "text": "Create VPCs in each AWS account within the organization in AWS Organization",
            "letter": "C"
          },
          {
            "text": "Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure accoun",
            "letter": "D"
          },
          {
            "text": "Peer the VPCs in each individual account with the VPC in the infrastructure account, F. Create a resource share in AWS Resource Access Manager in the infrastructure accoun G. Select the specific AWS Organizations OU that will use the shared networ H. Select each subnet to associate with the resource share. I. Create a resource share in AWS Resource Access Manager in the infrastructure accoun J. Select the specific AWS Organizations OU that will use the shared networ K. Select each prefix list to associate with the resource share.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AE",
        "explanation": "https://docs.aws.amazon.com/vpc/latest/userguide/sharing-managed-prefix-lists.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "2e71e733f44a",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 86
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227335"
      }
    },
    {
      "question": {
        "id": "sap-c02_b66532160a62",
        "number": 171,
        "text": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly\nthrough a simple API over HTTPS. The solution must scale automatically in response to demand.\nWhich solutions meet these requirements? (Choose two.)\nPassing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
        "options": [
          {
            "text": "Create an Amazon API Gateway REST AP",
            "letter": "A"
          },
          {
            "text": "Configure this API with direct integrations to DynamoDB by using API Gatewayâ€™s AWS integration type.",
            "letter": "B"
          },
          {
            "text": "Create an Amazon API Gateway HTTP AP",
            "letter": "C"
          },
          {
            "text": "Configure this API with direct integrations to Dynamo DB by using API Gatewayâ€™s AWS integration type.",
            "letter": "D"
          },
          {
            "text": "Create an Amazon API Gateway HTTP AP F. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables. G. Create an accelerator in AWS Global Accelerato H. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables. I. Create a Network Load Balance J. Configure listener rules to forward requests to the appropriate AWS Lambda functions",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.htm",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "b66532160a62",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 88
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227344"
      }
    },
    {
      "question": {
        "id": "sap-c02_11640f8534fc",
        "number": 172,
        "text": "A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a\nsecond S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS\nLambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.\nThe application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout\nerrors. The function timeout is set to its maximum value. A solutions architect needs to refactor the applicationâ€™s architecture to prevent invocation failures. The\ncompany does not want to manage the underlying infrastructure.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
        "options": [
          {
            "text": "Modify the application deployment by building a Docker image that contains the application code.Publish the image to Amazon Elastic Container Registry (Amazon ECR).",
            "letter": "A"
          },
          {
            "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargat",
            "letter": "B"
          },
          {
            "text": "Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
            "letter": "C"
          },
          {
            "text": "Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function.Increase the provisioned concurrency of the Lambda function.",
            "letter": "D"
          },
          {
            "text": "Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3. F. Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instanc G. Adjust the Lambda function to mount the EFS file share.",
            "letter": "E"
          },
          {
            "text": "Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR). - This step is necessary to package the application code in a container and make it available for running on ECS. B. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
            "letter": "A"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AB",
        "explanation": "A. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR). - This step is necessary to package the application code in a container and make it available for running on ECS. B. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "11640f8534fc",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 91
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227355"
      }
    },
    {
      "question": {
        "id": "sap-c02_18d9a0e31a5b",
        "number": 173,
        "text": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common\nsecurity group rules for the AWS accounts in the organization.\nThe company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to\nand from the company's on-premises network.\nDevelopers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently,\nthe security team notifies the owners of the other AWS accounts when changes are made to the allow list.\nThe solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.\nWhich solution meets these requirements with the LEAST amount of operational overhead?",
        "options": [
          {
            "text": "Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS accoun",
            "letter": "A"
          },
          {
            "text": "Deploy an AWS Lambda function in each AWS accoun",
            "letter": "B"
          },
          {
            "text": "Configure the Lambda function to run every time an SNS topic receives a messag",
            "letter": "C"
          },
          {
            "text": "Configure the Lambda function to take an IP address as input and add it to a list of security groups in the accoun",
            "letter": "D"
          },
          {
            "text": "Instruct the security team to distribute changes by publishing messages to its SNS topic. F. Create new customer-managed prefix lists in each AWS account within the organizatio G. Populate the prefix lists in each account with all internal CIDR range H. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security group I. Instruct the security team to share updates with each AWS account owner. J. Create a new customer-managed prefix list in the security team's AWS accoun K. Populate the customer-managed prefix list with all internal CIDR range L. Share the customer-managed prefix listwith the organization by using AWS Resource Access Manage M. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups. N. Create an IAM role in each account in the organizatio O. Grant permissions to update security groups.Deploy an AWS Lambda function in the security team's AWS accoun P. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account.",
            "letter": "E"
          }
        ],
        "topic": 1,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "Create a new customer-managed prefix list in the security teamâ€™s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups. This solution meets the requirements Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "18d9a0e31a5b",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 94
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227363"
      }
    },
    {
      "question": {
        "id": "sap-c02_69138b3479c6",
        "number": 174,
        "text": "Passing Certification Exams Made Easy visit - https://www.surepassexam.com\n\n\nRecommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam\nhttps://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)\n- (Exam Topic 2)\nA company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement\ngroups and a single instance type.\nRecently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to\nimprove the overall reliability of the workload.\nWhich solution will meet this requirement?",
        "options": [
          {
            "text": "Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.",
            "letter": "A"
          },
          {
            "text": "Create a new launch template version that uses attribute-based instance type selectio",
            "letter": "B"
          },
          {
            "text": "Configure the Auto Scaling group to use the new launch template version.",
            "letter": "C"
          },
          {
            "text": "Update the launch template Auto Scaling group to increase the number of placement groups.",
            "letter": "D"
          },
          {
            "text": "Update the launch template to use a larger instance type.",
            "letter": "E"
          }
        ],
        "topic": 3,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-instance-type-requirements.html#use-attribut",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "69138b3479c6",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 110
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227384"
      }
    },
    {
      "question": {
        "id": "sap-c02_90f26731b2af",
        "number": 175,
        "text": "A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of events that the\nsolution receives. If a processing error occurs, the event must move into a separate queue for review.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Send event details to an Amazon Simple Notification Service (Amazon SNS) topi",
            "letter": "A"
          },
          {
            "text": "Configure an AWS Lambda function as a subscriber to the SNS topic to process the event",
            "letter": "B"
          },
          {
            "text": "Add an on-failure destination to the functio",
            "letter": "C"
          },
          {
            "text": "Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.",
            "letter": "D"
          },
          {
            "text": "Publish events to an Amazon Simple Queue Service (Amazon SQS) queu F. Create an Amazon EC2 Auto Scaling grou G. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queu Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions) H. Configure the application to write failed messages to a dead-letter queue. I. Write events to an Amazon DynamoDB tabl J. Configure a DynamoDB stream for the tabl K. Configure the stream to invoke an AWS Lambda functio L. Configure the Lambda function to process the events. M. Publish events to an Amazon EventBridge event bu N. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that isbehind an Application Load Balancer (ALB). Set the ALB as the event bus targe O. Configure the event bus to retry event P. Write messages to a dead-letter queue if the application cannot process the messages.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "Amazon Simple Notification Service (Amazon SNS) is a fully managed pub/sub messaging service that enables users to send messages to multiple subscribers1. Users can send event details to an Amazon SNS topic and configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Lambda is a serverless compute service that runs code in response to events and automatically manages the underlying compute resources2. Users can add an on-failure destination to the function and set an Amazon Simple Queue Service (Amazon SQS) queue as the target. Amazon SQS is a fully managed message queuing service that enables users to decouple and scale microservices, distributed systems, and serverless applications3. This way, if a processing error occurs, the event will move into the separate queue for review. Option B is incorrect because publishing events to an Amazon SQS queue and creating an Amazon EC2 Auto Scaling group will not have the ability to scale in and out based on the number of events that the solution receives. Amazon EC2 is a web service that provides secure, resizable compute capacity in the cloud. Auto Scaling is a feature that helps users maintain application availability and allows them to scale their EC2 capacity up or down automatically according to conditions they define. However, for this use case, using SQS and EC2 will not take advantage of the serverless capabilities of Lambda and SNS. Option C is incorrect because writing events to an Amazon DynamoDB table and configuring a DynamoDB stream for the table will not have the ability to move events into a separate queue for review if a processing error occurs. Amazon DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB Streams is a feature that captures data modification events in DynamoDB tables. Users can configure the stream to invoke a Lambda function, but they cannot configure an on-failure destination for the function. Option D is incorrect because publishing events to an Amazon EventBridge event bus and setting an Application Load Balancer (ALB) as the event bus target will not have the ability to move events into a separate queue for review if a processing error occurs. Amazon EventBridge is a serverless event bus service that makes it easy to connect applications with data from a variety of sources. An ALB is a load balancer that distributes incoming application traffic across multiple targets, such as EC2 instances, containers, IP addresses, Lambda functions, and virtual appliances. Users can configure EventBridge to retry events, but they cannot configure an on-failure destination for the ALB.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "90f26731b2af",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 114
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227396"
      }
    },
    {
      "question": {
        "id": "sap-c02_cc117c4bb5dc",
        "number": 176,
        "text": "A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text\nmessage.\nThe applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company\nuses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.\nWhich solution will meet these requirements with the LEAST ongoing operational overhead?",
        "options": [
          {
            "text": "Use Amazon Connect to replace the old call center hardwar",
            "letter": "A"
          },
          {
            "text": "Use Amazon Pinpoint to send text message surveys to customers.",
            "letter": "B"
          },
          {
            "text": "Use Amazon Connect to replace the old call center hardwar",
            "letter": "C"
          },
          {
            "text": "Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.",
            "letter": "D"
          },
          {
            "text": "Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling grou F. Use the EC2 instances to send text message surveys to customers. G. Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "A",
        "explanation": "Amazon Connect is a cloud-based contact center service that allows you to set up a virtual call center for your business. It provides an easy-to-use interface for managing customer interactions through voice and chat. Amazon Connect integrates with other AWS services, such as Amazon S3 and Amazon Kinesis, to help you collect, store, and analyze customer data for insights into customer behavior and trends. On the other hand, Amazon Pinpoint is a marketing automation and analytics service that allows you to engage with your customers across different channels, such as email, SMS, push notifications, and voice. It helps you create personalized campaigns based on user behavior and enables you to track user engagement and retention. While both services allow you to communicate with your customers, they serve different purposes. Amazon Connect is focused on customer support and service, while Amazon Pinpoint is focused on marketing and engagement.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "cc117c4bb5dc",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 119
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227406"
      }
    },
    {
      "question": {
        "id": "sap-c02_68b130a879ed",
        "number": 177,
        "text": "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-\npremises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application\nLoad Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple\ntarget groups and uses path-based routing to forward requests based on the URL path.\nThe company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to\nallow traffic flow to AWS from the on-premises network so that the clients can continue to access the application.\nWhich solution will meet these requirements?",
        "options": [
          {
            "text": "Configure the existing ALB to use static IP addresse",
            "letter": "A"
          },
          {
            "text": "Assign IP addresses in multiple Availability Zones to the AL",
            "letter": "B"
          },
          {
            "text": "Add the ALB IP addresses to the firewall appliance.",
            "letter": "C"
          },
          {
            "text": "Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zone",
            "letter": "D"
          },
          {
            "text": "Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall applianc F. Update the clients to connect to the NLB. G. Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zone H. Add the existing target groups to the NL I. Update the clients to connect to the NL J. Delete the ALB Add the NLB IP addresses to the firewall appliance. K. Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zone L. Create an ALB-type target group for the GWLB and add the existing AL M. Add the GWLB IP addresses to the firewall applianc N. Update the clients to connect to the GWLB.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "B",
        "explanation": "The company should create a Network Load Balancer (NLB) and associate it with one static IP address in multiple Availability Zones. The company should also create an ALB-type target group for the NLB and add the existing ALB. The company should add the NLB IP addresses to the firewall appliance and update the clients to connect to the NLB. This solution will allow traffic flow to AWS from the on-premises network by using static IP addresses that can be added to the firewall applianceâ€™s allow list. The NLB will forward requests to the ALB, which will use path-based routing to forward requests to the target groups.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "68b130a879ed",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 124
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227412"
      }
    },
    {
      "question": {
        "id": "sap-c02_facf04153a4a",
        "number": 178,
        "text": "A company has built a high performance computing (HPC) cluster in AWS tor a tightly coupled workload that generates a large number of shared files stored in\nAmazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the\ncluster size to 1,000 EC2 instances, overall performance was well below expectations.\nWhich collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Select THREE.)",
        "options": [
          {
            "text": "Ensure the HPC cluster Is launched within a single Availability Zone.",
            "letter": "A"
          },
          {
            "text": "Launch the EC2 instances and attach elastic network interfaces in multiples of four.",
            "letter": "B"
          },
          {
            "text": "Select EC2 Instance types with an Elastic Fabric Adapter (EFA) enabled. Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "C"
          },
          {
            "text": "Ensure the cluster Is launched across multiple Availability Zones.",
            "letter": "D"
          },
          {
            "text": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array. F. Replace Amazon EFS with Amazon FSx for Lustre.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "AC",
        "explanation": "* A. High performance computing (HPC) workload cluster should be in a single AZ. * C. Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instances to accelerate High Performance Computing (HPC) * F. Amazon FSx for Lustre - Use it for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling. Cluster â€“ packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "facf04153a4a",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 126
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227418"
      }
    },
    {
      "question": {
        "id": "sap-c02_ffe77306df3c",
        "number": 179,
        "text": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS workloads. The\ncompany has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be highly available and cannot be down\nfor longer than 10 minutes. The company needs to minimize ongoing maintenance.\nWhich solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "text": "Migrate to Amazon CloudWatch dashboard Passing Certification Exams Made Easy visit - https://www.surepassexam.com Recommend!! Get the Full SAP-C02 dumps in VCE and PDF From SurePassExam https://www.surepassexam.com/SAP-C02-exam-dumps.html (300 New Questions)",
            "letter": "A"
          },
          {
            "text": "Recreate the dashboards to match the existing Grafana dashboard",
            "letter": "B"
          },
          {
            "text": "Use automatic dashboards where possible.",
            "letter": "C"
          },
          {
            "text": "Create an Amazon Managed Grafana workspac",
            "letter": "D"
          },
          {
            "text": "Configure a new Amazon CloudWatch data source.Export dashboards from the existing Grafana instanc F. Import the dashboards into the new workspace. G. Create an AMI that has Grafana pre-installe H. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AM I. Set the Auto Scaling group's minimum, desired, and maximum number of instances to on J. Create an Application Load Balancer that serves at least two Availability Zones. K. Configure AWS Backup to back up the EC2 instance that runs Grafana once each hou L. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required.",
            "letter": "E"
          }
        ],
        "topic": 2,
        "difficulty": "medium",
        "keywords": []
      },
      "answer": {
        "correct_answer": "C",
        "explanation": "By creating an AMI that has Grafana pre-installed and storing the existing dashboards in Amazon Elastic File System (Amazon EFS) it allows for faster and more efficient scaling, and by creating an Auto Scaling group that uses the new AMI and setting the Auto Scaling group's minimum, desired, and maximum number of instances to one and creating an Application Load Balancer that serves at least two Availability Zones, it ensures high availability and minimized downtime.",
        "confidence": "high"
      },
      "metadata": {
        "content_hash": "ffe77306df3c",
        "sources": [
          {
            "file": "sap-c02_8_study_data.json",
            "question_number": 133
          }
        ],
        "extraction_method": "consolidated_v2",
        "consolidation_date": "2025-08-11T21:16:20.227432"
      }
    }
  ]
}